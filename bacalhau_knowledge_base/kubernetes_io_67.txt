URL: https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/

DocumentationKubernetes BlogTrainingPartnersCommunityCase StudiesVersionsRelease Informationv1.31v1.30v1.29v1.28v1.27English中文 (Chinese)Français (French)Bahasa Indonesia (Indonesian)日本語 (Japanese)DocumentationAvailable Documentation VersionsGetting startedLearning environmentProduction environmentContainer RuntimesInstalling Kubernetes with deployment toolsBootstrapping clusters with kubeadmInstalling kubeadmTroubleshooting kubeadmCreating a cluster with kubeadmCustomizing components with the kubeadm APIOptions for Highly Available TopologyCreating Highly Available Clusters with kubeadmSet up a High Availability etcd Cluster with kubeadmConfiguring each kubelet in your cluster using kubeadmDual-stack support with kubeadmTurnkey Cloud SolutionsBest practicesConsiderations for large clustersRunning in multiple zonesValidate node setupEnforcing Pod Security StandardsPKI certificates and requirementsConceptsOverviewKubernetes ComponentsObjects In KubernetesKubernetes Object ManagementObject Names and IDsLabels and SelectorsNamespacesAnnotationsField SelectorsFinalizersOwners and DependentsRecommended LabelsThe Kubernetes APICluster ArchitectureNodesCommunication between Nodes and the Control PlaneControllersLeasesCloud Controller ManagerAbout cgroup v2Container Runtime Interface (CRI)Garbage CollectionMixed Version ProxyContainersImagesContainer EnvironmentRuntime ClassContainer Lifecycle HooksWorkloadsPodsPod LifecycleInit ContainersSidecar ContainersEphemeral ContainersDisruptionsPod Quality of Service ClassesUser NamespacesDownward APIWorkload ManagementDeploymentsReplicaSetStatefulSetsDaemonSetJobsAutomatic Cleanup for Finished JobsCronJobReplicationControllerAutoscaling WorkloadsManaging WorkloadsServices, Load Balancing, and NetworkingServiceIngressIngress ControllersGateway APIEndpointSlicesNetwork PoliciesDNS for Services and PodsIPv4/IPv6 dual-stackTopology Aware RoutingNetworking on WindowsService ClusterIP allocationService Internal Traffic PolicyStorageVolumesPersistent VolumesProjected VolumesEphemeral VolumesStorage ClassesVolume Attributes ClassesDynamic Volume ProvisioningVolume SnapshotsVolume Snapshot ClassesCSI Volume CloningStorage CapacityNode-specific Volume LimitsVolume Health MonitoringWindows StorageConfigurationConfiguration Best PracticesConfigMapsSecretsLiveness, Readiness, and Startup ProbesResource Management for Pods and ContainersOrganizing Cluster Access Using kubeconfig FilesResource Management for Windows nodesSecurityCloud Native SecurityPod Security StandardsPod Security AdmissionService AccountsPod Security PoliciesSecurity For Windows NodesControlling Access to the Kubernetes APIRole Based Access Control Good PracticesGood practices for Kubernetes SecretsMulti-tenancyHardening Guide - Authentication MechanismsKubernetes API Server Bypass RisksLinux kernel security constraints for Pods and containersSecurity ChecklistPoliciesLimit RangesResource QuotasProcess ID Limits And ReservationsNode Resource ManagersScheduling, Preemption and EvictionKubernetes SchedulerAssigning Pods to NodesPod OverheadPod Scheduling ReadinessPod Topology Spread ConstraintsTaints and TolerationsScheduling FrameworkDynamic Resource AllocationScheduler Performance TuningResource Bin PackingPod Priority and PreemptionNode-pressure EvictionAPI-initiated EvictionCluster AdministrationNode ShutdownsCertificatesCluster NetworkingLogging ArchitectureMetrics For Kubernetes System ComponentsMetrics for Kubernetes Object StatesSystem LogsTraces For Kubernetes System ComponentsProxies in KubernetesAPI Priority and FairnessCluster AutoscalingInstalling AddonsCoordinated Leader ElectionWindows in KubernetesWindows containers in KubernetesGuide for Running Windows Containers in KubernetesExtending KubernetesCompute, Storage, and Networking ExtensionsNetwork PluginsDevice PluginsExtending the Kubernetes APICustom ResourcesKubernetes API Aggregation LayerOperator patternTasksInstall ToolsInstall and Set Up kubectl on LinuxInstall and Set Up kubectl on macOSInstall and Set Up kubectl on WindowsAdminister a ClusterAdministration with kubeadmCertificate Management with kubeadmConfiguring a cgroup driverReconfiguring a kubeadm clusterUpgrading kubeadm clustersUpgrading Linux nodesUpgrading Windows nodesChanging The Kubernetes Package RepositoryMigrating from dockershimChanging the Container Runtime on a Node from Docker Engine to containerdMigrate Docker Engine nodes from dockershim to cri-dockerdFind Out What Container Runtime is Used on a NodeTroubleshooting CNI plugin-related errorsCheck whether dockershim removal affects youMigrating telemetry and security agents from dockershimGenerate Certificates ManuallyManage Memory, CPU, and API ResourcesConfigure Default Memory Requests and Limits for a NamespaceConfigure Default CPU Requests and Limits for a NamespaceConfigure Minimum and Maximum Memory Constraints for a NamespaceConfigure Minimum and Maximum CPU Constraints for a NamespaceConfigure Memory and CPU Quotas for a NamespaceConfigure a Pod Quota for a NamespaceInstall a Network Policy ProviderUse Antrea for NetworkPolicyUse Calico for NetworkPolicyUse Cilium for NetworkPolicyUse Kube-router for NetworkPolicyRomana for NetworkPolicyWeave Net for NetworkPolicyAccess Clusters Using the Kubernetes APIAdvertise Extended Resources for a NodeAutoscale the DNS Service in a ClusterChange the Access Mode of a PersistentVolume to ReadWriteOncePodChange the default StorageClassSwitching from Polling to CRI Event-based Updates to Container StatusChange the Reclaim Policy of a PersistentVolumeCloud Controller Manager AdministrationConfigure a kubelet image credential providerConfigure Quotas for API ObjectsControl CPU Management Policies on the NodeControl Topology Management Policies on a nodeCustomizing DNS ServiceDebugging DNS ResolutionDeclare Network PolicyDeveloping Cloud Controller ManagerEnable Or Disable A Kubernetes APIEncrypting Confidential Data at RestDecrypt Confidential Data that is Already Encrypted at RestGuaranteed Scheduling For Critical Add-On PodsIP Masquerade Agent User GuideLimit Storage ConsumptionMigrate Replicated Control Plane To Use Cloud Controller ManagerNamespaces WalkthroughOperating etcd clusters for KubernetesReserve Compute Resources for System DaemonsRunning Kubernetes Node Components as a Non-root UserSafely Drain a NodeSecuring a ClusterSet Kubelet Parameters Via A Configuration FileShare a Cluster with NamespacesUpgrade A ClusterUse Cascading Deletion in a ClusterUsing a KMS provider for data encryptionUsing CoreDNS for Service DiscoveryUsing NodeLocal DNSCache in Kubernetes ClustersUsing sysctls in a Kubernetes ClusterUtilizing the NUMA-aware Memory ManagerVerify Signed Kubernetes ArtifactsConfigure Pods and ContainersAssign Memory Resources to Containers and PodsAssign CPU Resources to Containers and PodsConfigure GMSA for Windows Pods and containersResize CPU and Memory Resources assigned to ContainersConfigure RunAsUserName for Windows pods and containersCreate a Windows HostProcess PodConfigure Quality of Service for PodsAssign Extended Resources to a ContainerConfigure a Pod to Use a Volume for StorageConfigure a Pod to Use a PersistentVolume for StorageConfigure a Pod to Use a Projected Volume for StorageConfigure a Security Context for a Pod or ContainerConfigure Service Accounts for PodsPull an Image from a Private RegistryConfigure Liveness, Readiness and Startup ProbesAssign Pods to NodesAssign Pods to Nodes using Node AffinityConfigure Pod InitializationAttach Handlers to Container Lifecycle EventsConfigure a Pod to Use a ConfigMapShare Process Namespace between Containers in a PodUse a User Namespace With a PodUse an Image Volume With a PodCreate static PodsTranslate a Docker Compose File to Kubernetes ResourcesEnforce Pod Security Standards by Configuring the Built-in Admission ControllerEnforce Pod Security Standards with Namespace LabelsMigrate from PodSecurityPolicy to the Built-In PodSecurity Admission ControllerMonitoring, Logging, and DebuggingTroubleshooting ApplicationsDebug PodsDebug ServicesDebug a StatefulSetDetermine the Reason for Pod FailureDebug Init ContainersDebug Running PodsGet a Shell to a Running ContainerTroubleshooting ClustersTroubleshooting kubectlResource metrics pipelineTools for Monitoring ResourcesMonitor Node HealthDebugging Kubernetes nodes with crictlAuditingDebugging Kubernetes Nodes With KubectlDeveloping and debugging services locally using telepresenceWindows debugging tipsManage Kubernetes ObjectsDeclarative Management of Kubernetes Objects Using Configuration FilesDeclarative Management of Kubernetes Objects Using KustomizeManaging Kubernetes Objects Using Imperative CommandsImperative Management of Kubernetes Objects Using Configuration FilesUpdate API Objects in Place Using kubectl patchMigrate Kubernetes Objects Using Storage Version MigrationManaging SecretsManaging Secrets using kubectlManaging Secrets using Configuration FileManaging Secrets using KustomizeInject Data Into ApplicationsDefine a Command and Arguments for a ContainerDefine Dependent Environment VariablesDefine Environment Variables for a ContainerExpose Pod Information to Containers Through Environment VariablesExpose Pod Information to Containers Through FilesDistribute Credentials Securely Using SecretsRun ApplicationsRun a Stateless Application Using a DeploymentRun a Single-Instance Stateful ApplicationRun a Replicated Stateful ApplicationScale a StatefulSetDelete a StatefulSetForce Delete StatefulSet PodsHorizontal Pod AutoscalingHorizontalPodAutoscaler WalkthroughSpecifying a Disruption Budget for your ApplicationAccessing the Kubernetes API from a PodRun JobsRunning Automated Tasks with a CronJobCoarse Parallel Processing Using a Work QueueFine Parallel Processing Using a Work QueueIndexed Job for Parallel Processing with Static Work AssignmentJob with Pod-to-Pod CommunicationParallel Processing using ExpansionsHandling retriable and non-retriable pod failures with Pod failure policyAccess Applications in a ClusterDeploy and Access the Kubernetes DashboardAccessing ClustersConfigure Access to Multiple ClustersUse Port Forwarding to Access Applications in a ClusterUse a Service to Access an Application in a ClusterConnect a Frontend to a Backend Using ServicesCreate an External Load BalancerList All Container Images Running in a ClusterSet up Ingress on Minikube with the NGINX Ingress ControllerCommunicate Between Containers in the Same Pod Using a Shared VolumeConfigure DNS for a ClusterAccess Services Running on ClustersExtend KubernetesConfigure the Aggregation LayerUse Custom ResourcesExtend the Kubernetes API with CustomResourceDefinitionsVersions in CustomResourceDefinitionsSet up an Extension API ServerConfigure Multiple SchedulersUse an HTTP Proxy to Access the Kubernetes APIUse a SOCKS5 Proxy to Access the Kubernetes APISet up Konnectivity serviceTLSConfigure Certificate Rotation for the KubeletManage TLS Certificates in a ClusterManual Rotation of CA CertificatesManage Cluster DaemonsPerform a Rolling Update on a DaemonSetPerform a Rollback on a DaemonSetRunning Pods on Only Some NodesNetworkingAdding entries to Pod /etc/hosts with HostAliasesExtend Service IP RangesValidate IPv4/IPv6 dual-stackExtend kubectl with pluginsManage HugePagesSchedule GPUsTutorialsHello MinikubeLearn Kubernetes BasicsCreate a ClusterUsing Minikube to Create a ClusterDeploy an AppUsing kubectl to Create a DeploymentExplore Your AppViewing Pods and NodesExpose Your App PubliclyUsing a Service to Expose Your AppScale Your AppRunning Multiple Instances of Your AppUpdate Your AppPerforming a Rolling UpdateConfigurationExample: Configuring a Java MicroserviceExternalizing config using MicroProfile, ConfigMaps and SecretsUpdating Configuration via a ConfigMapConfiguring Redis using a ConfigMapAdopting Sidecar ContainersSecurityApply Pod Security Standards at the Cluster LevelApply Pod Security Standards at the Namespace LevelRestrict a Container's Access to Resources with AppArmorRestrict a Container's Syscalls with seccompStateless ApplicationsExposing an External IP Address to Access an Application in a ClusterExample: Deploying PHP Guestbook application with RedisStateful ApplicationsStatefulSet BasicsExample: Deploying WordPress and MySQL with Persistent VolumesExample: Deploying Cassandra with a StatefulSetRunning ZooKeeper, A Distributed System CoordinatorServicesConnecting Applications with ServicesUsing Source IPExplore Termination Behavior for Pods And Their EndpointsReferenceGlossaryAPI OverviewKubernetes API ConceptsServer-Side ApplyClient LibrariesCommon Expression Language in KubernetesKubernetes Deprecation PolicyDeprecated API Migration GuideKubernetes API health endpointsAPI Access ControlAuthenticatingAuthenticating with Bootstrap TokensAuthorizationUsing RBAC AuthorizationUsing Node AuthorizationWebhook ModeUsing ABAC AuthorizationAdmission ControllersDynamic Admission ControlManaging Service AccountsCertificates and Certificate Signing RequestsMapping PodSecurityPolicies to Pod Security StandardsKubelet authentication/authorizationTLS bootstrappingValidating Admission PolicyWell-Known Labels, Annotations and TaintsAudit AnnotationsKubernetes APIWorkload ResourcesPodBindingPodTemplateReplicationControllerReplicaSetDeploymentStatefulSetControllerRevisionDaemonSetJobCronJobHorizontalPodAutoscalerHorizontalPodAutoscalerPriorityClassPodSchedulingContext v1alpha3ResourceClaim v1alpha3ResourceClaimTemplate v1alpha3ResourceSlice v1alpha3Service ResourcesServiceEndpointsEndpointSliceIngressIngressClassConfig and Storage ResourcesConfigMapSecretCSIDriverCSINodeCSIStorageCapacityPersistentVolumeClaimPersistentVolumeStorageClassStorageVersionMigration v1alpha1VolumeVolumeAttachmentVolumeAttributesClass v1beta1Authentication ResourcesServiceAccountTokenRequestTokenReviewCertificateSigningRequestClusterTrustBundle v1alpha1SelfSubjectReviewAuthorization ResourcesLocalSubjectAccessReviewSelfSubjectAccessReviewSelfSubjectRulesReviewSubjectAccessReviewClusterRoleClusterRoleBindingRoleRoleBindingPolicy ResourcesFlowSchemaLimitRangeResourceQuotaNetworkPolicyPodDisruptionBudgetPriorityLevelConfigurationValidatingAdmissionPolicyValidatingAdmissionPolicyBindingExtend ResourcesCustomResourceDefinitionDeviceClass v1alpha3MutatingWebhookConfigurationValidatingWebhookConfigurationCluster ResourcesAPIServiceComponentStatusEventIPAddress v1beta1LeaseLeaseCandidate v1alpha1NamespaceNodeRuntimeClassServiceCIDR v1beta1Common DefinitionsDeleteOptionsLabelSelectorListMetaLocalObjectReferenceNodeSelectorRequirementObjectFieldSelectorObjectMetaObjectReferencePatchQuantityResourceFieldSelectorStatusTypedLocalObjectReferenceCommon ParametersInstrumentationService Level Indicator MetricsCRI Pod & Container MetricsNode metrics dataKubernetes Metrics ReferenceKubernetes Issues and SecurityKubernetes Issue TrackerKubernetes Security and Disclosure InformationCVE feedNode Reference InformationKubelet Checkpoint APILinux Kernel Version RequirementsArticles on dockershim Removal and on Using CRI-compatible RuntimesNode Labels Populated By The KubeletKubelet Configuration Directory MergingKubelet Device Manager API VersionsNode StatusNetworking ReferenceProtocols for ServicesPorts and ProtocolsVirtual IPs and Service ProxiesSetup toolsKubeadmkubeadm initkubeadm joinkubeadm upgradekubeadm configkubeadm resetkubeadm tokenkubeadm versionkubeadm alphakubeadm certskubeadm init phasekubeadm join phasekubeadm kubeconfigkubeadm reset phasekubeadm upgrade phaseImplementation detailsCommand line tool (kubectl)Introduction to kubectlkubectl Quick Referencekubectl referencekubectlkubectl annotatekubectl api-resourceskubectl api-versionskubectl applykubectl apply edit-last-appliedkubectl apply set-last-appliedkubectl apply view-last-appliedkubectl attachkubectl authkubectl auth can-ikubectl auth reconcilekubectl auth whoamikubectl autoscalekubectl certificatekubectl certificate approvekubectl certificate denykubectl cluster-infokubectl cluster-info dumpkubectl completionkubectl configkubectl config current-contextkubectl config delete-clusterkubectl config delete-contextkubectl config delete-userkubectl config get-clusterskubectl config get-contextskubectl config get-userskubectl config rename-contextkubectl config setkubectl config set-clusterkubectl config set-contextkubectl config set-credentialskubectl config unsetkubectl config use-contextkubectl config viewkubectl cordonkubectl cpkubectl createkubectl create clusterrolekubectl create clusterrolebindingkubectl create configmapkubectl create cronjobkubectl create deploymentkubectl create ingresskubectl create jobkubectl create namespacekubectl create poddisruptionbudgetkubectl create priorityclasskubectl create quotakubectl create rolekubectl create rolebindingkubectl create secretkubectl create secret docker-registrykubectl create secret generickubectl create secret tlskubectl create servicekubectl create service clusteripkubectl create service externalnamekubectl create service loadbalancerkubectl create service nodeportkubectl create serviceaccountkubectl create tokenkubectl debugkubectl deletekubectl describekubectl diffkubectl drainkubectl editkubectl eventskubectl execkubectl explainkubectl exposekubectl getkubectl kustomizekubectl labelkubectl logskubectl optionskubectl patchkubectl pluginkubectl plugin listkubectl port-forwardkubectl proxykubectl replacekubectl rolloutkubectl rollout historykubectl rollout pausekubectl rollout restartkubectl rollout resumekubectl rollout statuskubectl rollout undokubectl runkubectl scalekubectl setkubectl set envkubectl set imagekubectl set resourceskubectl set selectorkubectl set serviceaccountkubectl set subjectkubectl taintkubectl topkubectl top nodekubectl top podkubectl uncordonkubectl versionkubectl waitkubectl CommandskubectlJSONPath Supportkubectl for Docker Userskubectl Usage ConventionsComponent toolsFeature GatesFeature Gates (removed)kubeletkube-apiserverkube-controller-managerkube-proxykube-schedulerDebug clusterFlow controlConfiguration APIsClient Authentication (v1)Client Authentication (v1beta1)Event Rate Limit Configuration (v1alpha1)Image Policy API (v1alpha1)kube-apiserver Admission (v1)kube-apiserver Audit Configuration (v1)kube-apiserver Configuration (v1)kube-apiserver Configuration (v1alpha1)kube-apiserver Configuration (v1beta1)kube-controller-manager Configuration (v1alpha1)kube-proxy Configuration (v1alpha1)kube-scheduler Configuration (v1)kubeadm Configuration (v1beta3)kubeadm Configuration (v1beta4)kubeconfig (v1)Kubelet Configuration (v1)Kubelet Configuration (v1alpha1)Kubelet Configuration (v1beta1)Kubelet CredentialProvider (v1)WebhookAdmission Configuration (v1)External APIsKubernetes Custom Metrics (v1beta2)Kubernetes External Metrics (v1beta1)Kubernetes Metrics (v1beta1)SchedulingScheduler ConfigurationScheduling PoliciesOther ToolsMapping from dockercli to crictlContributeContribute to Kubernetes DocumentationSuggesting content improvementsContributing new contentOpening a pull requestDocumenting for a releaseBlogs and case studiesReviewing changesReviewing pull requestsFor approvers and reviewersLocalizing Kubernetes documentationParticipating in SIG DocsRoles and responsibilitiesIssue WranglersPR wranglersDocumentation style overviewContent guideStyle guideDiagram guideWriting a new topicPage content typesContent organizationCustom Hugo ShortcodesUpdating Reference DocumentationQuickstartContributing to the Upstream Kubernetes CodeGenerating Reference Documentation for the Kubernetes APIGenerating Reference Documentation for kubectl CommandsGenerating Reference Documentation for MetricsGenerating Reference Pages for Kubernetes Components and ToolsAdvanced contributingViewing Site AnalyticsDocs smoke test pageKubernetes DocumentationGetting startedProduction environmentInstalling Kubernetes with deployment toolsBootstrapping clusters with kubeadmCreating a cluster with kubeadmCreating a cluster with kubeadmUsingkubeadm, you can create a minimum viable Kubernetes cluster that conforms to best practices.
In fact, you can usekubeadmto set up a cluster that will pass theKubernetes Conformance tests.kubeadmalso supports other cluster lifecycle functions, such asbootstrap tokensand cluster upgrades.Thekubeadmtool is good if you need:A simple way for you to try out Kubernetes, possibly for the first time.A way for existing users to automate setting up a cluster and test their application.A building block in other ecosystem and/or installer tools with a larger
scope.You can install and usekubeadmon various machines: your laptop, a set
of cloud servers, a Raspberry Pi, and more. Whether you're deploying into the
cloud or on-premises, you can integratekubeadminto provisioning systems such
as Ansible or Terraform.Before you beginTo follow this guide, you need:One or more machines running a deb/rpm-compatible Linux OS; for example: Ubuntu or CentOS.2 GiB or more of RAM per machine--any less leaves little room for your
apps.At least 2 CPUs on the machine that you use as a control-plane node.Full network connectivity among all machines in the cluster. You can use either a
public or a private network.You also need to use a version ofkubeadmthat can deploy the version
of Kubernetes that you want to use in your new cluster.Kubernetes' version and version skew support policyapplies tokubeadmas well as to Kubernetes overall.
Check that policy to learn about what versions of Kubernetes andkubeadmare supported. This page is written for Kubernetes v1.31.Thekubeadmtool's overall feature state is General Availability (GA). Some sub-features are
still under active development. The implementation of creating the cluster may change
slightly as the tool evolves, but the overall implementation should be pretty stable.Note:Any commands underkubeadm alphaare, by definition, supported on an alpha level.ObjectivesInstall a single control-plane Kubernetes clusterInstall a Pod network on the cluster so that your Pods can
talk to each otherInstructionsPreparing the hostsComponent installationInstall acontainer runtimeand kubeadm on all the hosts.
For detailed instructions and other prerequisites, seeInstalling kubeadm.Note:If you have already installed kubeadm, see the first two steps of theUpgrading Linux nodesdocument for instructions on how to upgrade kubeadm.When you upgrade, the kubelet restarts every few seconds as it waits in a crashloop for
kubeadm to tell it what to do. This crashloop is expected and normal.
After you initialize your control-plane, the kubelet runs normally.Network setupkubeadm similarly to other Kubernetes components tries to find a usable IP on
the network interfaces associated with a default gateway on a host. Such
an IP is then used for the advertising and/or listening performed by a component.To find out what this IP is on a Linux host you can use:ip route show# Look for a line starting with "default via"Note:If two or more default gateways are present on the host, a Kubernetes component will
try to use the first one it encounters that has a suitable global unicast IP address.
While making this choice, the exact ordering of gateways might vary between different
operating systems and kernel versions.Kubernetes components do not accept custom network interface as an option,
therefore a custom IP address must be passed as a flag to all components instances
that need such a custom configuration.Note:If the host does not have a default gateway and if a custom IP address is not passed
to a Kubernetes component, the component may exit with an error.To configure the API server advertise address for control plane nodes created with bothinitandjoin, the flag--apiserver-advertise-addresscan be used.
Preferably, this option can be set in thekubeadm APIasInitConfiguration.localAPIEndpointandJoinConfiguration.controlPlane.localAPIEndpoint.For kubelets on all nodes, the--node-ipoption can be passed in.nodeRegistration.kubeletExtraArgsinside a kubeadm configuration file
(InitConfigurationorJoinConfiguration).For dual-stack seeDual-stack support with kubeadm.The IP addresses that you assign to control plane components become part of their X.509 certificates'
subject alternative name fields. Changing these IP addresses would require
signing new certificates and restarting the affected components, so that the change in
certificate files is reflected. SeeManual certificate renewalfor more details on this topic.Warning:The Kubernetes project recommends against this approach (configuring all component instances
with custom IP addresses). Instead, the Kubernetes maintainers recommend to setup the host network,
so that the default gateway IP is the one that Kubernetes components auto-detect and use.
On Linux nodes, you can use commands such asip routeto configure networking; your operating
system might also provide higher level network management tools. If your node's default gateway
is a public IP address, you should configure packet filtering or other security measures that
protect the nodes and your cluster.Preparing the required container imagesThis step is optional and only applies in case you wishkubeadm initandkubeadm jointo not download the default container images which are hosted atregistry.k8s.io.Kubeadm has commands that can help you pre-pull the required images
when creating a cluster without an internet connection on its nodes.
SeeRunning kubeadm without an internet connectionfor more details.Kubeadm allows you to use a custom image repository for the required images.
SeeUsing custom imagesfor more details.Initializing your control-plane nodeThe control-plane node is the machine where the control plane components run, includingetcd(the cluster database) and theAPI Server(which thekubectlcommand line tool
communicates with).(Recommended) If you have plans to upgrade this single control-planekubeadmcluster
to high availability you should specify the--control-plane-endpointto set the shared endpoint
for all control-plane nodes. Such an endpoint can be either a DNS name or an IP address of a load-balancer.Choose a Pod network add-on, and verify whether it requires any arguments to
be passed tokubeadm init. Depending on which
third-party provider you choose, you might need to set the--pod-network-cidrto
a provider-specific value. SeeInstalling a Pod network add-on.(Optional)kubeadmtries to detect the container runtime by using a list of well
known endpoints. To use different container runtime or if there are more than one installed
on the provisioned node, specify the--cri-socketargument tokubeadm. SeeInstalling a runtime.To initialize the control-plane node run:kubeadm init <args>Considerations about apiserver-advertise-address and ControlPlaneEndpointWhile--apiserver-advertise-addresscan be used to set the advertise address for this particular
control-plane node's API server,--control-plane-endpointcan be used to set the shared endpoint
for all control-plane nodes.--control-plane-endpointallows both IP addresses and DNS names that can map to IP addresses.
Please contact your network administrator to evaluate possible solutions with respect to such mapping.Here is an example mapping:192.168.0.102 cluster-endpointWhere192.168.0.102is the IP address of this node andcluster-endpointis a custom DNS name that maps to this IP.
This will allow you to pass--control-plane-endpoint=cluster-endpointtokubeadm initand pass the same DNS name tokubeadm join. Later you can modifycluster-endpointto point to the address of your load-balancer in an
high availability scenario.Turning a single control plane cluster created without--control-plane-endpointinto a highly available cluster
is not supported by kubeadm.More informationFor more information aboutkubeadm initarguments, see thekubeadm reference guide.To configurekubeadm initwith a configuration file seeUsing kubeadm init with a configuration file.To customize control plane components, including optional IPv6 assignment to liveness probe
for control plane components and etcd server, provide extra arguments to each component as documented incustom arguments.To reconfigure a cluster that has already been created seeReconfiguring a kubeadm cluster.To runkubeadm initagain, you must firsttear down the cluster.If you join a node with a different architecture to your cluster, make sure that your deployed DaemonSets
have container image support for this architecture.kubeadm initfirst runs a series of prechecks to ensure that the machine
is ready to run Kubernetes. These prechecks expose warnings and exit on errors.kubeadm initthen downloads and installs the cluster control plane components. This may take several minutes.
After it finishes you should see:Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a Pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  /docs/concepts/cluster-administration/addons/

You can now join any number of machines by running the following on each node
as root:

  kubeadm join <control-plane-host>:<control-plane-port> --token <token> --discovery-token-ca-cert-hash sha256:<hash>To make kubectl work for your non-root user, run these commands, which are
also part of thekubeadm initoutput:mkdir -p$HOME/.kubesudo cp -i /etc/kubernetes/admin.conf$HOME/.kube/configsudo chown$(id -u):$(id -g)$HOME/.kube/configAlternatively, if you are therootuser, you can run:exportKUBECONFIG=/etc/kubernetes/admin.confWarning:The kubeconfig fileadmin.confthatkubeadm initgenerates contains a certificate withSubject: O = kubeadm:cluster-admins, CN = kubernetes-admin. The groupkubeadm:cluster-adminsis bound to the built-incluster-adminClusterRole.
Do not share theadmin.conffile with anyone.kubeadm initgenerates another kubeconfig filesuper-admin.confthat contains a certificate withSubject: O = system:masters, CN = kubernetes-super-admin.system:mastersis a break-glass, super user group that bypasses the authorization layer (for example RBAC).
Do not share thesuper-admin.conffile with anyone. It is recommended to move the file to a safe location.SeeGenerating kubeconfig files for additional userson how to usekubeadm kubeconfig userto generate kubeconfig files for additional users.Make a record of thekubeadm joincommand thatkubeadm initoutputs. You
need this command tojoin nodes to your cluster.The token is used for mutual authentication between the control-plane node and the joining
nodes. The token included here is secret. Keep it safe, because anyone with this
token can add authenticated nodes to your cluster. These tokens can be listed,
created, and deleted with thekubeadm tokencommand. See thekubeadm reference guide.Installing a Pod network add-onCaution:This section contains important information about networking setup and
deployment order.
Read all of this advice carefully before proceeding.You must deploy aContainer Network Interface(CNI) based Pod network add-on so that your Pods can communicate with each other.
Cluster DNS (CoreDNS) will not start up before a network is installed.Take care that your Pod network must not overlap with any of the host
networks: you are likely to see problems if there is any overlap.
(If you find a collision between your network plugin's preferred Pod
network and some of your host networks, you should think of a suitable
CIDR block to use instead, then use that duringkubeadm initwith--pod-network-cidrand as a replacement in your network plugin's YAML).By default,kubeadmsets up your cluster to use and enforce use ofRBAC(role based access
control).
Make sure that your Pod network plugin supports RBAC, and so do any manifests
that you use to deploy it.If you want to use IPv6--either dual-stack, or single-stack IPv6 only
networking--for your cluster, make sure that your Pod network plugin
supports IPv6.
IPv6 support was added to CNI inv0.6.0.Note:Kubeadm should be CNI agnostic and the validation of CNI providers is out of the scope of our current e2e testing.
If you find an issue related to a CNI plugin you should log a ticket in its respective issue
tracker instead of the kubeadm or kubernetes issue trackers.Several external projects provide Kubernetes Pod networks using CNI, some of which also
supportNetwork Policy.See a list of add-ons that implement theKubernetes networking model.Please refer to theInstalling Addonspage for a non-exhaustive list of networking addons supported by Kubernetes.
You can install a Pod network add-on with the following command on the
control-plane node or a node that has the kubeconfig credentials:kubectl apply -f <add-on.yaml>You can install only one Pod network per cluster.Once a Pod network has been installed, you can confirm that it is working by
checking that the CoreDNS Pod isRunningin the output ofkubectl get pods --all-namespaces.
And once the CoreDNS Pod is up and running, you can continue by joining your nodes.If your network is not working or CoreDNS is not in theRunningstate, check out thetroubleshooting guideforkubeadm.Managed node labelsBy default, kubeadm enables theNodeRestrictionadmission controller that restricts what labels can be self-applied by kubelets on node registration.
The admission controller documentation covers what labels are permitted to be used with the kubelet--node-labelsoption.
Thenode-role.kubernetes.io/control-planelabel is such a restricted label and kubeadm manually applies it using
a privileged client after a node has been created. To do that manually you can do the same by usingkubectl labeland ensure it is using a privileged kubeconfig such as the kubeadm managed/etc/kubernetes/admin.conf.Control plane node isolationBy default, your cluster will not schedule Pods on the control plane nodes for security
reasons. If you want to be able to schedule Pods on the control plane nodes,
for example for a single machine Kubernetes cluster, run:kubectl taint nodes --all node-role.kubernetes.io/control-plane-The output will look something like:node "test-01" untainted
...This will remove thenode-role.kubernetes.io/control-plane:NoScheduletaint
from any nodes that have it, including the control plane nodes, meaning that the
scheduler will then be able to schedule Pods everywhere.Additionally, you can execute the following command to remove thenode.kubernetes.io/exclude-from-external-load-balancerslabel
from the control plane node, which excludes it from the list of backend servers:kubectl label nodes --all node.kubernetes.io/exclude-from-external-load-balancers-Joining your nodesThe nodes are where your workloads (containers and Pods, etc) run. To add new nodes to your cluster do the following for each machine:SSH to the machineBecome root (e.g.sudo su -)Install a runtimeif neededRun the command that was output bykubeadm init. For example:kubeadm join --token <token> <control-plane-host>:<control-plane-port> --discovery-token-ca-cert-hash sha256:<hash>If you do not have the token, you can get it by running the following command on the control-plane node:kubeadm token listThe output is similar to this:TOKEN                    TTL  EXPIRES              USAGES           DESCRIPTION            EXTRA GROUPS8ewj1p.9r9hcjoqgajrj4gi  23h  2018-06-12T02:51:28Z authentication,  The default bootstrap  system:signing          token generated by     bootstrappers:'kubeadm init'.        kubeadm:default-node-tokenBy default, tokens expire after 24 hours. If you are joining a node to the cluster after the current token has expired,
you can create a new token by running the following command on the control-plane node:kubeadm token createThe output is similar to this:5didvk.d09sbcov8ph2amjwIf you don't have the value of--discovery-token-ca-cert-hash, you can get it by running the
following command chain on the control-plane node:openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null |\openssl dgst -sha256 -hex | sed's/^.* //'The output is similar to:8cb2de97839780a412b93877f8507ad6c94f73add17d5d7058e91741c9d5ec78Note:To specify an IPv6 tuple for<control-plane-host>:<control-plane-port>, IPv6 address must be enclosed in square brackets, for example:[2001:db8::101]:2073.The output should look something like:[preflight] Running pre-flight checks

... (log output of join workflow) ...

Node join complete:
* Certificate signing request sent to control-plane and response
  received.
* Kubelet informed of new secure connection details.

Run 'kubectl get nodes' on control-plane to see this machine join.A few seconds later, you should notice this node in the output fromkubectl get nodeswhen run on the control-plane node.Note:As the cluster nodes are usually initialized sequentially, the CoreDNS Pods are likely to all run
on the first control-plane node. To provide higher availability, please rebalance the CoreDNS Pods
withkubectl -n kube-system rollout restart deployment corednsafter at least one new node is joined.(Optional) Controlling your cluster from machines other than the control-plane nodeIn order to get a kubectl on some other computer (e.g. laptop) to talk to your
cluster, you need to copy the administrator kubeconfig file from your control-plane node
to your workstation like this:scp root@<control-plane-host>:/etc/kubernetes/admin.conf .kubectl --kubeconfig ./admin.conf get nodesNote:The example above assumes SSH access is enabled for root. If that is not the
case, you can copy theadmin.conffile to be accessible by some other user
andscpusing that other user instead.Theadmin.conffile gives the usersuperuserprivileges over the cluster.
This file should be used sparingly. For normal users, it's recommended to
generate an unique credential to which you grant privileges. You can do
this with thekubeadm kubeconfig user --client-name <CN>command. That command will print out a KubeConfig file to STDOUT which you
should save to a file and distribute to your user. After that, grant
privileges by usingkubectl create (cluster)rolebinding.(Optional) Proxying API Server to localhostIf you want to connect to the API Server from outside the cluster you can usekubectl proxy:scp root@<control-plane-host>:/etc/kubernetes/admin.conf .kubectl --kubeconfig ./admin.conf proxyYou can now access the API Server locally athttp://localhost:8001/api/v1Clean upIf you used disposable servers for your cluster, for testing, you can
switch those off and do no further clean up. You can usekubectl config delete-clusterto delete your local references to the
cluster.However, if you want to deprovision your cluster more cleanly, you should
firstdrain the nodeand make sure that the node is empty, then deconfigure the node.Remove the nodeTalking to the control-plane node with the appropriate credentials, run:kubectl drain <node name> --delete-emptydir-data --force --ignore-daemonsetsBefore removing the node, reset the state installed bykubeadm:kubeadm resetThe reset process does not reset or clean up iptables rules or IPVS tables. If you wish to reset iptables, you must do so manually:iptables -F&&iptables -t nat -F&&iptables -t mangle -F&&iptables -XIf you want to reset the IPVS tables, you must run the following command:ipvsadm -CNow remove the node:kubectl delete node <node name>If you wish to start over, runkubeadm initorkubeadm joinwith the
appropriate arguments.Clean up the control planeYou can usekubeadm reseton the control plane host to trigger a best-effort
clean up.See thekubeadm resetreference documentation for more information about this subcommand and its
options.Version skew policyWhile kubeadm allows version skew against some components that it manages, it is recommended that you
match the kubeadm version with the versions of the control plane components, kube-proxy and kubelet.kubeadm's skew against the Kubernetes versionkubeadm can be used with Kubernetes components that are the same version as kubeadm
or one version older. The Kubernetes version can be specified to kubeadm by using the--kubernetes-versionflag ofkubeadm initor theClusterConfiguration.kubernetesVersionfield when using--config. This option will control the versions
of kube-apiserver, kube-controller-manager, kube-scheduler and kube-proxy.Example:kubeadm is at 1.31kubernetesVersionmust be at 1.31 or 1.30kubeadm's skew against the kubeletSimilarly to the Kubernetes version, kubeadm can be used with a kubelet version that is
the same version as kubeadm or three versions older.Example:kubeadm is at 1.31kubelet on the host must be at 1.31, 1.30, 1.29 or 1.28kubeadm's skew against kubeadmThere are certain limitations on how kubeadm commands can operate on existing nodes or whole clusters
managed by kubeadm.If new nodes are joined to the cluster, the kubeadm binary used forkubeadm joinmust match
the last version of kubeadm used to either create the cluster withkubeadm initor to upgrade
the same node withkubeadm upgrade. Similar rules apply to the rest of the kubeadm commands
with the exception ofkubeadm upgrade.Example forkubeadm join:kubeadm version 1.31 was used to create a cluster withkubeadm initJoining nodes must use a kubeadm binary that is at version 1.31Nodes that are being upgraded must use a version of kubeadm that is the same MINOR
version or one MINOR version newer than the version of kubeadm used for managing the
node.Example forkubeadm upgrade:kubeadm version 1.30 was used to create or upgrade the nodeThe version of kubeadm used for upgrading the node must be at 1.30
or 1.31To learn more about the version skew between the different Kubernetes component see
theVersion Skew Policy.LimitationsCluster resilienceThe cluster created here has a single control-plane node, with a single etcd database
running on it. This means that if the control-plane node fails, your cluster may lose
data and may need to be recreated from scratch.Workarounds:Regularlyback up etcd. The
etcd data directory configured by kubeadm is at/var/lib/etcdon the control-plane node.Use multiple control-plane nodes. You can readOptions for Highly Available topologyto pick a cluster
topology that provideshigh-availability.Platform compatibilitykubeadm deb/rpm packages and binaries are built for amd64, arm (32-bit), arm64, ppc64le, and s390x
following themulti-platform
proposal.Multiplatform container images for the control plane and addons are also supported since v1.12.Only some of the network providers offer solutions for all platforms. Please consult the list of
network providers above or the documentation from each provider to figure out whether the provider
supports your chosen platform.TroubleshootingIf you are running into difficulties with kubeadm, please consult ourtroubleshooting docs.What's nextVerify that your cluster is running properly withSonobuoySeeUpgrading kubeadm clustersfor details about upgrading your cluster usingkubeadm.Learn about advancedkubeadmusage in thekubeadm reference documentationLearn more about Kubernetesconceptsandkubectl.See theCluster Networkingpage for a bigger list
of Pod network add-ons.See thelist of add-onsto
explore other add-ons, including tools for logging, monitoring, network policy, visualization &
control of your Kubernetes cluster.Configure how your cluster handles logs for cluster events and from
applications running in Pods.
SeeLogging Architecturefor
an overview of what is involved.FeedbackFor bugs, visit thekubeadm GitHub issue trackerFor support, visit the#kubeadmSlack channelGeneral SIG Cluster Lifecycle development Slack channel:#sig-cluster-lifecycleSIG Cluster LifecycleSIG informationSIG Cluster Lifecycle mailing list:kubernetes-sig-cluster-lifecycleFeedbackWas this page helpful?YesNoThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it onStack Overflow.
Open an issue in theGitHub Repositoryif you want toreport a problemorsuggest an improvement.Last modified July 05, 2024 at 4:06 PM PST:kubeadm: use v1beta4 in all docs examples (efc1133fa4)Edit this pageCreate child pageCreate documentation issuePrint entire sectionBefore you beginObjectivesInstructionsPreparing the hostsPreparing the required container imagesInitializing your control-plane nodeConsiderations about apiserver-advertise-address and ControlPlaneEndpointMore informationInstalling a Pod network add-onManaged node labelsControl plane node isolationJoining your nodes(Optional) Controlling your cluster from machines other than the control-plane node(Optional) Proxying API Server to localhostClean upRemove the nodeClean up the control planeVersion skew policykubeadm's skew against the Kubernetes versionkubeadm's skew against the kubeletkubeadm's skew against kubeadmLimitationsCluster resiliencePlatform compatibilityTroubleshootingWhat's nextFeedbackDocumentationBlogTrainingPartnersCommunityCase Studies© 2024 The Kubernetes Authors | Documentation Distributed underCC BY 4.0Copyright © 2024 The Linux Foundation ®. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see ourTrademark Usage pageICP license: 京ICP备17074266号-3