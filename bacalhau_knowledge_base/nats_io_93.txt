URL: https://nats.io/blog/nats-server-2.9.16-release/#-false-positive-vulnerabilities

AboutDownloadDocumentationBlogCommunitySupportPrivacyNATS Server 2.9.16 ReleaseByron Ruth‚Äî April 19, 2023Back to BlogThe NATS maintainers are very proud to announce the2.9.16 release! ü•≥This release brings another round of stability and performance improvements, leveraging our new quality engineering practices described in the2.9.15 announcement post, and iterating directly with users having high-scale and latency-sensitive workloads.Given the positive feedback on the previous announcement post focusing on the higher-level impact of the changes, going forward all releases will have an announcement post!In this post we will cover a few key areas:False positive vulnerabilitiesNightly buildsRaft self-healing and cleanupMinimizing restart impactP99 performanceFor the entirety of the improvements and fixes, check out therelease
notes.ü™Ü False positive vulnerabilitiesAreport from last monthindicated that security vulnerability scanners, such asGrypeandTrivy, would report the server had an active CVE or would not pass due to a ‚Äúmalformed version‚Äù.We eventually discovered that usinggo build .rather thango build main.goresulted in Go not listing the package itself as a dependency with the(devel)version which was tripping up the vulnerability scanners.Although a subtle change, after updating this, there are no more false positives. Given the increasing sensitivity of vulnerable dependencies and supply chain attacks, we take reports like these very seriously and want to ensure even false positives are remediated.Relevant PRs#3993üèóÔ∏è Nightly buildsAs part of this development cycle, a new nightly build based off of themainbranch was introduced, which follows the current minor release series, such as2.9.x. This was driven by user demand to be able to try and test out recently-landed server PRs as they are merged throughout the development cycle in preparation for the next patch release.Making this available has been invaluable for the NATS team since it shortens the feedback loop for users actively engaged and testing changes in their own environments.The build is available as aDocker imageunder thesynadiaDocker Hub organization with thenightly-maintag:$ docker run -p 4222:4222 synadia/nats-server:nightly-main -jsFor those that were not aware, there is anexisting Docker imagehaving thenightlytag based on thedevbranch which tracks the nextminorversion, in this case2.10.0.$ docker run -p 4222:4222 synadia/nats-server:nightly -jsRelevant PRs#3961#3972#4019ü™µ Raft self-healing and cleanupOne of the NATS core pillars is for the server to beself-healing. Specifically, the ability to preserve itself against slow or rogue clients, and recover in the face of partial failure.The JetStream subsystem, managing streams and consumers, relies on acustom Raft implementationbuilt on top of Core NATS itself enhanced by two distinctive features:Optimized hand-offs using cooperative step-down on leader transfers to minimize interruptionDetection of storage faults and corruption using checksums on read and writesThis release brings additional fixes and improvements making this self-healing attribute more robust.One fix addressed a memory leak that could occur if the JetStream subsystem shut down on a server, making it inactive from the cluster‚Äôs standpoint, but the server itself was still holding on to resources.A second improvement detects servers inlame duck modeand omits them as possible leaders for Raft groups. Although the previous behavior would eventually correct itself once the server finally shut down, this optimizes the leader election so that a server that is about to shut down will not be selected, reducing impact to active JetStream assets.Finally, an uncommon split-brain failure mode, resulting in two separate leaders trying to run for the same asset leadership term, has been addressed. Previously, manual intervention was required to forcestep-downone of the leaders, but this is now performed automatically when detected.Relevant PRs#3999#4049#4002#4056‚è≥ Minimizing restart impactFor NATS deployments that have hundreds or thousands (or more) streams and consumers with high loads, a server restart (or failure) can be potentially disruptive in production.When a clustered server goes offline whilst running as replica leaders, although leader election is handled transparently, all of the load will ultimately shift to other servers, increasing their load further. When the offline server comes back online (or is replaced), it will need to be deemedhealthybefore it is possible to shift leaders and load back to this server.The single entrypoint for determining server health is the/healthzmonitoring endpoint. For Kubernetes-based deployments, this is the endpoint used for the readiness and liveness probes (using different parameters). For servers having many assigned assets, becoming healthy forallassets could take a significant amount of time, delaying the server from being an active participant for the assets that were asserted to be healthy.A key improvement in this release is to provide more granular, per asset checking to prevent blocking healthy assets from participating in their Raft group.As noted above, for high throughput workloads, the amount of data the server needs to churn through may not be small, so optimizing this path can be critical for reducing startup times and aiding cluster recovery.A second improvement is to better handle the case that recovering the local state for a JetStream asset had stalled (usually resulting inCatching up for server X stalledwarnings), instead resetting the log and recovering with the assistance of other healthy servers in the cluster.Previous manual intervention required scaling down a stream to R1, effectively wiping local replica state, followed by scaling back up, causing a fresh copy to sync from the leader.Thisstalldetection will reduce the time for a server to become healthy (via/healthz), thus minimizing the impact on planned or unplanned restarts.Relevant PRs#4031#4058‚ö° P99 performanceSynadiawas fortunate to work with a customer in the financial services industry measuring JetStream publish latency. They cared deeply about 99th percentile (P99) publish latencies, keeping the variance (even at peak load) as low as possible.Their setup consisted of ~200 R3 streams spread out across a private network tying together all three major public clouds. Their workload had peak publish throughput of ~180-200k messages/second.After their own evaluation, they discovered their P99 metric had some publish latencies reachingseveral seconds. To level-set, in this context, P99 refers to the 1%worstpublish latencies that were measured. The other 99% of measured publish latencies were in range of their needs.The quality engineering team modeled the workload with a similar topology, number streams, etc. and added it as part of the load testing suite. Throughout the development cycle, this workload was run repeatedly to ensure proposed changes were in fact improving performance.After a day‚Äôs worth of changes were incorporated, the customer was able to pull a nightly build and re-test within their own environment, resulting in a productive daily feedback loop. üôåThe range of improvements and optimizations varied, but by the end of this journey, the team was able to reduce the P99 publish latency to under30msat peak load.üìù As an obligatory disclaimer, performance numbers are very workload and environment-specific. The takeaway from this section should be themagnitude of improvementthat was achieved as a result of this effort.Relevant PRs#3965#3981#4022ConclusionThe NATS team is incredibly proud of this release and reinforces the impact the new quality engineering process provides (and now our nightly builds) in optimizing the feedback loop for discovering, assessing, and addressing fixes and improvements.As always, refer to thedownload pagefor direct links to the GitHub release page and the official Docker image.About the AuthorByron Ruthis the Director of Developer Relations atSynadiaand a long-time NATS user.Back to BlogCopyright ¬© NATS Authors 2024NATS is aCloud Native Computing Foundationincubating projectThe Linux Foundation has registered trademarks and uses trademarks.For a list of trademarks of The Linux Foundation, please seeTrademark Usage page.