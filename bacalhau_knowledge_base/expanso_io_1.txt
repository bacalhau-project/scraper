Source: expanso_io
URL: https://expanso.io/use-cases/log-processing/

Skip to contentWhy Expanso?Why Choose Expanso?Security and GovernanceProductBacalhauFeaturesSolutionsUse CasesDistributed Data WarehouseLog ProcessingEdge Machine LearningDistributed Fleet ManagementResourcesDocumentationGitHubBacalhau BlogEventsFAQsHelp CenterCompanyWhat We ValueWho We AreCareersNewsroomContactGitHubInstall FreeMenuWhy Expanso?Why Choose Expanso?Security and GovernanceProductBacalhauFeaturesSolutionsUse CasesDistributed Data WarehouseLog ProcessingEdge Machine LearningDistributed Fleet ManagementResourcesDocumentationGitHubBacalhau BlogEventsFAQsHelp CenterCompanyWhat We ValueWho We AreCareersNewsroomContactGitHubInstall FreeInstall FreeUse CasesLog ProcessingLog VendingIntroductionAWS is one of the largest deployments of compute services in the world. Like anyone of significant size, in order to maintain quality of service and reliability, they have to take as much signal from their services, often in the form of logs. In order to deliver high reliability services, they had to handle petabytes of logs generated every hour across Amazon’s massive global infrastructure supporting millions of customers.Their users had diverse needs – real-time access to logs, long-term storage for compliance and efficient storage for large-scale batch processing – all without burning a hole in their pocket. The traditional approach, a centralized data lake, would have been prohibitively expensive, less efficient and a nightmare to operate. So, they went distributed. They would process logs where they were generated and orchestrated multiple delivery streams tailored to specific needs, whether that be real-time access or optimized storage for batch processing or long-term retention.Thinking of achieving this level of efficiency and cost-saving outside of AWS? Our open-source product Bacalhau is the solution.Challenges in traditional log vending.Organizations today juggle the need for detailed logs with the challenges of managing massive data volumes. Platforms like Splunk, Data Dog, and others offer rich features, but costs spike with increased data intake. Key log management challenges across industries include the following.VOLUME VS. VALUEMost log ingestion involves write-intensive operations, but only a minor fraction of these logs are accessed or deemed valuable.REAL-TIME NEEDSCritical applications like threat detection and system health checks rely on specific metrics. Real-time metric solutions tend to be costlier and harder to oversee. Hence, early aggregation and filtering of these metrics are essential for both cost savings and operational scalability.OPERATIONAL INSIGHTSAt times, operators require access to application and server logs for immediate troubleshooting. This access should be facilitated without connecting directly to the numerous production servers producing these logs.ARCHIVAL NEEDSPreserving raw logs is crucial for compliance, audits, and various analytical and operational improvement processes.To address these varied needs, especially in real-time scenarios, many organizations resort to streaming all logs to a centralized platform or data warehouse. While this method can work, it often leads to spiraling costs and slower insights.Solution: Distributed Log OrchestrationBacalhau is a distributed compute framework that offers efficient log processing solutions, enhancing current platforms. Its strength lies in its adaptable job orchestration. Let’s explore its multi-job approach:Daemon Jobs:Purpose: Bacalhau agents run continuously on selected nodes, auto-deployed by the orchestrator.Function: These jobs handle logs at the source, aggregate, and compress them. They then send aggregated logs periodically to platforms like Kafka or Kinesis, or suitable logging services. Every hour, raw logs intended for archiving or batch processing move to places like S3.Service Jobs:Purpose: Handle continuous intermediate processing like log aggregation, basic statistics, deduplication, and issue detection. They run on specific nodes, with Bacalhau ensuring their optimal performance.Function: Continuous log processing and integration with logging services for instant insights, e.g., Splunk.Batch Jobs:Purpose: Run on-demand on chosen nodes, with Bacalhau managing node choice and task monitoring.Function: Operates intermittently on data in S3, focusing on in-depth investigations without moving the data, turning nodes into a distributed data warehouse.Ops Jobs:Purpose: Similar to batch jobs but spanning all fitting nodes.Function: Executes ad-hoc queries across all jobs in a network, allowing end-users limited log access on host machines, avoiding any S3 transfer delays and ensuring rapid insights.Bacalhau ensures log management is not only efficient but also responsive to changing business needs.Bacalhau's global edge.Bacalhau is designed for global reach and reliability. Here’s a snapshot of its worldwide log solution:LOCAL LOG TRANSFERSDaemon jobs swiftly send logs to close-by storages like regional S3 or MinIO. They stay active, even without Bacalhau connection, safeguarding data during outages.REGIONAL LOG HANDLINGAutonomous service jobs in each region channel logs to either local or global platforms, preserving metrics when the network’s down.SMART BATCH OPERATIONSBacalhau guides batch jobs to nearby data sources, cutting network costs and streamlining global tasks.OPS JOBS FLEXIBILITYBased on permissions, operators can target specific hosts, regions, or the entire network for queries.With Bacalhau, global log management is both efficient and user-friendly, marrying the perks of decentralization with centralized clarity.BENCHMARKINGIn the following we have calculated the cost for a case where one uploads the data directly to Splunk and processes the data there. In the second scenario data is preprocessed on-device, uploaded both to an archive server and a S3 bucket with a EC2 unit in AWS. The savings are higher than 99%.EnvironmentFleetTPS Per Host10,000r/sNumber of Hosts30Raw LogsAverage Nginx Access Log Size325BytesPer Host Hourly Logs11GBPer Host Hourly Compressed Logs (zstd)0.8GBFleet Wide Daily Raw Logs7,845GBFleet Wide Daily Compressed Raw Logs541GBAggregated LogsAverage Aggregated Event Size170BytesAggregation Window10secondsMetrics Per Aggregation Window100Per Host Hourly Aggregated Logs0.006GBFleet Wide Daily Aggregated Logs4GBFleet Wide TPS300EventsSplunk Cloud Ingestion Prices (Ref)PlanAnnual TotalGB/DaySplunk annual 5GB/Day$8,100$4.4Splunk annual 10GB/Day$13,800$3.8Splunk annual 20GB/Day$24,000$3.3Splunk annual 50GB/Day$50,000$2.7Splunk annual 100GB/Day$80,000$2.2Assuming 60% Discount For Higher Volume$32,000$0.9Direct to SplunkBacalhau Pre-ProcessingSplunk Daily Ingestion (GB)7,8454Splunk Annual Cost$2,510,548$6,648S3 Monthly New Storage (GB)16,232S3 Monthly PUTs21,600S3 Monthly GETs64,800S3 Annual Standard Storage (first month)$373S3 Annual Infrequent Access (next 2 months)$406S3 Annual Glacier Instant Retrieval (next 9 months)$584S3 Annual PUTs Cost$1.30S3 Annual GETs Cost$0.31S3 Annual Cost$1,365EC2 Monthly Orchestrator m7g.medium Instances (x3)$90EC2 Monthly Compute m7g.large Instances (x3)$251EC2 Annual Cost$4,091Kinesis Monthly Shard Cost (x3 Shards)$33Kinesis Monthly PUT Units Cost$11Kinesis Annual Cost$527Total Annual Cost$2,510,548$12,104Cost Saving99.52%Log Context:We benchmarked using web server access logs, common tools to monitor server health, user activity, and security. Our goal was to mirror real-world situations for wide applicability.Setup and Capacity:We simulated 30 web servers across multiple zones, each handling about 10k TPS (Transactions Per Second) of web requests. This provides a solid foundation for gauging Bacalhau's log processing prowess.Platform Considerations:Our tests were on AWS, but Bacalhau's flexibility means similar results on Google Cloud, Azure, or on-premises systems.MAIN TAKEAWAYSBANDWIDTH EFFICIENCYBacalhau cut the bandwidth for log transfers from 11GB to 800MB per host hourly – a ~93% reduction, maintaining data integrity and real-time metrics.COST SAVINGSDirect logs to a service like Splunk would cost roughly $2.5 million annually. Bacalhau’s approach? A mere $12k per year, marking a cost slash of over 99%.REAL-TIME INSIGHTSUsing aggregated data, we crafted detailed dashboards for monitoring web services, tracking traffic shifts, and spotting suspicious users.ADDED PERKSWith Bacalhau, benefits span faster threat detection using Kinesis, intricate batch tasks with raw logs in S3, and long-term storage in Glacier. Plus, you can keep using your go-to log visualization and alert tools.Step 0 - PrerequisitesBacalhau CLI installed. If you haven’t yet, followthis guide.AWS CDK CLI. You can find more infohere.An active AWS account (any other cloud provider works too, but the commands will be different).In this example, your cluster will include:A Bacalhau orchestrator EC2 instanceThree EC2 instances as web servers running Bacalhau agentsAn S3 bucket for raw log storageAn OpenSearch cluster with a pre-configured dashboard and visualizationsStep 1 - Cluster deployment with CDKStep-by-Step DeploymentFirst, clone the GitHub repository:git clonecd log-orchestration/cdkNow, install the required Node.js packages:npm installBootstrap your AWS account if you haven’t used AWS CDK on your account already:cdk bootstrapTo deploy your stack without SSH access, run:cdk deployNeed SSH access to your hosts? Use this instead:cdk deploy --context keyName=Note:If you don’t have an SSH key pair, followthesesteps. Deployment will take a few minutes. Perfect time for a quick coffee break!Step 2 - CDK OutputsOnce the stack is deployed, you’ll receive these outputs:OrchestratorPublicIp: Connect Bacalhau CLI to this IP.OpenSearchEndpoint: The endpoint for aggregated logs.BucketName: The S3 bucket for raw logs.OpenSearchDashboard: Access your OpenSearch dashboard here.OpenSearchPasswordRetriever:Retrieve OpenSearch master password with this command.Step 3 - Access Bacalhau networkTo configure your Bacalhau CLI, execute:export BACALHAU_NODE_CLIENTAPI_HOST=Verify your setup with:bacalhau node listYou should see three compute nodes labeledservice=web-serveralong with the orchestrator node.ID        TYPE       LABELS                                              CPU     MEMORY      DISK         GPU
 QmUWYeTV  Compute    Architecture=amd64 Operating-System=linux           1.6 /   1.5 GB /    78.3 GB /    0 /
                      git-lfs=False name=web-server-1 service=web-server  1.6     1.5 GB      78.3 GB      0
 QmVBdXFW  Compute    Architecture=amd64 Operating-System=linux           1.6 /   1.5 GB /    78.3 GB /    0 /
                      git-lfs=False name=web-server-3 service=web-server  1.6     1.5 GB      78.3 GB      0
 QmWaRH4X  Requester  Architecture=amd64 Operating-System=linux
                      git-lfs=False
 QmXVWfVT  Compute    Architecture=amd64 Operating-System=linux           1.6 /   1.5 GB /    78.3 GB /    0 /
                      git-lfs=False name=web-server-2 service=web-server  1.6     1.5 GB      78.3 GB      0Step 4 - Accessing OpenSearch dashboardAfter your CDK stack is up and running, the OpenSearch dashboard URL will pop up in your console, courtesy of the CDK outputs. You’ll hit a login page the first time you try to access the dashboard. No sweat, just useadminas the username.To get your password, you don’t have to hunt; CDK outputs include a handy command tailored for this. Just fire up your terminal and run:aws secretsmanager get-secret-value --secret-id "" --query 'SecretString' --output textSwap<Secret ARN>with the actual ARN displayed in your CDK outputs.Logged in successfully? Fantastic, let’s proceed!Step 5 - Deploy log generatorWe’ve prepared a[log-generator.yaml](<https://github.com/bacalhau-project/examples/blob/main/log-orchestration/jobs/log-generator.yaml>)file for deploying a daemon job to simulate web access logs.Name: LogGenerator
Type: daemon
Namespace: logging
Constraints:
  - Key: service
    Operator: ==
    Values:
      - web-server
Tasks:
  - Name: main
    Engine:
      Type: docker
      Params:
        Image: expanso/nginx-access-log-generator:1.0.0
        Parameters:
          - --rate
          - "10"
          - --output-log-file
          - /app/output/application.log
    Resources:
      CPU: 0.1
      Memory: 512MB
    InputSources:
      - Target: /app/output
        Source:
          Type: localDirectory
          Params:
            SourcePath: /data/log-orchestration/logs
            ReadWrite: trueWhat’s Happening: The daemon job is deployed on all nodes with the labelservice=web-server. The job employs a Docker image that mimics Nginx access logs and stores them locally.Deploy it like so:bacalhau job run log-generator.yamlCheck the job’s status:bacalhau job executionsYou should see three executions in your three web servers inRunningstate:CREATED   MODIFIED  ID          NODE ID   REV.  COMPUTE     DESIRED  COMMENT 
                                                 STATE       STATE            
 12:43:31  12:43:31  e-d5433aec  QmUWYeTV  2     BidAccepte  Running          
                                                 d                            
 12:43:31  12:43:31  e-a7410744  QmVBdXFW  2     BidAccepte  Running          
                                                 d                            
 12:43:31  12:43:31  e-d6868bd2  QmXVWfVT  2     BidAccepte  Running          
                                                 dStep 6 - Deploy logging agentBacalhau’s versatility means that you can deploy any type of job, use any existing logging agent or implement your own. We’ll use Logstash for this example.Take a look at[logstash.yaml](<https://github.com/bacalhau-project/examples/blob/main/log-orchestration/jobs/logstash.yaml>).Name: Logstash
Type: daemon
Namespace: logging
Constraints:
  - Key: service
    Operator: ==
    Values:
      - web-server
Tasks:
  - Name: main
    Engine:
      Type: docker
      Params:
        Image: expanso/nginx-access-log-agent:1.0.0
        EnvironmentVariables:
          - OPENSEARCH_ENDPOINT=- S3_BUCKET=- AWS_REGION=- AGGREGATE_DURATION=10
    Resources:
      CPU: 0.5
      Memory: 1Gi
    Network: 
      Type: Full
    InputSources:
      - Target: /app/logs
        Source:
          Type: localDirectory
          Params:
            SourcePath: /data/log-orchestration/logs
      - Target: /app/state
        Source:
          Type: localDirectory
          Params:
            SourcePath: /data/log-orchestration/state
            ReadWrite: trueDon’t Forget: Update<OpenSearchEndpoint>,<S3BucketName>, and<AWSRegion>according to your CDK outputs. Deploy your logging agent:bacalhau job run logstash.yamlLogstash might need a few moments to get up and running. To keep tabs on its start-up progress, you can use:bacalhau logs --followStep 7 - How the logging agent worksIn our setup, we’re using theexpanso/nginx-access-log-agentDocker image. Here’s a quick rundown of what you get out of the box with this choice:Raw Logs to S3: Every hour, compressed raw logs are sent to your specified S3 bucket. This is great for archival and deep-dive analysis.Real-time Metrics to OpenSearch: The agent pushes aggregated metrics to OpenSearch everyAGGREGATE_DURATIONseconds (e.g., every 10 seconds).You can learn more about our Logstash pipeline configuration and aggregation implementationhere. These are a subset of the aggregated metrics published to OpenSearch:Request Counts: Grouped by HTTP status codes.Top IPs: Top 10 source IPs by request count.Geo Sources: Top 10 geographic locations by request count.User Agents: Top 10 user agents by request count.Popular APIs & Pages: Top 10 most-hit APIs and pages.Gone Pages: Top 10 requested but non-existent pages.Unauthorized IPs: Top 10 IPs failing authentication.Throttled IPs: Top 10 IPs getting rate-limited.Data Volume: Total data transmitted in bytes.This gives you real-time insights into traffic patterns, performance issues, and potential security risks. All of this is visible through your OpenSearch dashboards.After a while, your OpenSearch dashboard will be bursting with insights. Metrics on user behavior, traffic hotspots, error rates, you name it!FURTHER INFORMATIONCAVEATYour mileage may vary depending on the specific types of logs, the volume, and your existing infrastructure. You can use thiscalculatorto get an estimate of your cost saving. Compression and decompression have CPU costs. But given that we’re saving significantly on data transfer and storage, it’s often a cost well worth incurring.So there you have it. With Bacalhau, setting up a robust log management system is pretty much a walk in the park. But hey, we’re just scratching the surface here. The framework’s adaptability and resilience make it a must-have tool for any enterprise aiming to keep their log data in check.ConclusionWith Bacalhau, setting up a robust log management system is pretty much a walk in the park. But, we’re just scratching the surface here. The framework’s adaptability and resilience make it a must-have tool for any enterprise aiming to keep their log data in check.By decentralizing the log processing system, Bacalhau not only vastly reduces operational costs – as evidenced by our benchmarking results – but also ensures real-time data processing and compliance with archival needs. Its seamless integration across various cloud platforms, including AWS, Azure, and Google Cloud, demonstrates our commitment to versatile, cross-platform solutions.While Bacalhau is open source software, the Bacalhau binaries go through the security, verification, and signing build process lovingly crafted byExpanso. You can read more about the difference between open source Bacalhau and commercially supported Bacalhau in ourFAQ. If you would like to use our pre-built binaries and receive commercial support, pleasecontact us.Feeling inspired to get started?For more information, read moreUse Casesor visit ourDocumentation.
For an in-depth exploration, visit ourGetting Started Tutorial.Install FreeContact UsStay up to date and sign up to our blog:Visit BlogThe Compute Over Data PlatformCompanyHomeWhat We ValueWho We AreNewsroomContact UsCareersResourcesDocumentationFAQBlogSupportSocialLinkedInYouTubeSlackX ExpansoX BacalhauGitHub© Expanso Inc. All Rights Reserved 2023Privacy PolicyTrademarksTerms & ConditionsPrivacy PolicyTrademarksTerms & Conditions