URL: https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html

Wiki|git|Apache Hadoop| Last Published: 2024-03-04
               | Version: 3.4.0GeneralOverviewSingle Node SetupCluster SetupCommands ReferenceFileSystem ShellCompatibility SpecificationDownstream Developer's GuideAdmin Compatibility GuideInterface ClassificationFileSystem SpecificationCommonCLI Mini ClusterFair Call QueueNative LibrariesProxy UserRack AwarenessSecure ModeService Level AuthorizationHTTP AuthenticationCredential Provider APIHadoop KMSTracingUnix Shell GuideRegistryAsync ProfilerHDFSArchitectureUser GuideCommands ReferenceNameNode HA With QJMNameNode HA With NFSObserver NameNodeFederationViewFsViewFsOverloadSchemeSnapshotsEdits ViewerImage ViewerPermissions and HDFSQuotas and HDFSlibhdfs (C API)WebHDFS (REST API)HttpFSShort Circuit Local ReadsCentralized Cache ManagementNFS GatewayRolling UpgradeExtended AttributesTransparent EncryptionMultihomingStorage PoliciesMemory Storage SupportSynthetic Load GeneratorErasure CodingDisk BalancerUpgrade DomainDataNode AdminRouter FederationProvided StorageMapReduceTutorialCommands ReferenceCompatibility with 1.xEncrypted ShufflePluggable Shuffle/SortDistributed Cache DeploySupport for YARN Shared CacheMapReduce REST APIsMR Application MasterMR History ServerYARNArchitectureCommands ReferenceCapacity SchedulerFair SchedulerResourceManager RestartResourceManager HAResource ModelNode LabelsNode AttributesWeb Application ProxyTimeline ServerTimeline Service V.2Writing YARN ApplicationsYARN Application SecurityNodeManagerRunning Applications in Docker ContainersRunning Applications in runC ContainersUsing CGroupsSecure ContainersReservation SystemGraceful DecommissionOpportunistic ContainersYARN FederationShared CacheUsing GPUUsing FPGAPlacement ConstraintsYARN UI2YARN REST APIsIntroductionResource ManagerNode ManagerTimeline ServerTimeline Service V.2YARN ServiceOverviewQuickStartConceptsYarn Service APIService DiscoverySystem ServicesHadoop Compatible File SystemsAliyun OSSAmazon S3Azure Blob StorageAzure Data Lake StorageTencent COSHuaweicloud OBSAuthOverviewExamplesConfigurationBuildingToolsHadoop StreamingHadoop ArchivesHadoop Archive LogsDistCpHDFS Federation BalanceGridMixRumenResource Estimator ServiceScheduler Load SimulatorHadoop BenchmarkingDynamometerReferenceChangelog and Release NotesJava API docsUnix Shell APIMetricsConfigurationcore-default.xmlhdfs-default.xmlhdfs-rbf-default.xmlmapred-default.xmlyarn-default.xmlkms-default.xmlhttpfs-default.xmlDeprecated PropertiesHadoop: Setting up a Single Node Cluster.PurposePrerequisitesSupported PlatformsRequired SoftwareInstalling SoftwareDownloadPrepare to Start the Hadoop ClusterStandalone OperationPseudo-Distributed OperationConfigurationSetup passphraseless sshExecutionYARN on a Single NodeFully-Distributed OperationPurposeThis document describes how to set up and configure a single-node Hadoop installation so that you can quickly perform simple operations using Hadoop MapReduce and the Hadoop Distributed File System (HDFS).Important: all production Hadoop clusters use Kerberos to authenticate callers and secure access to HDFS data as well as restriction access to computation services (YARN etc.).These instructions do not cover integration with any Kerberos services, -everyone bringing up a production cluster should include connecting to their organisation’s Kerberos infrastructure as a key part of the deployment.SeeSecurityfor details on how to secure a cluster.PrerequisitesSupported PlatformsGNU/Linux is supported as a development and production platform. Hadoop has been demonstrated on GNU/Linux clusters with 2000 nodes.Required SoftwareRequired software for Linux include:Java™ must be installed. Recommended Java versions are described atHadoopJavaVersions.ssh must be installed and sshd must be running to use the Hadoop scripts that manage remote Hadoop daemons if the optional start and stop scripts are to be used. Additionally, it is recommmended that pdsh also be installed for better ssh resource management.Installing SoftwareIf your cluster doesn’t have the requisite software you will need to install it.For example on Ubuntu Linux:$ sudo apt-get install ssh
  $ sudo apt-get install pdshDownloadTo get a Hadoop distribution, download a recent stable release from one of theApache Download Mirrors.Prepare to Start the Hadoop ClusterUnpack the downloaded Hadoop distribution. In the distribution, edit the fileetc/hadoop/hadoop-env.shto define some parameters as follows:# set to the root of your Java installation
  export JAVA_HOME=/usr/java/latestTry the following command:$ bin/hadoopThis will display the usage documentation for the hadoop script.Now you are ready to start your Hadoop cluster in one of the three supported modes:Local (Standalone) ModePseudo-Distributed ModeFully-Distributed ModeStandalone OperationBy default, Hadoop is configured to run in a non-distributed mode, as a single Java process. This is useful for debugging.The following example copies the unpacked conf directory to use as input and then finds and displays every match of the given regular expression. Output is written to the given output directory.$ mkdir input
  $ cp etc/hadoop/*.xml input
  $ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.4.0.jar grep input output 'dfs[a-z.]+'
  $ cat output/*Pseudo-Distributed OperationHadoop can also be run on a single-node in a pseudo-distributed mode where each Hadoop daemon runs in a separate Java process.ConfigurationUse the following:etc/hadoop/core-site.xml:<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>etc/hadoop/hdfs-site.xml:<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>Setup passphraseless sshNow check that you can ssh to the localhost without a passphrase:$ ssh localhostIf you cannot ssh to localhost without a passphrase, execute the following commands:$ ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
  $ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
  $ chmod 0600 ~/.ssh/authorized_keysExecutionThe following instructions are to run a MapReduce job locally. If you want to execute a job on YARN, seeYARN on Single Node.Format the filesystem:$ bin/hdfs namenode -formatStart NameNode daemon and DataNode daemon:$ sbin/start-dfs.shThe hadoop daemon log output is written to the$HADOOP_LOG_DIRdirectory (defaults to$HADOOP_HOME/logs).Browse the web interface for the NameNode; by default it is available at:NameNode -http://localhost:9870/Make the HDFS directories required to execute MapReduce jobs:$ bin/hdfs dfs -mkdir -p /user/<username>Copy the input files into the distributed filesystem:$ bin/hdfs dfs -mkdir input
  $ bin/hdfs dfs -put etc/hadoop/*.xml inputRun some of the examples provided:$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.4.0.jar grep input output 'dfs[a-z.]+'Examine the output files: Copy the output files from the distributed filesystem to the local filesystem and examine them:$ bin/hdfs dfs -get output output
  $ cat output/*orView the output files on the distributed filesystem:$ bin/hdfs dfs -cat output/*When you’re done, stop the daemons with:$ sbin/stop-dfs.shYARN on a Single NodeYou can run a MapReduce job on YARN in a pseudo-distributed mode by setting a few parameters and running ResourceManager daemon and NodeManager daemon in addition.The following instructions assume that 1. ~ 4. steps ofthe above instructionsare already executed.Configure parameters as follows:etc/hadoop/mapred-site.xml:<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>mapreduce.application.classpath</name>
        <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</value>
    </property>
</configuration>etc/hadoop/yarn-site.xml:<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_HOME,PATH,LANG,TZ,HADOOP_MAPRED_HOME</value>
    </property>
</configuration>Start ResourceManager daemon and NodeManager daemon:$ sbin/start-yarn.shBrowse the web interface for the ResourceManager; by default it is available at:ResourceManager -http://localhost:8088/Run a MapReduce job.When you’re done, stop the daemons with:$ sbin/stop-yarn.shFully-Distributed OperationFor information on setting up fully-distributed, non-trivial clusters seeCluster Setup.©            2008-2024
              Apache Software Foundation
            
                          -Privacy Policy.
        Apache Maven, Maven, Apache, the Apache feather logo, and the Apache Maven project logos are trademarks of The Apache Software Foundation.