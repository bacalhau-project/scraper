URL: https://etcd.io/docs/v3.6/learning/design-learner/

etcdDocsBlogCommunityInstallPlayVersionsv3.6v3.5v3.4v3.3v3.2v3.1v2.3Versionsv3.6-DRAFTQuickstartDemoTutorialsHow to Set Up a Demo etcd ClusterReading from etcdWriting to etcdHow to get keys by prefixHow to delete keysHow to make multiple writes in a transactionHow to watch keysHow to create leaseHow to create locksHow to conduct leader election in etcd clusterHow to check Cluster statusHow to save the databaseHow to migrate etcd from v2 to v3How to Add and Remove MembersInstallFAQLibraries and toolsMetricsReporting bugsTuningDiscovery service protocolLogging conventionsGolang modulesLearningData modeletcd client designetcd learner designetcd v3 authentication designetcd APIetcd persistent storage filesetcd API guaranteesetcd versus other key-value storesGlossaryDeveloper guideDiscovery service protocolSet up a local clusterInteracting with etcdWhy gRPC gatewaygRPC naming and discoverySystem limitsetcd featuresAPI referenceAPI reference: concurrencyOperations guideAuthentication GuidesRole-based access controlAuthenticationConfiguration optionsTransport security modelClustering GuideRun etcd clusters as a Kubernetes StatefulSetRun etcd clusters inside containersFailure modesDisaster recoveryetcd gatewaygRPC proxyHardware recommendationsMaintenanceMonitoring etcdPerformanceDesign of runtime reconfigurationRuntime reconfigurationSupported platformsVersioningData CorruptionBenchmarksStorage Memory Usage BenchmarkWatch Memory Usage BenchmarkBenchmarking etcd v3Benchmarking etcd v2.2.0-rc-memoryBenchmarking etcd v2.2.0-rcBenchmarking etcd v2.2.0Benchmarking etcd v2.1.0UpgradingUpgrading etcd clusters and applicationsUpgrade etcd from 3.4 to 3.5Upgrade etcd from 3.3 to 3.4Upgrade etcd from 3.2 to 3.3Upgrade etcd from 3.1 to 3.2Upgrade etcd from 3.0 to 3.1Upgrade etcd from 2.3 to 3.0TriageIssue triage guidelinesPR managementv3.5QuickstartDemoTutorialsHow to Set Up a Demo etcd ClusterReading from etcdWriting to etcdHow to get keys by prefixHow to delete keysHow to make multiple writes in a transactionHow to watch keysHow to create leaseHow to create locksHow to conduct leader election in etcd clusterHow to check Cluster statusHow to save the databaseHow to migrate etcd from v2 to v3How to Add and Remove MembersInstallFAQLibraries and toolsMetricsReporting bugsTuningDiscovery service protocolLogging conventionsGolang modulesLearningData modeletcd client designetcd learner designetcd v3 authentication designetcd APIetcd persistent storage filesetcd API guaranteesetcd versus other key-value storesGlossaryDeveloper guideDiscovery service protocolSet up a local clusterInteracting with etcdWhy gRPC gatewaygRPC naming and discoverySystem limitsetcd featuresAPI referenceAPI reference: concurrencyOperations guideAuthentication GuidesRole-based access controlAuthenticationConfiguration optionsTransport security modelClustering GuideRun etcd clusters as a Kubernetes StatefulSetRun etcd clusters inside containersFailure modesDisaster recoveryetcd gatewaygRPC proxyHardware recommendationsMaintenanceMonitoring etcdPerformanceDesign of runtime reconfigurationRuntime reconfigurationSupported platformsVersioningData CorruptionBenchmarksStorage Memory Usage BenchmarkWatch Memory Usage BenchmarkBenchmarking etcd v3Benchmarking etcd v2.2.0-rc-memoryBenchmarking etcd v2.2.0-rcBenchmarking etcd v2.2.0Benchmarking etcd v2.1.0DowngradingDowngrading etcd clusters and applicationsDowngrade etcd from 3.5 to 3.4UpgradingUpgrading etcd clusters and applicationsUpgrade etcd from 3.4 to 3.5Upgrade etcd from 3.3 to 3.4Upgrade etcd from 3.2 to 3.3Upgrade etcd from 3.1 to 3.2Upgrade etcd from 3.0 to 3.1Upgrade etcd from 2.3 to 3.0TriageIssue triage guidelinesPR managementv3.4QuickstartOverviewDemoInstallFAQLibraries and toolsMetricsReporting bugsTuningDiscovery service protocolLogging conventionsLearningData modeletcd client designetcd learner designetcd v3 authentication designetcd3 APIetcd API guaranteesetcd versus other key-value storesGlossaryDeveloper guideDiscovery service protocolSet up a local clusterInteracting with etcdWhy gRPC gatewaygRPC naming and discoverySystem limitsetcd featuresAPI referenceAPI reference: concurrencyOperations guideConfiguration optionsRole-based access controlTransport security modelClustering GuideRun etcd clusters inside containersFailure modesDisaster recoveryetcd gatewaygRPC proxyHardware recommendationsMaintenancePerformanceDesign of runtime reconfigurationRuntime reconfigurationSupported platformsMigrate applications from using API v2 to API v3VersioningData CorruptionMonitoring etcdBenchmarksStorage Memory Usage BenchmarkWatch Memory Usage BenchmarkBenchmarking etcd v3Benchmarking etcd v2.2.0-rc-memoryBenchmarking etcd v2.2.0-rcBenchmarking etcd v2.2.0Benchmarking etcd v2.1.0UpgradingUpgrading etcd clusters and applicationsUpgrade etcd from 3.4 to 3.5Upgrade etcd from 3.3 to 3.4Upgrade etcd from 3.2 to 3.3Upgrade etcd from 3.1 to 3.2Upgrade etcd from 3.0 to 3.1Upgrade etcd from 2.3 to 3.0PlatformsAmazon Web ServicesContainer Linux with systemdFreeBSDTriageIssue Triage Guidelinesv3.3InstallLibraries and toolsMetricsBenchmarksBenchmarking etcd v2.1.0Benchmarking etcd v2.2.0Benchmarking etcd v2.2.0-rcBenchmarking etcd v2.2.0-rc-memoryBenchmarking etcd v3Storage Memory Usage BenchmarkWatch Memory Usage BenchmarkDemoDeveloper guideDiscovery service protocoletcd API Referenceetcd concurrency API ReferenceExperimental APIs and featuresgRPC naming and discoveryInteracting with etcdSet up a local clusterSystem limitsWhy gRPC gatewayDiscovery service protocoletcd v3 APIFrequently Asked Questions (FAQ)Learningetcd client architectureClient feature matrixData modeletcd v3 authentication designetcd versus other key-value storesetcd3 APIGlossaryKV API guaranteesLearnerLogging conventionsOperations guideMonitoring etcdVersioningClustering GuideConfiguration flagsDesign of runtime reconfigurationDisaster recoveryetcd gatewayFailure modesgRPC proxyHardware recommendationsMaintenanceMigrate applications from using API v2 to API v3PerformanceRole-based access controlRun etcd clusters inside containersRuntime reconfigurationSupported systemsTransport security modelPlatformsAmazon Web ServicesContainer Linux with systemdFreeBSDProduction usersReporting bugsTuningUpgradingUpgrade etcd from 2.3 to 3.0Upgrade etcd from 3.0 to 3.1Upgrade etcd from 3.1 to 3.2Upgrade etcd from 3.2 to 3.3Upgrade etcd from 3.3 to 3.4Upgrade etcd from 3.4 to 3.5Upgrading etcd clusters and applicationsv3.2BenchmarksBenchmarking etcd v2.1.0Benchmarking etcd v2.2.0Benchmarking etcd v2.2.0-rcBenchmarking etcd v2.2.0-rc-memoryBenchmarking etcd v3-demoStorage Memory Usage BenchmarkWatch Memory Usage BenchmarkData modelDemoDeveloper guideDiscovery service protocoletcd API referenceetcd concurrency API ReferenceExperimental APIs and featuresgRPC gatewaygRPC naming and discoveryInteracting with etcdSet up a local clusterSystem limitsetcd dev internalDiscovery service protocolLogging conventionsetcd operations guideAuthentication GuideClustering GuideConfiguration flagsDesign of runtime reconfigurationDisaster recoveryetcd gatewaygRPC proxyHardware recommendationsMaintenanceMigrate applications from using API v2 to API v3Monitoring etcdPerformanceRun etcd clusters inside containersRuntime reconfigurationSecurity modelSupported platformsUnderstand failuresVersioningetcd upgradesUpgrade etcd from 2.3 to 3.0Upgrade etcd from 3.0 to 3.1Upgrade etcd from 3.1 to 3.2Upgrade etcd from 3.2 to 3.3Upgrade etcd from 3.3 to 3.4Upgrading etcd clusters and applicationsetcd v3 authentication designetcd versus other key-value storesetcd3 APIFrequently Asked Questions (FAQ)GlossaryInstallKV API guaranteesLibraries and toolsMetricsPlatformsAmazon Web ServicesFreeBSDRun etcd on Container Linux with systemdProduction usersReporting bugsRFCetcd v3 APITuningv3.1Data modelDemoetcd benchmarksetcd v2.1.0-alpha benchmarksetcd v2.2.0 benchmarksetcd v2.2.0-rc benchmarksetcd v2.2.0-rc-memory benchmarksetcd v3-demo benchmarksStorage Memory Usage BenchmarkWatch Memory Usage Benchmarketcd developer guideDiscovery service protocoletcd API ReferenceExperimental APIs and featuresgRPC GatewaygRPC naming and discoveryInteracting with etcdSetup a local clusterSystem limitsetcd internal devDiscovery service protocolLogging conventionsetcd operations guideClustering GuideConfiguration flagsDesign of runtime reconfigurationDisaster recoveryetcd gatewaygRPC proxyHardware recommendationsMaintenanceMigrate applications from using API v2 to API v3Monitoring etcdPerformanceRun etcd clusters inside containersRuntime reconfigurationSecurity modelSupported platformsUnderstand failuresVersioningetcd3 APIFrequently Asked Questions (FAQ)GlossaryInstallKV API guaranteesLibraries and toolsMetricsPlatformsFreeBSDProduction usersReporting bugsRFCetcd v3 APITuningUpgrading etcd clusters and applicationsUpgrade etcd from 2.3 to 3.0Upgrade etcd from 3.0 to 3.1Upgrade etcd from 3.1 to 3.2Upgrade etcd from 3.2 to 3.3Upgrade etcd from 3.3 to 3.4Why etcdv2.3AdministrationAuthentication GuideBackward CompatibilityBenchmarksBenchmarking etcd v2.2.0etcd 2.1.0-alpha benchmarksetcd 2.2.0-rc benchmarksetcd 2.2.0-rc memory benchmarksetcd 3 demo benchmarksStorage Memory Usage BenchmarkWatch Memory Usage BenchmarkClustering GuideConfiguration FlagsDesign of Runtime ReconfigurationDevelopmentDiscovery Service ProtocolError Codeetcd APIetcd v3 APIFAQGlossaryLibraries and ToolsMembers APIMetricsMiscellaneous APIsPlatformsFreeBSProduction UsersProxyReporting BugsRunning etcd under DockerRuntime ReconfigurationSecurity ModelSnapshot MigrationTuningUpgrade etcd from 2.1 to 2.2Upgrade etcd from 2.1 to 2.2Upgrade etcd from 2.2 to 2.3v2 Auth and SecurityVersioningView page sourceEdit this pageCreate child pageCreate documentation issueCreate project issue1. New Cluster member overloads Leader2. Network Partitions scenarios3. Cluster MisconfigurationsFeatures in v3.4Proposed features for future releasesVersionsv3.6-DRAFTLearningetcd learner designetcd learner designMitigating common challenges with membership reconfigurationetcd LearnerGyuho Lee (github.com/gyuho, Amazon Web Services, Inc.), Joe Betz (github.com/jpbetz, Google Inc.)BackgroundMembership reconfiguration has been one of the biggest operational challenges. Let’s review common challenges.1. New Cluster member overloads LeaderA newly joined etcd member starts with no data, thus demanding more updates from leader until it catches up with leader’s logs. Then leader’s network is more likely to be overloaded, blocking or dropping leader heartbeats to followers. In such case, a follower may election-timeout to start a new leader election. That is, a cluster with a new member is more vulnerable to leader election. Both leader election and the subsequent update propagation to the new member are prone to causing periods of cluster unavailability (seeFigure 1).2. Network Partitions scenariosWhat if network partition happens? It depends on leader partition. If the leader still maintains the active quorum, the cluster would continue to operate (seeFigure 2).2.1 Leader isolationWhat if the leader becomes isolated from the rest of the cluster? Leader monitors progress of each follower. When leader loses connectivity from the quorum, it reverts back to follower which will affect the cluster availability (seeFigure 3).When a new node is added to 3 node cluster, the cluster size becomes 4 and the quorum size becomes 3. What if a new node had joined the cluster, and then network partition happens? It depends on which partition the new member gets located after partition.2.2 Cluster Split 3+1If the new node happens to be located in the same partition as leader’s, the leader still maintains the active quorum of 3. No leadership election happens, and no cluster availability gets affected (seeFigure 4).2.3 Cluster Split 2+2If the cluster is 2-and-2 partitioned, then neither of partition maintains the quorum of 3. In this case, leadership election happens (seeFigure 5).2.4 Quorum LostWhat if network partition happens first, and then a new member gets added? A partitioned 3-node cluster already has one disconnected follower. When a new member is added, the quorum changes from 2 to 3. Now, this cluster has only 2 active nodes out 4, thus losing quorum and starting a new leadership election (seeFigure 6).Since member add operation can change the size of quorum, it is always recommended to “member remove” first to replace an unhealthy node.Adding a new member to a 1-node cluster changes the quorum size to 2, immediately causing a leader election when the previous leader finds out quorum is not active. This is because “member add” operation is a 2-step process where user needs to apply “member add” command first, and then starts the new node process (seeFigure 7).3. Cluster MisconfigurationsAn even worse case is when an added member is misconfigured. Membership reconfiguration is a two-step process: “etcdctl member add” and starting an etcd server process with the given peer URL. That is, “member add” command is applied regardless of URL, even when the URL value is invalid. If the first step is applied with invalid URLs, the second step cannot even start the new etcd. Once the cluster loses quorum, there is no way to revert the membership change (seeFigure 8).Same applies to a multi-node cluster. For example, the cluster has two members down (one is failed, the other is misconfigured) and two members up, but now it requires at least 3 votes to change the cluster membership (seeFigure 9).As seen above, a simple misconfiguration can fail the whole cluster into an inoperative state. In such case, an operator need manually recreate the cluster withetcd --force-new-clusterflag. As etcd has become a mission-critical service for Kubernetes, even the slightest outage may have significant impact on users. What can we better to make etcd such operations easier? Among other things, leader election is most critical to cluster availability: Can we make membership reconfiguration less disruptive by not changing the size of quorum? Can a new node be idle, only requesting the minimum updates from leader, until it catches up? Can membership misconfiguration be always reversible and handled in a more secure way (wrong member add command run should never fail the cluster)? Should an user worry about network topology when adding a new member? Can member add API work regardless of the location of nodes and ongoing network partitions?Raft LearnerIn order to mitigate such availability gaps in the previous section,Raft §4.2.1introduces a new node state “Learner”, which joins the cluster as anon-voting memberuntil it catches up to leader’s logs.Features in v3.4An operator should do the minimum amount of work possible to add a new learner node.member add --learnercommand to add a new learner, which joins cluster as a non-voting member but still receives all data from leader (seeFigure 10).When a learner has caught up with leader’s progress, the learner can be promoted to a voting member usingmember promoteAPI, which then counts towards the quorum (seeFigure 11).etcd server validates promote request to ensure its operational safety. Only after its log has caught up to leader’s can learner be promoted to a voting member (seeFigure 12).Learner only serves as a standby node until promoted: Leadership cannot be transferred to learner. Learner rejects client reads and writes (client balancer should not route requests to learner). Which means learner does not need issue Read Index requests to leader. Such limitation simplifies the initial learner implementation in v3.4 release (seeFigure 13).In addition, etcd limits the total number of learners that a cluster can have, and avoids overloading the leader with log replication. Learner never promotes itself. While etcd provides learner status information and safety checks, cluster operator must make the final decision whether to promote learner or not.Proposed features for future releasesMake learner state only and default: Defaulting a new member state to learner will greatly improve membership reconfiguration safety, because learner does not change the size of quorum. Misconfiguration will always be reversible without losing the quorum.Make voting-member promotion fully automatic: Once a learner catches up to leader’s logs, a cluster can automatically promote the learner. etcd requires certain thresholds to be defined by the user, and once the requirements are satisfied, learner promotes itself to a voting member. From a user’s perspective, “member add” command would work the same way as today but with greater safety provided by learner feature.Make learner standby failover node: A learner joins as a standby node, and gets automatically promoted when the cluster availability is affected.Make learner read-only: A learner can serve as a read-only node that never gets promoted. In a weak consistency mode, learner only receives data from leader and never process writes. Serving reads locally without consensus overhead would greatly decrease the workloads to leader but may serve stale data. In a strong consistency mode, learner requests read index from leader to serve latest data, but still rejects writes.Learner vs. Mirror Makeretcd implements “mirror maker” using watch API to continuously relay key creates and updates to a separate cluster. Mirroring usually has low latency overhead once it completes initial synchronization. Learner and mirroring overlap in that both can be used to replicate existing data for read-only. However, mirroring does not guarantee linearizability. During network disconnects, previous key-values might have been discarded, and clients are expected to verify watch responses for correct ordering. Thus, there is no ordering guarantee in mirror. Use mirror for minimum latency (e.g. cross data center) at the costs of consistency. Use learner to retain all historical data and its ordering.Appendix: Learner Implementation in v3.4Expose “Learner” node type to “MemberAdd” API.etcd client adds a flag to “MemberAdd” API for learner node. And etcd server handler applies membership change entry withpb.ConfChangeAddLearnerNodetype. Once the command has been applied, a server joins the cluster withetcd --initial-cluster-state=existingflag. This learner node can neither vote nor count as quorum.etcd server must not transfer leadership to learner, since it may still lag behind and does not count as quorum. etcd server limits the number of learners that cluster can have to one: the more learners we have, the more data the leader has to propagate. Clients may talk to learner node, but learner rejects all requests other than serializable read and member status API. This is for simplicity of initial implementation. In the future, learner can be extended as a read-only server that continuously mirrors cluster data. Client balancer must provide helper function to exclude learner node endpoint. Otherwise, request sent to learner may fail. Client sync member call should factor into learner node type. So should client endpoints update call.MemberListandMemberStatusresponses should indicate which node is learner.Add “MemberPromote” API.Internally in Raft, secondMemberAddcall to learner node promotes it to a voting member. Leader maintains the progress of each follower and learner. If learner has not completed its snapshot message, reject promote request. Only accept promote request if and only if: The learner node is in a healthy state. The learner is in sync with leader or the delta is within the threshold (e.g. the number of entries to replicate to learner is less than 1/10 of snapshot count, which means it is less likely that even after promotion leader would not need send snapshot to the learner). All these logic are hard-coded inetcdserverpackage and not configurable.ReferenceOriginal github issue:etcd#9161Use case:etcd#3715Use case:etcd#8888Use case:etcd#10114FeedbackWas this page helpful?YesNoGlad to hear it! Pleasetell us how we can improve.Sorry to hear that. Pleasetell us how we can improve.Last modified February 28, 2022:Create doc branch for v3.6 (9ed5c74)©
2013–2024etcd AuthorsTerms|Privacy|Trademarks|LicenseAll Rights Reserved