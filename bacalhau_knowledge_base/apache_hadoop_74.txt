URL: https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/ViewFsOverloadScheme.html

Wiki|git|Apache Hadoop| Last Published: 2024-03-04
               | Version: 3.4.0GeneralOverviewSingle Node SetupCluster SetupCommands ReferenceFileSystem ShellCompatibility SpecificationDownstream Developer's GuideAdmin Compatibility GuideInterface ClassificationFileSystem SpecificationCommonCLI Mini ClusterFair Call QueueNative LibrariesProxy UserRack AwarenessSecure ModeService Level AuthorizationHTTP AuthenticationCredential Provider APIHadoop KMSTracingUnix Shell GuideRegistryAsync ProfilerHDFSArchitectureUser GuideCommands ReferenceNameNode HA With QJMNameNode HA With NFSObserver NameNodeFederationViewFsViewFsOverloadSchemeSnapshotsEdits ViewerImage ViewerPermissions and HDFSQuotas and HDFSlibhdfs (C API)WebHDFS (REST API)HttpFSShort Circuit Local ReadsCentralized Cache ManagementNFS GatewayRolling UpgradeExtended AttributesTransparent EncryptionMultihomingStorage PoliciesMemory Storage SupportSynthetic Load GeneratorErasure CodingDisk BalancerUpgrade DomainDataNode AdminRouter FederationProvided StorageMapReduceTutorialCommands ReferenceCompatibility with 1.xEncrypted ShufflePluggable Shuffle/SortDistributed Cache DeploySupport for YARN Shared CacheMapReduce REST APIsMR Application MasterMR History ServerYARNArchitectureCommands ReferenceCapacity SchedulerFair SchedulerResourceManager RestartResourceManager HAResource ModelNode LabelsNode AttributesWeb Application ProxyTimeline ServerTimeline Service V.2Writing YARN ApplicationsYARN Application SecurityNodeManagerRunning Applications in Docker ContainersRunning Applications in runC ContainersUsing CGroupsSecure ContainersReservation SystemGraceful DecommissionOpportunistic ContainersYARN FederationShared CacheUsing GPUUsing FPGAPlacement ConstraintsYARN UI2YARN REST APIsIntroductionResource ManagerNode ManagerTimeline ServerTimeline Service V.2YARN ServiceOverviewQuickStartConceptsYarn Service APIService DiscoverySystem ServicesHadoop Compatible File SystemsAliyun OSSAmazon S3Azure Blob StorageAzure Data Lake StorageTencent COSHuaweicloud OBSAuthOverviewExamplesConfigurationBuildingToolsHadoop StreamingHadoop ArchivesHadoop Archive LogsDistCpHDFS Federation BalanceGridMixRumenResource Estimator ServiceScheduler Load SimulatorHadoop BenchmarkingDynamometerReferenceChangelog and Release NotesJava API docsUnix Shell APIMetricsConfigurationcore-default.xmlhdfs-default.xmlhdfs-rbf-default.xmlmapred-default.xmlyarn-default.xmlkms-default.xmlhttpfs-default.xmlDeprecated PropertiesView File System Overload Scheme GuideIntroductionView File System Overload SchemeDetailsEnabling View File System Overload SchemeExample ConfigurationsCentral Mount Table ConfigurationsDFSAdmin commands with View File System Overload SchemeAccessing paths without authoritySolutionAppendix: A Mount Table Configuration with XIncludeIntroductionThe View File System Overload Scheme introduced to solve two key challenges with the View File System(ViewFS). The first problem is, to use ViewFS, users need to update fs.defaultFS with viewfs scheme (viewfs://). The second problem is that users need to copy the mount-table configurations to all the client nodes. The ViewFileSystemOverloadScheme is addressing these challenges.View File System Overload SchemeDetailsThe View File System Overload Scheme is an extension to the View File System. This will allow users to continue to use their existing fs.defaultFS configured scheme or any new scheme name instead of using schemeviewfs. Mount link configurations key, value formats are same as inViewFS Guide. If a user wants to continue use the same fs.defaultFS and wants to have more mount points, then mount link configurations should have the ViewFileSystemOverloadScheme initialized uri’s hostname as the mount table name. Example if fs.defaultFS ishdfs://mycluster, then the mount link configuration key name should be like in the following formatfs.viewfs.mounttable.*mycluster*.link.<mountLinkPath>. Even if the initialized fs uri has hostname:port, it will simply ignore the port number and only consider the hostname as the mount table name.  We will discuss more example configurations in following sections. If there are no mount links configured with the initializing uri’s hostname as the mount table name, then it will automatically consider the current uri as fallback(fs.viewfs.mounttable.*mycluster*.linkFallback) target fs uri. If the initialized uri contains path part, it will consider only scheme and authority part, but not the path part. Example, if the initialized uri containshdfs://mycluster/data, it will consider onlyhdfs://myclusteras fallback target fs uri. The path partdatawill be ignored.Another important improvement with the ViewFileSystemOverloadScheme is, administrators need not copy themount-table.xmlconfiguration file to 1000s of client nodes. Instead, they can keep the mount-table configuration file in a Hadoop compatible file system. So, keeping the configuration file in a central place makes administrators life easier as they can update mount-table in single place.Enabling View File System Overload SchemeTo use this class, the following configurations needed to be added in core-site.xml file.<property>
  <name>fs.<scheme>.impl</name>
  <value>org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme</value>
</property>Here<scheme>should be same as the uri-scheme configured in fs.defautFS. For example if fs.defaultFS was configured withhdfs://mycluster, then the above configuration would be like below:<property>
  <name>fs.hdfs.impl</name>
  <value>org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme</value>
</property>Example ConfigurationsExample 1:If users want some of their existing cluster (hdfs://cluster) data to mount with hdfs(hdfs://cluster) and other object store clusters(o3fs://bucket1.volume1.omhost/,s3a://bucket1/), the following example configurations can show how to add mount links.<property>
  <name>fs.viewfs.mounttable.cluster.link./user</name>
  <value>hdfs://cluster/user</value>
</property>

<property>
  <name>fs.viewfs.mounttable.cluster.link./data</name>
  <value>o3fs://bucket1.volume1/data</value>
</property>

<property>
  <name>fs.viewfs.mounttable.cluster.link./backup</name>
  <value>s3a://bucket1/backup/</value>
</property>Let’s consider the following operations to understand where these operations will be delegated based on mount links.Op1:Create a file with the pathhdfs://cluster/user/fileA, then physically this file will be created athdfs://cluster/user/fileA. This delegation happened based on the first configuration parameter in above configurations. Here/usermapped tohdfs://cluster/user/.Op2:Create a file the pathhdfs://cluster/data/datafile, then this file will be created ato3fs://bucket1.volume1.omhost/data/datafile. This delegation happened based on second configurations parameter in above configurations. Here/datawas mapped witho3fs://bucket1.volume1.omhost/data/.Op3:Create a file with the pathhdfs://cluster/backup/data.zip, then  physically this file will be created ats3a://bucket1/backup/data.zip. This delegation happened based on the third configuration parameter in above configurations. Here/backupwas mapped tos3a://bucket1/backup/.Example 2:If users want some of their existing cluster (s3a://bucketA/) data to mount with other hdfs cluster(hdfs://cluster) and object store clusters(o3fs://bucket1.volume1.omhost/,s3a://bucketA/), the following example configurations can show how to add mount links.<property>
  <name>fs.viewfs.mounttable.bucketA.link./user</name>
  <value>hdfs://cluster/user</value>
</property>

<property>
  <name>fs.viewfs.mounttable.bucketA.link./data</name>
  <value>o3fs://bucket1.volume1.omhost/data</value>
</property>

<property>
  <name>fs.viewfs.mounttable.bucketA.link./salesDB</name>
  <value>s3a://bucketA/salesDB/</value>
</property>Let’s consider the following operations to understand to where these operations will be delegated based on mount links.Op1:Create a file with the paths3a://bucketA/user/fileA, then this file will be created physically athdfs://cluster/user/fileA. This delegation happened based on the first configuration parameter in above configurations. Here/usermapped tohdfs://cluster/user.Op2:Create a file the paths3a://bucketA/data/datafile, then this file will be created ato3fs://bucket1.volume1.omhost/data/datafile. This delegation happened based on second configurations parameter in above configurations. Here/datawas mapped witho3fs://bucket1.volume1.omhost/data/.Op3:Create a file with the paths3a://bucketA/salesDB/dbfile, then  physically this file will be created ats3a://bucketA/salesDB/dbfile. This delegation happened based on the third configuration parameter in above configurations. Here/salesDBwas mapped tos3a://bucket1/salesDB.Note: In above examples we used create operation only, but the same mechanism applies to any other file system APIs here.The following picture shows how the different schemes can be used in ViewFileSystemOverloadScheme compared to the ViewFileSystem.Note: In ViewFsOverloadScheme, by default the mount links will not be represented as symlinks. The permission bits and isDirectory value will be propagated from the target directory/file.Central Mount Table ConfigurationsTo enable central mount table configuration, we need to configurefs.viewfs.mounttable.pathincore-site.xmlwith the value as the Hadoop compatible file system directory/file path, where themount-table.<versionNumber>.xmlfile copied. Here versionNumber is an integer number and need to increase the version number and upload new file in same directory.The ViewFileSystemOverloadScheme always loads the highest version numbermount-table.<versionNumber>.xml. Please don’t replace the file with same name. Always increment the version number to take new file picked by newly initializing clients. Why we don’t recommend to replace the files is that, some client might have already opened the connections to old mount-table files already and in middle of loading configuration files, and replacing files can make them fail.<property>
  <name>fs.viewfs.mounttable.path</name>
  <value>hdfs://cluster/config/mount-table-dir</value>
</property>If you are sure, you will never do updates to mount-table file, you can also configure file path directly like below. If you configure file path, it will not check any highest version number loading. Whatever file configured it will be loaded. However file name format should be same.<property>
  <name>fs.viewfs.mounttable.path</name>
  <value>hdfs://cluster/config/mount-table-dir/mount-table.<versionNumber>.xml</value>
</property>Note: we recommend not to configure mount-links incore-site.xmlif you configure above valid path. Otherwise both mount links will be mixed and can lead to a confused behavior.If you copy themount-table.<versionNumber>.xml, you may consider having big replication factor depending on your cluster size. So, that file will be available locally to majority of clients as applications(MR/YARN/HBASE..etc) use locality on HDFS when readingmount-table.<versionNumber>.xml.DFSAdmin commands with View File System Overload SchemePlease refer to theHDFSCommands GuideAccessing paths without authorityAccessing paths likehdfs:///foo/bar,hdfs:/foo/barorviewfs:/foo/bar, where the authority (cluster name or hostname) of the path is not specified, is very common. This is especially true when the same code is expected to run on multiple clusters with different names or HDFS Namenodes.WhenViewFileSystemOverloadSchemeis used (as described above), and if (a) the scheme of the path being accessed is different from the scheme of the path specified asfs.defaultFSand (b) if the path doesn’t have an authority specified, accessing the path can result in an error likeEmpty Mount table in config for viewfs://default/. For example, when the following configuration is used but a path likeviewfs:/foo/barorviewfs:///foo/baris accessed, such an error arises.<property>
  <name>fs.hdfs.impl</name>
  <value>org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme</value>
</property>

<property>
  <name>fs.defaultFS</name>
  <value>hdfs://cluster/</value>
</property>SolutionTo avoid the above problem, the configurationfs.viewfs.mounttable.default.name.keyhas to be set to the name of the cluster, i.e, the following should be added tocore-site.xml<property>
  <name>fs.viewfs.mounttable.default.name.key</name>
  <value>cluster</value>
</property>The string in this configurationclustershould match the name of the authority in the value offs.defaultFS. Further, the configuration should have a mount table configured correctly as in the above examples, i.e., the configurationsfs.viewfs.mounttable.*cluster*.link.<mountLinkPath>should be set (note the same stringclusteris used in these configurations).Appendix: A Mount Table Configuration with XIncludeIf users have a HTTP server in trusted network and don’t need authentication mechanism to it, you can also place your mount-table.xml file in that server and configure XInclude xml tag withmount-table.xmlfile.<configuration xmlns:xi="http://www.w3.org/2001/XInclude">
  <xi:include href="http://myserver/mountTable/mountTable.xml" />
</configuration>The Apache Hadoop configuration has the capability to read the http urls from XInclude and load into configurations. If you choose this option, please don’t configure mount-table configuration items incore-site.xmlor atfs.viewfs.mounttable.path. Please note, Hadoop configuration XInclude does not use SPNego authentication when opening url. So, this will not work if http server where you placedmount-table.xmlneeds authentication.©            2008-2024
              Apache Software Foundation
            
                          -Privacy Policy.
        Apache Maven, Maven, Apache, the Apache feather logo, and the Apache Maven project logos are trademarks of The Apache Software Foundation.