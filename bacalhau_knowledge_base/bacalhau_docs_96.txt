URL: https://docs.bacalhau.org/references/jobs/job/task/publishers/s3

Bacalhau Docsv.1.4.0v.1.3.0v.1.3.1v.1.3.2v.1.4.0GitHubSlackContactMoreGitHubSlackContactAsk or SearchCtrl+ KWelcomeGetting StartedHow Bacalhau WorksInstallationCreate NetworkHardware SetupContainer OnboardingDocker WorkloadsWebAssembly (Wasm) WorkloadsSetting UpRunning NodesNode OnboardingGPU InstallationJob selection policyAccess ManagementNode persistenceConnect StorageConfiguration ManagementConfiguring Transport Level SecurityLimits and TimeoutsTest Network LocallyBacalhau WebUIWorkload OnboardingContainerDocker Workload OnboardingWebAssembly (Wasm) WorkloadsBacalhau Docker ImageHow To Work With Custom Containers in BacalhauPythonBuilding and Running Custom Python ContainerRunning Pandas on BacalhauRunning a Python ScriptRunning Jupyter Notebooks on BacalhauScripting Bacalhau with PythonR (language)Building and Running your Custom R Containers on BacalhauRunning a Simple R Script on BacalhauRun CUDA programs on BacalhauRunning a Prolog ScriptReading Data from Multiple S3 Buckets using BacalhauRunning Rust programs as WebAssembly (WASM)Generate Synthetic Data using Sparkov Data Generation techniqueData IngestionCopy Data from URL to Public StoragePinning DataRunning a Job over S3 dataNetworking InstructionsAccessing the Internet from JobsUtilizing NATS.io within BacalhauGPU Workloads SetupAutomatic Update CheckingMarketplace DeploymentsGoogle Cloud MarketplaceGuidesWrite a config.yamlWrite a SpecConfigExamplesData EngineeringUsing Bacalhau with DuckDBEthereum Blockchain Analysis with Ethereum-ETL and BacalhauConvert CSV To Parquet Or AvroSimple Image ProcessingOceanography - Data ConversionVideo ProcessingModel InferenceEasyOCR (Optical Character Recognition) on BacalhauRunning Inference on Dolly 2.0 Model with Hugging FaceSpeech Recognition using WhisperStable Diffusion on a GPUStable Diffusion on a CPUObject Detection with YOLOv5 on BacalhauGenerate Realistic Images using StyleGAN3 and BacalhauStable Diffusion Checkpoint InferenceRunning Inference on a Model stored on S3Model TrainingTraining Pytorch Model with BacalhauTraining Tensorflow ModelStable Diffusion Dreambooth (Finetuning)Molecular DynamicsRunning BIDS Apps on BacalhauCoresets On BacalhauGenomics Data GenerationGromacs for AnalysisMolecular Simulation with OpenMM and BacalhauReferencesJobs GuideJob SpecificationJob TypesTask SpecificationEnginesDocker Engine SpecificationWebAssembly (WASM) Engine SpecificationPublishersIPFS Publisher SpecificationLocal Publisher SpecificationS3 Publisher SpecificationSourcesIPFS Source SpecificationLocal Source SpecificationS3 Source SpecificationURL Source SpecificationNetwork SpecificationInput Source SpecificationResources SpecificationResultPath SpecificationConstraint SpecificationLabels SpecificationMeta SpecificationJob TemplatesQueuing & TimeoutsJob QueuingTimeouts SpecificationJob ResultsStateCLI GuideSingle CLI commandsAgentAgent OverviewAgent AliveAgent NodeAgent VersionConfigConfig OverviewConfig Auto-ResourcesConfig DefaultConfig ListConfig SetJobJob OverviewJob DescribeJob ExecJob ExecutionsJob HistoryJob ListJob LogsJob RunJob StopNodeNode OverviewNode ApproveNode DeleteNode ListNode DescribeNode RejectCLI Commands OverviewCommand MigrationAPI GuideBacalhau API overviewBest PracticesAgent EndpointOrchestrator EndpointMigration APINode ManagementAuthentication & AuthorizationDatabase IntegrationDebuggingDebugging Failed JobsDebugging LocallyOpen Telemetry in BacalhauRunning locally in 'devstack'Setting up Dev EnvironmentHelp & FAQBacalhau FAQsRelease NotesGlossaryIntegrationsApache Airflow Provider for BacalhauLilypadBacalhau Python SDKObservability for WebAssembly WorkloadsCommunitySocial MediaStyle GuideWays to ContributePowered by GitBookS3 Publisher SpecificationBacalhau's S3 Publisher provides users with a secure and efficient method to publish task results to any S3-compatible storage service. This publisher supports not just AWS S3, but other S3-compatible services offered by cloud providers like Google Cloud Storage and Azure Blob Storage, as well as open-source options like MinIO. The integration is designed to be highly flexible, ensuring users can choose the storage option that aligns with their needs, privacy preferences, and operational requirements.Publisher ParametersBucket(string: <required>): The name of the S3 bucket where the task results will be stored.Key(string: <required>): The object key within the specified bucket where the task results will be stored.Endpoint(string: <optional>): The endpoint URL of the S3 service (useful for S3-compatible services).Region(string: <optional>): The region where the S3 bucket is located.Published Result SpecResults published to S3 are stored as objects that can also be used as inputs to other Bacalhau jobs by usingS3 Input Source. The published result specification includes the following parameters:Bucket: Confirms the name of the bucket containing the stored results.Key: Identifies the unique object key within the specified bucket.Region: Notes the AWS region of the bucket.Endpoint: Records the endpoint URL for S3-compatible storage services.VersionID: The version ID of the stored object, enabling versioning support for retrieving specific versions of stored data.ChecksumSHA256: The SHA-256 checksum of the stored object, providing a method to verify data integrity.Dynamic NamingWith the S3 Publisher in Bacalhau, you have the flexibility to use dynamic naming for the objects you publish to S3. This allows you to incorporate specific job and execution details into the object key, making it easier to trace, manage, and organize your published artifacts.Bacalhau supports the following dynamic placeholders that will be replaced with their actual values during the publishing process:{executionID}: Replaced with the specific execution ID.{jobID}: Replaced with the ID of the job.{nodeID}: Replaced with the ID of the node where the execution took place{date}: Replaced with the current date in the formatYYYYMMDD.{time}: Replaced with the current time in the formatHHMMSS.Additionally, if you are publishing an archive and the object key does not end with.tar.gz, it will be automatically appended. Conversely, if you're not archiving and the key doesn't end with a/, a trailing slash will be added.ExampleImagine you've specified the following object key pattern for publishing:Copyresults/{jobID}/{date}/{time}/Given a job with IDabc123, executed on2023-09-26at14:05:30, the published object key would be:Copyresults/abc123/20230926/140530/This dynamic naming feature offers a powerful way to create organized, intuitive naming conventions for your Bacalhau published objects in S3.ExamplesDeclarative ExamplesHere’s an example YAML configuration that outlines the process of using the S3 Publisher with Bacalhau:CopyPublisher:Type:"s3"Params:Bucket:"my-task-results"Key:"task123/result.tar.gz"Endpoint:"https://s3.us-west-2.amazonaws.com"In this configuration, task results will be published to the specified S3 bucket and object key. If you’re using an S3-compatible service, simply update theEndpointparameter with the appropriate URL.The results will be compressed into a single object, and the published result specification will look like:CopyPublishedResult:Type:"s3"Params:Bucket:"my-task-results"Key:"task123/result.tar.gz"Endpoint:"https://s3.us-west-2.amazonaws.com"Region:"us-west-2"ChecksumSHA256:"0x9a3a..."VersionID:"3/L4kqtJlcpXroDTDmJ+rmDbwQaHWyOb..."Imperative ExamplesThe Bacalhau command-line interface (CLI) provides an imperative approach to specify the S3 Publisher. Below are a few examples showcasing how to define an S3 publisher using CLI commands:Basic Docker job writing to S3 with default configurations:Copybacalhaudockerrun-ps3://bucket/keyubuntu...This command writes to the S3 bucket using default endpoint and region settings.Docker job writing to S3 with a specific endpoint and region:Copybacalhaudockerrun-ps3://bucket/key,opt=endpoint=http://s3.example.com,opt=region=us-east-1ubuntu...This command specifies a unique endpoint and region for the S3 bucket.Using naming placeholders:Copybacalhaudockerrun-ps3://bucket/result-{date}-{jobID}ubuntu...Dynamic naming placeholders like{date}and{jobID}allow for organized naming structures, automatically replacing these placeholders with appropriate values upon execution.Remember to replace the placeholders likebucket,key, and other parameters with your specific values. These CLI commands offer a quick and customizable way to submit jobs and specify how the results should be published to S3.Credential RequirementsTo support this storage provider, no extra dependencies are necessary. However, valid AWS credentials are essential to sign the requests. The storage provider employs the default credentials chain to retrieve credentials, primarily sourcing them from:Environment variables: AWS credentials can be specified usingAWS_ACCESS_KEY_IDandAWS_SECRET_ACCESS_KEYenvironment variables.Credentials file: The credentials file typically located at~/.aws/credentialscan also be used to fetch the necessary AWS credentials.IAM Roles for Amazon EC2 Instances: If you're running your tasks within an Amazon EC2 instance, IAM roles can be utilized to provide the necessary permissions and credentials.For a more detailed overview on AWS credential management and other ways to provide these credentials, please refer to the AWS official documentation onstandardized credentials.Required IAM PoliciesCompute NodesCompute nodes must run with the following policies to publish to S3:Copy{"Version":"2012-10-17","Statement":[{"Effect":"Allow","Action":["s3:PutObject"],"Resource":"arn:aws:s3:::BUCKET_NAME/*"}]}PutObject Permissions:Thes3:PutObjectpermission is necessary to publish objects to the specified S3 bucket.Resource:TheResourcefield in the policy specifies the Amazon Resource Name (ARN) of the S3 bucket. The/*suffix is necessary to allow publishing with any prefix within the bucket or can be replaced with a prefix to limit the scope of the policy. You can also specify multiple resources in the policy to allow publishing to multiple buckets, or*to allow publishing to all buckets in the account.Requester NodeTo enable downloading published results usingbacalhau job get <job_id>command, the requester node must run with the following policies:Copy{"Version":"2012-10-17","Statement":[{"Effect":"Allow","Action":["s3:GetObject"],"Resource":"arn:aws:s3:::BUCKET_NAME/*"}]}GetObject Permissions:Thes3:GetObjectpermission is necessary for the requester node to provide a pre-signed URL to download the published results by the client.For more information on IAM policies specific to Amazon S3 buckets and users, please refer to theAWS documentation on Using IAM Policies with Amazon S3.PreviousLocal Publisher SpecificationNextSourcesLast updated2 months agoOn this pagePublisher ParametersPublished Result SpecDynamic NamingExamplesDeclarative ExamplesImperative ExamplesCredential RequirementsRequired IAM PoliciesCompute NodesRequester NodeWas this helpful?Edit on GitHubExport as PDFGet SupportExpansoSupportUse CasesDistributed ETLEdge MLDistributed Data WarehousingFlett ManagementAbout UsWho we areWhat we valueNews & BlogBlogNewsExpanso (2024). All Rights Reserved.