URL: https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/HttpAuthentication.html

Wiki|git|Apache Hadoop| Last Published: 2024-03-04
               | Version: 3.4.0GeneralOverviewSingle Node SetupCluster SetupCommands ReferenceFileSystem ShellCompatibility SpecificationDownstream Developer's GuideAdmin Compatibility GuideInterface ClassificationFileSystem SpecificationCommonCLI Mini ClusterFair Call QueueNative LibrariesProxy UserRack AwarenessSecure ModeService Level AuthorizationHTTP AuthenticationCredential Provider APIHadoop KMSTracingUnix Shell GuideRegistryAsync ProfilerHDFSArchitectureUser GuideCommands ReferenceNameNode HA With QJMNameNode HA With NFSObserver NameNodeFederationViewFsViewFsOverloadSchemeSnapshotsEdits ViewerImage ViewerPermissions and HDFSQuotas and HDFSlibhdfs (C API)WebHDFS (REST API)HttpFSShort Circuit Local ReadsCentralized Cache ManagementNFS GatewayRolling UpgradeExtended AttributesTransparent EncryptionMultihomingStorage PoliciesMemory Storage SupportSynthetic Load GeneratorErasure CodingDisk BalancerUpgrade DomainDataNode AdminRouter FederationProvided StorageMapReduceTutorialCommands ReferenceCompatibility with 1.xEncrypted ShufflePluggable Shuffle/SortDistributed Cache DeploySupport for YARN Shared CacheMapReduce REST APIsMR Application MasterMR History ServerYARNArchitectureCommands ReferenceCapacity SchedulerFair SchedulerResourceManager RestartResourceManager HAResource ModelNode LabelsNode AttributesWeb Application ProxyTimeline ServerTimeline Service V.2Writing YARN ApplicationsYARN Application SecurityNodeManagerRunning Applications in Docker ContainersRunning Applications in runC ContainersUsing CGroupsSecure ContainersReservation SystemGraceful DecommissionOpportunistic ContainersYARN FederationShared CacheUsing GPUUsing FPGAPlacement ConstraintsYARN UI2YARN REST APIsIntroductionResource ManagerNode ManagerTimeline ServerTimeline Service V.2YARN ServiceOverviewQuickStartConceptsYarn Service APIService DiscoverySystem ServicesHadoop Compatible File SystemsAliyun OSSAmazon S3Azure Blob StorageAzure Data Lake StorageTencent COSHuaweicloud OBSAuthOverviewExamplesConfigurationBuildingToolsHadoop StreamingHadoop ArchivesHadoop Archive LogsDistCpHDFS Federation BalanceGridMixRumenResource Estimator ServiceScheduler Load SimulatorHadoop BenchmarkingDynamometerReferenceChangelog and Release NotesJava API docsUnix Shell APIMetricsConfigurationcore-default.xmlhdfs-default.xmlhdfs-rbf-default.xmlmapred-default.xmlyarn-default.xmlkms-default.xmlhttpfs-default.xmlDeprecated PropertiesAuthentication for Hadoop HTTP web-consolesIntroductionConfigurationCORSTrusted ProxyIntroductionThis document describes how to configure Hadoop HTTP web-consoles to require user authentication.By default Hadoop HTTP web-consoles (ResourceManager, NameNode, NodeManagers and DataNodes) allow access without any form of authentication.Hadoop HTTP web-consoles can be configured to require Kerberos authentication using HTTP SPNEGO protocol (supported by browsers like Firefox and Internet Explorer).In addition, Hadoop HTTP web-consoles support the equivalent of Hadoop’s Pseudo/Simple authentication. If this option is enabled, the user name must be specified in the first browser interaction using the user.name query string parameter. e.g.http://localhost:8088/cluster?user.name=babu.If a custom authentication mechanism is required for the HTTP web-consoles, it is possible to implement a plugin to support the alternate authentication mechanism (refer to Hadoop hadoop-auth for details on writing anAuthenticationHandler).The next section describes how to configure Hadoop HTTP web-consoles to require user authentication.ConfigurationThe following properties should be in thecore-site.xmlof all the nodes in the cluster.Property NameDefault ValueDescriptionhadoop.http.filter.initializersAdd to this property theorg.apache.hadoop.security.AuthenticationFilterInitializerinitializer class.hadoop.http.authentication.typesimpleDefines authentication used for the HTTP web-consoles. The supported values are:simple|kerberos|#AUTHENTICATION_HANDLER_CLASSNAME#.hadoop.http.authentication.token.validity36000Indicates how long (in seconds) an authentication token is valid before it has to be renewed.hadoop.http.authentication.token.max-inactive-interval-1(disabled)Specifies the time, in seconds, between client requests the server will invalidate the token.hadoop.http.authentication.signature.secret.file$user.home/hadoop-http-auth-signature-secretThe signature secret file for signing the authentication tokens. A different secret should be used for each service in the cluster, ResourceManager, NameNode, DataNode and NodeManager. This file should be readable only by the Unix user running the daemons.hadoop.http.authentication.cookie.domainThe domain to use for the HTTP cookie that stores the authentication token. For authentication to work correctly across all nodes in the cluster the domain must be correctly set. There is no default value, the HTTP cookie will not have a domain working only with the hostname issuing the HTTP cookie.hadoop.http.authentication.cookie.persistentfalse(session cookie)Specifies the persistence of the HTTP cookie. If the value is true, the cookie is a persistent one. Otherwise, it is a session cookie.IMPORTANT: when using IP addresses, browsers ignore cookies with domain settings. For this setting to work properly all nodes in the cluster must be configured to generate URLs withhostname.domainnames on it.hadoop.http.authentication.simple.anonymous.allowedtrueIndicates whether anonymous requests are allowed when using ‘simple’ authentication.hadoop.http.authentication.kerberos.principalHTTP/_HOST@$LOCALHOSTIndicates the Kerberos principal to be used for HTTP endpoint when using ‘kerberos’ authentication. The principal short name must beHTTPper Kerberos HTTP SPNEGO specification._HOST-if present- is replaced with bind address of the HTTP server.hadoop.http.authentication.kerberos.keytab$user.home/hadoop.keytabLocation of the keytab file with the credentials for the Kerberos principal used for the HTTP endpoint.CORSTo enable cross-origin support (CORS), please set the following configuration parameters:Add org.apache.hadoop.security.HttpCrossOriginFilterInitializer to hadoop.http.filter.initializers in core-site.xml. You will also need to set the following properties in core-site.xml -PropertyDefault ValueDescriptionhadoop.http.cross-origin.enabledfalseEnables cross origin support for all web-serviceshadoop.http.cross-origin.allowed-origins*Comma separated list of origins that are allowed. Values prefixed withregex:are interpreted as regular expressions. Values containing wildcards (*) are possible as well, here a regular expression is generated, the use is discouraged and support is only available for backward compatibility.hadoop.http.cross-origin.allowed-methodsGET,POST,HEADComma separated list of methods that are allowedhadoop.http.cross-origin.allowed-headersX-Requested-With,Content-Type,Accept,OriginComma separated list of headers that are allowedhadoop.http.cross-origin.max-age1800Number of seconds a pre-flighted request can be cachedTrusted ProxyTrusted Proxy adds support to perform operations using end user instead of proxy user. It fetches the end user from doAs query parameter. To enable Trusted Proxy, please set the following configuration parameter:Add org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilterInitializer to hadoop.http.filter.initializers in core-site.xml instead of org.apache.hadoop.security.AuthenticationFilterInitializer.©            2008-2024
              Apache Software Foundation
            
                          -Privacy Policy.
        Apache Maven, Maven, Apache, the Apache feather logo, and the Apache Maven project logos are trademarks of The Apache Software Foundation.