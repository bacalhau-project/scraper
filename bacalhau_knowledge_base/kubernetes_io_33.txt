URL: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/

DocumentationKubernetes BlogTrainingPartnersCommunityCase StudiesVersionsRelease Informationv1.31v1.30v1.29v1.28v1.27English中文 (Chinese)日本語 (Japanese)한국어 (Korean)Português (Portuguese)Español (Spanish)DocumentationAvailable Documentation VersionsGetting startedLearning environmentProduction environmentContainer RuntimesInstalling Kubernetes with deployment toolsBootstrapping clusters with kubeadmInstalling kubeadmTroubleshooting kubeadmCreating a cluster with kubeadmCustomizing components with the kubeadm APIOptions for Highly Available TopologyCreating Highly Available Clusters with kubeadmSet up a High Availability etcd Cluster with kubeadmConfiguring each kubelet in your cluster using kubeadmDual-stack support with kubeadmTurnkey Cloud SolutionsBest practicesConsiderations for large clustersRunning in multiple zonesValidate node setupEnforcing Pod Security StandardsPKI certificates and requirementsConceptsOverviewKubernetes ComponentsObjects In KubernetesKubernetes Object ManagementObject Names and IDsLabels and SelectorsNamespacesAnnotationsField SelectorsFinalizersOwners and DependentsRecommended LabelsThe Kubernetes APICluster ArchitectureNodesCommunication between Nodes and the Control PlaneControllersLeasesCloud Controller ManagerAbout cgroup v2Container Runtime Interface (CRI)Garbage CollectionMixed Version ProxyContainersImagesContainer EnvironmentRuntime ClassContainer Lifecycle HooksWorkloadsPodsPod LifecycleInit ContainersSidecar ContainersEphemeral ContainersDisruptionsPod Quality of Service ClassesUser NamespacesDownward APIWorkload ManagementDeploymentsReplicaSetStatefulSetsDaemonSetJobsAutomatic Cleanup for Finished JobsCronJobReplicationControllerAutoscaling WorkloadsManaging WorkloadsServices, Load Balancing, and NetworkingServiceIngressIngress ControllersGateway APIEndpointSlicesNetwork PoliciesDNS for Services and PodsIPv4/IPv6 dual-stackTopology Aware RoutingNetworking on WindowsService ClusterIP allocationService Internal Traffic PolicyStorageVolumesPersistent VolumesProjected VolumesEphemeral VolumesStorage ClassesVolume Attributes ClassesDynamic Volume ProvisioningVolume SnapshotsVolume Snapshot ClassesCSI Volume CloningStorage CapacityNode-specific Volume LimitsVolume Health MonitoringWindows StorageConfigurationConfiguration Best PracticesConfigMapsSecretsLiveness, Readiness, and Startup ProbesResource Management for Pods and ContainersOrganizing Cluster Access Using kubeconfig FilesResource Management for Windows nodesSecurityCloud Native SecurityPod Security StandardsPod Security AdmissionService AccountsPod Security PoliciesSecurity For Windows NodesControlling Access to the Kubernetes APIRole Based Access Control Good PracticesGood practices for Kubernetes SecretsMulti-tenancyHardening Guide - Authentication MechanismsKubernetes API Server Bypass RisksLinux kernel security constraints for Pods and containersSecurity ChecklistPoliciesLimit RangesResource QuotasProcess ID Limits And ReservationsNode Resource ManagersScheduling, Preemption and EvictionKubernetes SchedulerAssigning Pods to NodesPod OverheadPod Scheduling ReadinessPod Topology Spread ConstraintsTaints and TolerationsScheduling FrameworkDynamic Resource AllocationScheduler Performance TuningResource Bin PackingPod Priority and PreemptionNode-pressure EvictionAPI-initiated EvictionCluster AdministrationNode ShutdownsCertificatesCluster NetworkingLogging ArchitectureMetrics For Kubernetes System ComponentsMetrics for Kubernetes Object StatesSystem LogsTraces For Kubernetes System ComponentsProxies in KubernetesAPI Priority and FairnessCluster AutoscalingInstalling AddonsCoordinated Leader ElectionWindows in KubernetesWindows containers in KubernetesGuide for Running Windows Containers in KubernetesExtending KubernetesCompute, Storage, and Networking ExtensionsNetwork PluginsDevice PluginsExtending the Kubernetes APICustom ResourcesKubernetes API Aggregation LayerOperator patternTasksInstall ToolsInstall and Set Up kubectl on LinuxInstall and Set Up kubectl on macOSInstall and Set Up kubectl on WindowsAdminister a ClusterAdministration with kubeadmCertificate Management with kubeadmConfiguring a cgroup driverReconfiguring a kubeadm clusterUpgrading kubeadm clustersUpgrading Linux nodesUpgrading Windows nodesChanging The Kubernetes Package RepositoryMigrating from dockershimChanging the Container Runtime on a Node from Docker Engine to containerdMigrate Docker Engine nodes from dockershim to cri-dockerdFind Out What Container Runtime is Used on a NodeTroubleshooting CNI plugin-related errorsCheck whether dockershim removal affects youMigrating telemetry and security agents from dockershimGenerate Certificates ManuallyManage Memory, CPU, and API ResourcesConfigure Default Memory Requests and Limits for a NamespaceConfigure Default CPU Requests and Limits for a NamespaceConfigure Minimum and Maximum Memory Constraints for a NamespaceConfigure Minimum and Maximum CPU Constraints for a NamespaceConfigure Memory and CPU Quotas for a NamespaceConfigure a Pod Quota for a NamespaceInstall a Network Policy ProviderUse Antrea for NetworkPolicyUse Calico for NetworkPolicyUse Cilium for NetworkPolicyUse Kube-router for NetworkPolicyRomana for NetworkPolicyWeave Net for NetworkPolicyAccess Clusters Using the Kubernetes APIAdvertise Extended Resources for a NodeAutoscale the DNS Service in a ClusterChange the Access Mode of a PersistentVolume to ReadWriteOncePodChange the default StorageClassSwitching from Polling to CRI Event-based Updates to Container StatusChange the Reclaim Policy of a PersistentVolumeCloud Controller Manager AdministrationConfigure a kubelet image credential providerConfigure Quotas for API ObjectsControl CPU Management Policies on the NodeControl Topology Management Policies on a nodeCustomizing DNS ServiceDebugging DNS ResolutionDeclare Network PolicyDeveloping Cloud Controller ManagerEnable Or Disable A Kubernetes APIEncrypting Confidential Data at RestDecrypt Confidential Data that is Already Encrypted at RestGuaranteed Scheduling For Critical Add-On PodsIP Masquerade Agent User GuideLimit Storage ConsumptionMigrate Replicated Control Plane To Use Cloud Controller ManagerNamespaces WalkthroughOperating etcd clusters for KubernetesReserve Compute Resources for System DaemonsRunning Kubernetes Node Components as a Non-root UserSafely Drain a NodeSecuring a ClusterSet Kubelet Parameters Via A Configuration FileShare a Cluster with NamespacesUpgrade A ClusterUse Cascading Deletion in a ClusterUsing a KMS provider for data encryptionUsing CoreDNS for Service DiscoveryUsing NodeLocal DNSCache in Kubernetes ClustersUsing sysctls in a Kubernetes ClusterUtilizing the NUMA-aware Memory ManagerVerify Signed Kubernetes ArtifactsConfigure Pods and ContainersAssign Memory Resources to Containers and PodsAssign CPU Resources to Containers and PodsConfigure GMSA for Windows Pods and containersResize CPU and Memory Resources assigned to ContainersConfigure RunAsUserName for Windows pods and containersCreate a Windows HostProcess PodConfigure Quality of Service for PodsAssign Extended Resources to a ContainerConfigure a Pod to Use a Volume for StorageConfigure a Pod to Use a PersistentVolume for StorageConfigure a Pod to Use a Projected Volume for StorageConfigure a Security Context for a Pod or ContainerConfigure Service Accounts for PodsPull an Image from a Private RegistryConfigure Liveness, Readiness and Startup ProbesAssign Pods to NodesAssign Pods to Nodes using Node AffinityConfigure Pod InitializationAttach Handlers to Container Lifecycle EventsConfigure a Pod to Use a ConfigMapShare Process Namespace between Containers in a PodUse a User Namespace With a PodUse an Image Volume With a PodCreate static PodsTranslate a Docker Compose File to Kubernetes ResourcesEnforce Pod Security Standards by Configuring the Built-in Admission ControllerEnforce Pod Security Standards with Namespace LabelsMigrate from PodSecurityPolicy to the Built-In PodSecurity Admission ControllerMonitoring, Logging, and DebuggingTroubleshooting ApplicationsDebug PodsDebug ServicesDebug a StatefulSetDetermine the Reason for Pod FailureDebug Init ContainersDebug Running PodsGet a Shell to a Running ContainerTroubleshooting ClustersTroubleshooting kubectlResource metrics pipelineTools for Monitoring ResourcesMonitor Node HealthDebugging Kubernetes nodes with crictlAuditingDebugging Kubernetes Nodes With KubectlDeveloping and debugging services locally using telepresenceWindows debugging tipsManage Kubernetes ObjectsDeclarative Management of Kubernetes Objects Using Configuration FilesDeclarative Management of Kubernetes Objects Using KustomizeManaging Kubernetes Objects Using Imperative CommandsImperative Management of Kubernetes Objects Using Configuration FilesUpdate API Objects in Place Using kubectl patchMigrate Kubernetes Objects Using Storage Version MigrationManaging SecretsManaging Secrets using kubectlManaging Secrets using Configuration FileManaging Secrets using KustomizeInject Data Into ApplicationsDefine a Command and Arguments for a ContainerDefine Dependent Environment VariablesDefine Environment Variables for a ContainerExpose Pod Information to Containers Through Environment VariablesExpose Pod Information to Containers Through FilesDistribute Credentials Securely Using SecretsRun ApplicationsRun a Stateless Application Using a DeploymentRun a Single-Instance Stateful ApplicationRun a Replicated Stateful ApplicationScale a StatefulSetDelete a StatefulSetForce Delete StatefulSet PodsHorizontal Pod AutoscalingHorizontalPodAutoscaler WalkthroughSpecifying a Disruption Budget for your ApplicationAccessing the Kubernetes API from a PodRun JobsRunning Automated Tasks with a CronJobCoarse Parallel Processing Using a Work QueueFine Parallel Processing Using a Work QueueIndexed Job for Parallel Processing with Static Work AssignmentJob with Pod-to-Pod CommunicationParallel Processing using ExpansionsHandling retriable and non-retriable pod failures with Pod failure policyAccess Applications in a ClusterDeploy and Access the Kubernetes DashboardAccessing ClustersConfigure Access to Multiple ClustersUse Port Forwarding to Access Applications in a ClusterUse a Service to Access an Application in a ClusterConnect a Frontend to a Backend Using ServicesCreate an External Load BalancerList All Container Images Running in a ClusterSet up Ingress on Minikube with the NGINX Ingress ControllerCommunicate Between Containers in the Same Pod Using a Shared VolumeConfigure DNS for a ClusterAccess Services Running on ClustersExtend KubernetesConfigure the Aggregation LayerUse Custom ResourcesExtend the Kubernetes API with CustomResourceDefinitionsVersions in CustomResourceDefinitionsSet up an Extension API ServerConfigure Multiple SchedulersUse an HTTP Proxy to Access the Kubernetes APIUse a SOCKS5 Proxy to Access the Kubernetes APISet up Konnectivity serviceTLSConfigure Certificate Rotation for the KubeletManage TLS Certificates in a ClusterManual Rotation of CA CertificatesManage Cluster DaemonsPerform a Rolling Update on a DaemonSetPerform a Rollback on a DaemonSetRunning Pods on Only Some NodesNetworkingAdding entries to Pod /etc/hosts with HostAliasesExtend Service IP RangesValidate IPv4/IPv6 dual-stackExtend kubectl with pluginsManage HugePagesSchedule GPUsTutorialsHello MinikubeLearn Kubernetes BasicsCreate a ClusterUsing Minikube to Create a ClusterDeploy an AppUsing kubectl to Create a DeploymentExplore Your AppViewing Pods and NodesExpose Your App PubliclyUsing a Service to Expose Your AppScale Your AppRunning Multiple Instances of Your AppUpdate Your AppPerforming a Rolling UpdateConfigurationExample: Configuring a Java MicroserviceExternalizing config using MicroProfile, ConfigMaps and SecretsUpdating Configuration via a ConfigMapConfiguring Redis using a ConfigMapAdopting Sidecar ContainersSecurityApply Pod Security Standards at the Cluster LevelApply Pod Security Standards at the Namespace LevelRestrict a Container's Access to Resources with AppArmorRestrict a Container's Syscalls with seccompStateless ApplicationsExposing an External IP Address to Access an Application in a ClusterExample: Deploying PHP Guestbook application with RedisStateful ApplicationsStatefulSet BasicsExample: Deploying WordPress and MySQL with Persistent VolumesExample: Deploying Cassandra with a StatefulSetRunning ZooKeeper, A Distributed System CoordinatorServicesConnecting Applications with ServicesUsing Source IPExplore Termination Behavior for Pods And Their EndpointsReferenceGlossaryAPI OverviewKubernetes API ConceptsServer-Side ApplyClient LibrariesCommon Expression Language in KubernetesKubernetes Deprecation PolicyDeprecated API Migration GuideKubernetes API health endpointsAPI Access ControlAuthenticatingAuthenticating with Bootstrap TokensAuthorizationUsing RBAC AuthorizationUsing Node AuthorizationWebhook ModeUsing ABAC AuthorizationAdmission ControllersDynamic Admission ControlManaging Service AccountsCertificates and Certificate Signing RequestsMapping PodSecurityPolicies to Pod Security StandardsKubelet authentication/authorizationTLS bootstrappingValidating Admission PolicyWell-Known Labels, Annotations and TaintsAudit AnnotationsKubernetes APIWorkload ResourcesPodBindingPodTemplateReplicationControllerReplicaSetDeploymentStatefulSetControllerRevisionDaemonSetJobCronJobHorizontalPodAutoscalerHorizontalPodAutoscalerPriorityClassPodSchedulingContext v1alpha3ResourceClaim v1alpha3ResourceClaimTemplate v1alpha3ResourceSlice v1alpha3Service ResourcesServiceEndpointsEndpointSliceIngressIngressClassConfig and Storage ResourcesConfigMapSecretCSIDriverCSINodeCSIStorageCapacityPersistentVolumeClaimPersistentVolumeStorageClassStorageVersionMigration v1alpha1VolumeVolumeAttachmentVolumeAttributesClass v1beta1Authentication ResourcesServiceAccountTokenRequestTokenReviewCertificateSigningRequestClusterTrustBundle v1alpha1SelfSubjectReviewAuthorization ResourcesLocalSubjectAccessReviewSelfSubjectAccessReviewSelfSubjectRulesReviewSubjectAccessReviewClusterRoleClusterRoleBindingRoleRoleBindingPolicy ResourcesFlowSchemaLimitRangeResourceQuotaNetworkPolicyPodDisruptionBudgetPriorityLevelConfigurationValidatingAdmissionPolicyValidatingAdmissionPolicyBindingExtend ResourcesCustomResourceDefinitionDeviceClass v1alpha3MutatingWebhookConfigurationValidatingWebhookConfigurationCluster ResourcesAPIServiceComponentStatusEventIPAddress v1beta1LeaseLeaseCandidate v1alpha1NamespaceNodeRuntimeClassServiceCIDR v1beta1Common DefinitionsDeleteOptionsLabelSelectorListMetaLocalObjectReferenceNodeSelectorRequirementObjectFieldSelectorObjectMetaObjectReferencePatchQuantityResourceFieldSelectorStatusTypedLocalObjectReferenceCommon ParametersInstrumentationService Level Indicator MetricsCRI Pod & Container MetricsNode metrics dataKubernetes Metrics ReferenceKubernetes Issues and SecurityKubernetes Issue TrackerKubernetes Security and Disclosure InformationCVE feedNode Reference InformationKubelet Checkpoint APILinux Kernel Version RequirementsArticles on dockershim Removal and on Using CRI-compatible RuntimesNode Labels Populated By The KubeletKubelet Configuration Directory MergingKubelet Device Manager API VersionsNode StatusNetworking ReferenceProtocols for ServicesPorts and ProtocolsVirtual IPs and Service ProxiesSetup toolsKubeadmkubeadm initkubeadm joinkubeadm upgradekubeadm configkubeadm resetkubeadm tokenkubeadm versionkubeadm alphakubeadm certskubeadm init phasekubeadm join phasekubeadm kubeconfigkubeadm reset phasekubeadm upgrade phaseImplementation detailsCommand line tool (kubectl)Introduction to kubectlkubectl Quick Referencekubectl referencekubectlkubectl annotatekubectl api-resourceskubectl api-versionskubectl applykubectl apply edit-last-appliedkubectl apply set-last-appliedkubectl apply view-last-appliedkubectl attachkubectl authkubectl auth can-ikubectl auth reconcilekubectl auth whoamikubectl autoscalekubectl certificatekubectl certificate approvekubectl certificate denykubectl cluster-infokubectl cluster-info dumpkubectl completionkubectl configkubectl config current-contextkubectl config delete-clusterkubectl config delete-contextkubectl config delete-userkubectl config get-clusterskubectl config get-contextskubectl config get-userskubectl config rename-contextkubectl config setkubectl config set-clusterkubectl config set-contextkubectl config set-credentialskubectl config unsetkubectl config use-contextkubectl config viewkubectl cordonkubectl cpkubectl createkubectl create clusterrolekubectl create clusterrolebindingkubectl create configmapkubectl create cronjobkubectl create deploymentkubectl create ingresskubectl create jobkubectl create namespacekubectl create poddisruptionbudgetkubectl create priorityclasskubectl create quotakubectl create rolekubectl create rolebindingkubectl create secretkubectl create secret docker-registrykubectl create secret generickubectl create secret tlskubectl create servicekubectl create service clusteripkubectl create service externalnamekubectl create service loadbalancerkubectl create service nodeportkubectl create serviceaccountkubectl create tokenkubectl debugkubectl deletekubectl describekubectl diffkubectl drainkubectl editkubectl eventskubectl execkubectl explainkubectl exposekubectl getkubectl kustomizekubectl labelkubectl logskubectl optionskubectl patchkubectl pluginkubectl plugin listkubectl port-forwardkubectl proxykubectl replacekubectl rolloutkubectl rollout historykubectl rollout pausekubectl rollout restartkubectl rollout resumekubectl rollout statuskubectl rollout undokubectl runkubectl scalekubectl setkubectl set envkubectl set imagekubectl set resourceskubectl set selectorkubectl set serviceaccountkubectl set subjectkubectl taintkubectl topkubectl top nodekubectl top podkubectl uncordonkubectl versionkubectl waitkubectl CommandskubectlJSONPath Supportkubectl for Docker Userskubectl Usage ConventionsComponent toolsFeature GatesFeature Gates (removed)kubeletkube-apiserverkube-controller-managerkube-proxykube-schedulerDebug clusterFlow controlConfiguration APIsClient Authentication (v1)Client Authentication (v1beta1)Event Rate Limit Configuration (v1alpha1)Image Policy API (v1alpha1)kube-apiserver Admission (v1)kube-apiserver Audit Configuration (v1)kube-apiserver Configuration (v1)kube-apiserver Configuration (v1alpha1)kube-apiserver Configuration (v1beta1)kube-controller-manager Configuration (v1alpha1)kube-proxy Configuration (v1alpha1)kube-scheduler Configuration (v1)kubeadm Configuration (v1beta3)kubeadm Configuration (v1beta4)kubeconfig (v1)Kubelet Configuration (v1)Kubelet Configuration (v1alpha1)Kubelet Configuration (v1beta1)Kubelet CredentialProvider (v1)WebhookAdmission Configuration (v1)External APIsKubernetes Custom Metrics (v1beta2)Kubernetes External Metrics (v1beta1)Kubernetes Metrics (v1beta1)SchedulingScheduler ConfigurationScheduling PoliciesOther ToolsMapping from dockercli to crictlContributeContribute to Kubernetes DocumentationSuggesting content improvementsContributing new contentOpening a pull requestDocumenting for a releaseBlogs and case studiesReviewing changesReviewing pull requestsFor approvers and reviewersLocalizing Kubernetes documentationParticipating in SIG DocsRoles and responsibilitiesIssue WranglersPR wranglersDocumentation style overviewContent guideStyle guideDiagram guideWriting a new topicPage content typesContent organizationCustom Hugo ShortcodesUpdating Reference DocumentationQuickstartContributing to the Upstream Kubernetes CodeGenerating Reference Documentation for the Kubernetes APIGenerating Reference Documentation for kubectl CommandsGenerating Reference Documentation for MetricsGenerating Reference Pages for Kubernetes Components and ToolsAdvanced contributingViewing Site AnalyticsDocs smoke test pageKubernetes DocumentationConceptsConfigurationResource Management for Pods and ContainersResource Management for Pods and ContainersWhen you specify aPod, you can optionally specify how much of each resource acontainerneeds. The most common resources to specify are CPU and memory
(RAM); there are others.When you specify the resourcerequestfor containers in a Pod, thekube-scheduleruses this information to decide which node to place the Pod on.
When you specify a resourcelimitfor a container, thekubeletenforces those
limits so that the running container is not allowed to use more of that resource
than the limit you set. The kubelet also reserves at least therequestamount of
that system resource specifically for that container to use.Requests and limitsIf the node where a Pod is running has enough of a resource available, it's possible (and
allowed) for a container to use more resource than itsrequestfor that resource specifies.
However, a container is not allowed to use more than its resourcelimit.For example, if you set amemoryrequest of 256 MiB for a container, and that container is in
a Pod scheduled to a Node with 8GiB of memory and no other Pods, then the container can try to use
more RAM.If you set amemorylimit of 4GiB for that container, the kubelet (andcontainer runtime) enforce the limit.
The runtime prevents the container from using more than the configured resource limit. For example:
when a process in the container tries to consume more than the allowed amount of memory,
the system kernel terminates the process that attempted the allocation, with an out of memory
(OOM) error.Limits can be implemented either reactively (the system intervenes once it sees a violation)
or by enforcement (the system prevents the container from ever exceeding the limit). Different
runtimes can have different ways to implement the same restrictions.Note:If you specify a limit for a resource, but do not specify any request, and no admission-time
mechanism has applied a default request for that resource, then Kubernetes copies the limit
you specified and uses it as the requested value for the resource.Resource typesCPUandmemoryare each aresource type. A resource type has a base unit.
CPU represents compute processing and is specified in units ofKubernetes CPUs.
Memory is specified in units of bytes.
For Linux workloads, you can specifyhuge pageresources.
Huge pages are a Linux-specific feature where the node kernel allocates blocks of memory
that are much larger than the default page size.For example, on a system where the default page size is 4KiB, you could specify a limit,hugepages-2Mi: 80Mi. If the container tries allocating over 40 2MiB huge pages (a
total of 80 MiB), that allocation fails.Note:You cannot overcommithugepages-*resources.
This is different from thememoryandcpuresources.CPU and memory are collectively referred to ascompute resources, orresources. Compute
resources are measurable quantities that can be requested, allocated, and
consumed. They are distinct fromAPI resources. API resources, such as Pods andServicesare objects that can be read and modified
through the Kubernetes API server.Resource requests and limits of Pod and containerFor each container, you can specify resource limits and requests,
including the following:spec.containers[].resources.limits.cpuspec.containers[].resources.limits.memoryspec.containers[].resources.limits.hugepages-<size>spec.containers[].resources.requests.cpuspec.containers[].resources.requests.memoryspec.containers[].resources.requests.hugepages-<size>Although you can only specify requests and limits for individual containers,
it is also useful to think about the overall resource requests and limits for
a Pod.
For a particular resource, aPod resource request/limitis the sum of the
resource requests/limits of that type for each container in the Pod.Resource units in KubernetesCPU resource unitsLimits and requests for CPU resources are measured incpuunits.
In Kubernetes, 1 CPU unit is equivalent to1 physical CPU core,
or1 virtual core, depending on whether the node is a physical host
or a virtual machine running inside a physical machine.Fractional requests are allowed. When you define a container withspec.containers[].resources.requests.cpuset to0.5, you are requesting half
as much CPU time compared to if you asked for1.0CPU.
For CPU resource units, thequantityexpression0.1is equivalent to the
expression100m, which can be read as "one hundred millicpu". Some people say
"one hundred millicores", and this is understood to mean the same thing.CPU resource is always specified as an absolute amount of resource, never as a relative amount. For example,500mCPU represents the roughly same amount of computing power whether that container
runs on a single-core, dual-core, or 48-core machine.Note:Kubernetes doesn't allow you to specify CPU resources with a precision finer than1mor0.001CPU. To avoid accidentally using an invalid CPU quantity, it's useful to specify CPU units using the milliCPU form
instead of the decimal form when using less than 1 CPU unit.For example, you have a Pod that uses5mor0.005CPU and would like to decrease
its CPU resources. By using the decimal form, it's harder to spot that0.0005CPU
is an invalid value, while by using the milliCPU form, it's easier to spot that0.5mis an invalid value.Memory resource unitsLimits and requests formemoryare measured in bytes. You can express memory as
a plain integer or as a fixed-point number using one of thesequantitysuffixes:
E, P, T, G, M, k. You can also use the power-of-two equivalents: Ei, Pi, Ti, Gi,
Mi, Ki. For example, the following represent roughly the same value:128974848, 129e6, 129M,  128974848000m, 123MiPay attention to the case of the suffixes. If you request400mof memory, this is a request
for 0.4 bytes. Someone who types that probably meant to ask for 400 mebibytes (400Mi)
or 400 megabytes (400M).Container resources exampleThe following Pod has two containers. Both containers are defined with a request for
0.25 CPU
and 64MiB (226bytes) of memory. Each container has a limit of 0.5
CPU and 128MiB of memory. You can say the Pod has a request of 0.5 CPU and 128
MiB of memory, and a limit of 1 CPU and 256MiB of memory.---apiVersion:v1kind:Podmetadata:name:frontendspec:containers:-name:appimage:images.my-company.example/app:v4resources:requests:memory:"64Mi"cpu:"250m"limits:memory:"128Mi"cpu:"500m"-name:log-aggregatorimage:images.my-company.example/log-aggregator:v6resources:requests:memory:"64Mi"cpu:"250m"limits:memory:"128Mi"cpu:"500m"How Pods with resource requests are scheduledWhen you create a Pod, the Kubernetes scheduler selects a node for the Pod to
run on. Each node has a maximum capacity for each of the resource types: the
amount of CPU and memory it can provide for Pods. The scheduler ensures that,
for each resource type, the sum of the resource requests of the scheduled
containers is less than the capacity of the node.
Note that although actual memory
or CPU resource usage on nodes is very low, the scheduler still refuses to place
a Pod on a node if the capacity check fails. This protects against a resource
shortage on a node when resource usage later increases, for example, during a
daily peak in request rate.How Kubernetes applies resource requests and limitsWhen the kubelet starts a container as part of a Pod, the kubelet passes that container's
requests and limits for memory and CPU to the container runtime.On Linux, the container runtime typically configures
kernelcgroupsthat apply and enforce the
limits you defined.The CPU limit defines a hard ceiling on how much CPU time that the container can use.
During each scheduling interval (time slice), the Linux kernel checks to see if this
limit is exceeded; if so, the kernel waits before allowing that cgroup to resume execution.The CPU request typically defines a weighting. If several different containers (cgroups)
want to run on a contended system, workloads with larger CPU requests are allocated more
CPU time than workloads with small requests.The memory request is mainly used during (Kubernetes) Pod scheduling. On a node that uses
cgroups v2, the container runtime might use the memory request as a hint to setmemory.minandmemory.low.The memory limit defines a memory limit for that cgroup. If the container tries to
allocate more memory than this limit, the Linux kernel out-of-memory subsystem activates
and, typically, intervenes by stopping one of the processes in the container that tried
to allocate memory. If that process is the container's PID 1, and the container is marked
as restartable, Kubernetes restarts the container.The memory limit for the Pod or container can also apply to pages in memory backed
volumes, such as anemptyDir. The kubelet trackstmpfsemptyDir volumes as container
memory use, rather than as local ephemeral storage.　When using memory backedemptyDir,
be sure to check the notesbelow.If a container exceeds its memory request and the node that it runs on becomes short of
memory overall, it is likely that the Pod the container belongs to will beevicted.A container might or might not be allowed to exceed its CPU limit for extended periods of time.
However, container runtimes don't terminate Pods or containers for excessive CPU usage.To determine whether a container cannot be scheduled or is being killed due to resource limits,
see theTroubleshootingsection.Monitoring compute & memory resource usageThe kubelet reports the resource usage of a Pod as part of the Podstatus.If optionaltools for monitoringare available in your cluster, then Pod resource usage can be retrieved either
from theMetrics APIdirectly or from your monitoring tools.Considerations for memory backedemptyDirvolumesCaution:If you do not specify asizeLimitfor anemptyDirvolume, that volume may
consume up to that pod's memory limit (Pod.spec.containers[].resources.limits.memory).
If you do not set a memory limit, the pod has no upper bound on memory consumption,
and can consume all available memory on the node. Kubernetes schedules pods based
on resource requests (Pod.spec.containers[].resources.requests) and will not
consider memory usage above the request when deciding if another pod can fit on
a given node. This can result in a denial of service and cause the OS to do
out-of-memory (OOM) handling. It is possible to create any number ofemptyDirs
that could potentially consume all available memory on the node, making OOM
more likely.From the perspective of memory management, there are some similarities between
when a process uses memory as a work area and when using memory-backedemptyDir. But when using memory as a volume like memory-backedemptyDir,
there are additional points below that you should be careful of.Files stored on a memory-backed volume are almost entirely managed by the
user application. Unlike when used as a work area for a process, you can not
rely on things like language-level garbage collection.The purpose of writing files to a volume is to save data or pass it between
applications. Neither Kubernetes nor the OS may automatically delete files
from a volume, so memory used by those files can not be reclaimed when the
system or the pod are under memory pressure.A memory-backedemptyDiris useful because of its performance, but memory
is generally much smaller in size and much higher in cost than other storage
media, such as disks or SSDs. Using large amounts of memory foremptyDirvolumes may affect the normal operation of your pod or of the whole node,
so should be used carefully.If you are administering a cluster or namespace, you can also setResourceQuotathat limits memory use;
you may also want to define aLimitRangefor additional enforcement.
If you specify aspec.containers[].resources.limits.memoryfor each Pod,
then the maximum size of anemptyDirvolume will be the pod's memory limit.As an alternative, a cluster administrator can enforce size limits foremptyDirvolumes in new Pods using a policy mechanism such asValidationAdmissionPolicy.Local ephemeral storageFEATURE STATE:Kubernetes v1.25 [stable]Nodes have local ephemeral storage, backed by
locally-attached writeable devices or, sometimes, by RAM.
"Ephemeral" means that there is no long-term guarantee about durability.Pods use ephemeral local storage for scratch space, caching, and for logs.
The kubelet can provide scratch space to Pods using local ephemeral storage to
mountemptyDirvolumesinto containers.The kubelet also uses this kind of storage to holdnode-level container logs,
container images, and the writable layers of running containers.Caution:If a node fails, the data in its ephemeral storage can be lost.
Your applications cannot expect any performance SLAs (disk IOPS for example)
from local ephemeral storage.Note:To make the resource quota work on ephemeral-storage, two things need to be done:An admin sets the resource quota for ephemeral-storage in a namespace.A user needs to specify limits for the ephemeral-storage resource in the Pod spec.If the user doesn't specify the ephemeral-storage resource limit in the Pod spec,
the resource quota is not enforced on ephemeral-storage.Kubernetes lets you track, reserve and limit the amount
of ephemeral local storage a Pod can consume.Configurations for local ephemeral storageKubernetes supports two ways to configure local ephemeral storage on a node:Single filesystemTwo filesystemsIn this configuration, you place all different kinds of ephemeral local data
(emptyDirvolumes, writeable layers, container images, logs) into one filesystem.
The most effective way to configure the kubelet means dedicating this filesystem
to Kubernetes (kubelet) data.The kubelet also writesnode-level container logsand treats these similarly to ephemeral local storage.The kubelet writes logs to files inside its configured log directory (/var/logby default); and has a base directory for other locally stored data
(/var/lib/kubeletby default).Typically, both/var/lib/kubeletand/var/logare on the system root filesystem,
and the kubelet is designed with that layout in mind.Your node can have as many other filesystems, not used for Kubernetes,
as you like.You have a filesystem on the node that you're using for ephemeral data that
comes from running Pods: logs, andemptyDirvolumes. You can use this filesystem
for other data (for example: system logs not related to Kubernetes); it can even
be the root filesystem.The kubelet also writesnode-level container logsinto the first filesystem, and treats these similarly to ephemeral local storage.You also use a separate filesystem, backed by a different logical storage device.
In this configuration, the directory where you tell the kubelet to place
container image layers and writeable layers is on this second filesystem.The first filesystem does not hold any image layers or writeable layers.Your node can have as many other filesystems, not used for Kubernetes,
as you like.The kubelet can measure how much local storage it is using. It does this provided
that you have set up the node using one of the supported configurations for local
ephemeral storage.If you have a different configuration, then the kubelet does not apply resource
limits for ephemeral local storage.Note:The kubelet trackstmpfsemptyDir volumes as container memory use, rather
than as local ephemeral storage.Note:The kubelet will only track the root filesystem for ephemeral storage. OS layouts that mount a separate disk to/var/lib/kubeletor/var/lib/containerswill not report ephemeral storage correctly.Setting requests and limits for local ephemeral storageYou can specifyephemeral-storagefor managing local ephemeral storage. Each
container of a Pod can specify either or both of the following:spec.containers[].resources.limits.ephemeral-storagespec.containers[].resources.requests.ephemeral-storageLimits and requests forephemeral-storageare measured in byte quantities.
You can express storage as a plain integer or as a fixed-point number using one of these suffixes:
E, P, T, G, M, k. You can also use the power-of-two equivalents: Ei, Pi, Ti, Gi,
Mi, Ki. For example, the following quantities all represent roughly the same value:128974848129e6129M123MiPay attention to the case of the suffixes. If you request400mof ephemeral-storage, this is a request
for 0.4 bytes. Someone who types that probably meant to ask for 400 mebibytes (400Mi)
or 400 megabytes (400M).In the following example, the Pod has two containers. Each container has a request of
2GiB of local ephemeral storage. Each container has a limit of 4GiB of local ephemeral
storage. Therefore, the Pod has a request of 4GiB of local ephemeral storage, and
a limit of 8GiB of local ephemeral storage. 500Mi of that limit could be
consumed by theemptyDirvolume.apiVersion:v1kind:Podmetadata:name:frontendspec:containers:-name:appimage:images.my-company.example/app:v4resources:requests:ephemeral-storage:"2Gi"limits:ephemeral-storage:"4Gi"volumeMounts:-name:ephemeralmountPath:"/tmp"-name:log-aggregatorimage:images.my-company.example/log-aggregator:v6resources:requests:ephemeral-storage:"2Gi"limits:ephemeral-storage:"4Gi"volumeMounts:-name:ephemeralmountPath:"/tmp"volumes:-name:ephemeralemptyDir:sizeLimit:500MiHow Pods with ephemeral-storage requests are scheduledWhen you create a Pod, the Kubernetes scheduler selects a node for the Pod to
run on. Each node has a maximum amount of local ephemeral storage it can provide for Pods.
For more information, seeNode Allocatable.The scheduler ensures that the sum of the resource requests of the scheduled containers is less than the capacity of the node.Ephemeral storage consumption managementIf the kubelet is managing local ephemeral storage as a resource, then the
kubelet measures storage use in:emptyDirvolumes, excepttmpfsemptyDirvolumesdirectories holding node-level logswriteable container layersIf a Pod is using more ephemeral storage than you allow it to, the kubelet
sets an eviction signal that triggers Pod eviction.For container-level isolation, if a container's writable layer and log
usage exceeds its storage limit, the kubelet marks the Pod for eviction.For pod-level isolation the kubelet works out an overall Pod storage limit by
summing the limits for the containers in that Pod. In this case, if the sum of
the local ephemeral storage usage from all containers and also the Pod'semptyDirvolumes exceeds the overall Pod storage limit, then the kubelet also marks the Pod
for eviction.Caution:If the kubelet is not measuring local ephemeral storage, then a Pod
that exceeds its local storage limit will not be evicted for breaching
local storage resource limits.However, if the filesystem space for writeable container layers, node-level logs,
oremptyDirvolumes falls low, the nodetaintsitself as short on local storage
and this taint triggers eviction for any Pods that don't specifically tolerate the taint.See the supportedconfigurationsfor ephemeral local storage.The kubelet supports different ways to measure Pod storage use:Periodic scanningFilesystem project quotaThe kubelet performs regular, scheduled checks that scan eachemptyDirvolume, container log directory, and writeable container layer.The scan measures how much space is used.Note:In this mode, the kubelet does not track open file descriptors
for deleted files.If you (or a container) create a file inside anemptyDirvolume,
something then opens that file, and you delete the file while it is
still open, then the inode for the deleted file stays until you close
that file but the kubelet does not categorize the space as in use.<div class="feature-state-notice feature-beta" title="Feature Gate: LocalStorageCapacityIsolationFSQuotaMonitoring">
        <span class="feature-state-name">FEATURE STATE:</span> <code>Kubernetes v1.31 [beta]</code>
      </div>Project quotas are an operating-system level feature for managing
storage use on filesystems. With Kubernetes, you can enable project
quotas for monitoring storage use. Make sure that the filesystem
backing theemptyDirvolumes, on the node, provides project quota support.
For example, XFS and ext4fs offer project quotas.Note:Project quotas let you monitor storage use; they do not enforce limits.Kubernetes uses project IDs starting from1048576. The IDs in use are
registered in/etc/projectsand/etc/projid. If project IDs in
this range are used for other purposes on the system, those project
IDs must be registered in/etc/projectsand/etc/projidso that
Kubernetes does not use them.Quotas are faster and more accurate than directory scanning. When a
directory is assigned to a project, all files created under a
directory are created in that project, and the kernel merely has to
keep track of how many blocks are in use by files in that project.
If a file is created and deleted, but has an open file descriptor,
it continues to consume space. Quota tracking records that space accurately
whereas directory scans overlook the storage used by deleted files.To use quotas to track a pod's resource usage, the pod must be in
a user namespace. Within user namespaces, the kernel restricts changes
to projectIDs on the filesystem, ensuring the reliability of storage
metrics calculated by quotas.If you want to use project quotas, you should:Enable theLocalStorageCapacityIsolationFSQuotaMonitoring=truefeature gateusing thefeatureGatesfield in thekubelet configuration.Ensure theUserNamespacesSupportfeature gateis enabled, and that the kernel, CRI implementation and OCI runtime support user namespaces.Ensure that the root filesystem (or optional runtime filesystem)
has project quotas enabled. All XFS filesystems support project quotas.
For ext4 filesystems, you need to enable the project quota tracking feature
while the filesystem is not mounted.# For ext4, with /dev/block-device not mountedsudo tune2fs -O project -Q prjquota /dev/block-deviceEnsure that the root filesystem (or optional runtime filesystem) is
mounted with project quotas enabled. For both XFS and ext4fs, the
mount option is namedprjquota.If you don't want to use project quotas, you should:Disable theLocalStorageCapacityIsolationFSQuotaMonitoringfeature gateusing thefeatureGatesfield in thekubelet configuration.Extended resourcesExtended resources are fully-qualified resource names outside thekubernetes.iodomain. They allow cluster operators to advertise and users to
consume the non-Kubernetes-built-in resources.There are two steps required to use Extended Resources. First, the cluster
operator must advertise an Extended Resource. Second, users must request the
Extended Resource in Pods.Managing extended resourcesNode-level extended resourcesNode-level extended resources are tied to nodes.Device plugin managed resourcesSeeDevice
Pluginfor how to advertise device plugin managed resources on each node.Other resourcesTo advertise a new node-level extended resource, the cluster operator can
submit aPATCHHTTP request to the API server to specify the available
quantity in thestatus.capacityfor a node in the cluster. After this
operation, the node'sstatus.capacitywill include a new resource. Thestatus.allocatablefield is updated automatically with the new resource
asynchronously by the kubelet.Because the scheduler uses the node'sstatus.allocatablevalue when
evaluating Pod fitness, the scheduler only takes account of the new value after
that asynchronous update. There may be a short delay between patching the
node capacity with a new resource and the time when the first Pod that requests
the resource can be scheduled on that node.Example:Here is an example showing how to usecurlto form an HTTP request that
advertises five "example.com/foo" resources on nodek8s-node-1whose master
isk8s-master.curl --header"Content-Type: application/json-patch+json"\--request PATCH\--data'[{"op": "add", "path": "/status/capacity/example.com~1foo", "value": "5"}]'\http://k8s-master:8080/api/v1/nodes/k8s-node-1/statusNote:In the preceding request,~1is the encoding for the character/in the patch path. The operation path value in JSON-Patch is interpreted as a
JSON-Pointer. For more details, seeIETF RFC 6901, section 3.Cluster-level extended resourcesCluster-level extended resources are not tied to nodes. They are usually managed
by scheduler extenders, which handle the resource consumption and resource quota.You can specify the extended resources that are handled by scheduler extenders
inscheduler configurationExample:The following configuration for a scheduler policy indicates that the
cluster-level extended resource "example.com/foo" is handled by the scheduler
extender.The scheduler sends a Pod to the scheduler extender only if the Pod requests
"example.com/foo".TheignoredBySchedulerfield specifies that the scheduler does not check
the "example.com/foo" resource in itsPodFitsResourcespredicate.{"kind":"Policy","apiVersion":"v1","extenders": [{"urlPrefix":"<extender-endpoint>","bindVerb":"bind","managedResources": [{"name":"example.com/foo","ignoredByScheduler":true}]}]}Consuming extended resourcesUsers can consume extended resources in Pod specs like CPU and memory.
The scheduler takes care of the resource accounting so that no more than the
available amount is simultaneously allocated to Pods.The API server restricts quantities of extended resources to whole numbers.
Examples ofvalidquantities are3,3000mand3Ki. Examples ofinvalidquantities are0.5and1500m(because1500mwould result in1.5).Note:Extended resources replace Opaque Integer Resources.
Users can use any domain name prefix other thankubernetes.iowhich is reserved.To consume an extended resource in a Pod, include the resource name as a key
in thespec.containers[].resources.limitsmap in the container spec.Note:Extended resources cannot be overcommitted, so request and limit
must be equal if both are present in a container spec.A Pod is scheduled only if all of the resource requests are satisfied, including
CPU, memory and any extended resources. The Pod remains in thePENDINGstate
as long as the resource request cannot be satisfied.Example:The Pod below requests 2 CPUs and 1 "example.com/foo" (an extended resource).apiVersion:v1kind:Podmetadata:name:my-podspec:containers:-name:my-containerimage:myimageresources:requests:cpu:2example.com/foo:1limits:example.com/foo:1PID limitingProcess ID (PID) limits allow for the configuration of a kubelet
to limit the number of PIDs that a given Pod can consume. SeePID Limitingfor information.TroubleshootingMy Pods are pending with event messageFailedSchedulingIf the scheduler cannot find any node where a Pod can fit, the Pod remains
unscheduled until a place can be found. AnEventis produced
each time the scheduler fails to find a place for the Pod. You can usekubectlto view the events for a Pod; for example:kubectl describe pod frontend | grep -A9999999999EventsEvents:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  23s   default-scheduler  0/42 nodes available: insufficient cpuIn the preceding example, the Pod named "frontend" fails to be scheduled due to
insufficient CPU resource on any node. Similar error messages can also suggest
failure due to insufficient memory (PodExceedsFreeMemory). In general, if a Pod
is pending with a message of this type, there are several things to try:Add more nodes to the cluster.Terminate unneeded Pods to make room for pending Pods.Check that the Pod is not larger than all the nodes. For example, if all the
nodes have a capacity ofcpu: 1, then a Pod with a request ofcpu: 1.1will
never be scheduled.Check for node taints. If most of your nodes are tainted, and the new Pod does
not tolerate that taint, the scheduler only considers placements onto the
remaining nodes that don't have that taint.You can check node capacities and amounts allocated with thekubectl describe nodescommand. For example:kubectl describe nodes e2e-test-node-pool-4lw4Name:            e2e-test-node-pool-4lw4
[ ... lines removed for clarity ...]
Capacity:
 cpu:                               2
 memory:                            7679792Ki
 pods:                              110
Allocatable:
 cpu:                               1800m
 memory:                            7474992Ki
 pods:                              110
[ ... lines removed for clarity ...]
Non-terminated Pods:        (5 in total)
  Namespace    Name                                  CPU Requests  CPU Limits  Memory Requests  Memory Limits
  ---------    ----                                  ------------  ----------  ---------------  -------------
  kube-system  fluentd-gcp-v1.38-28bv1               100m (5%)     0 (0%)      200Mi (2%)       200Mi (2%)
  kube-system  kube-dns-3297075139-61lj3             260m (13%)    0 (0%)      100Mi (1%)       170Mi (2%)
  kube-system  kube-proxy-e2e-test-...               100m (5%)     0 (0%)      0 (0%)           0 (0%)
  kube-system  monitoring-influxdb-grafana-v4-z1m12  200m (10%)    200m (10%)  600Mi (8%)       600Mi (8%)
  kube-system  node-problem-detector-v0.1-fj7m3      20m (1%)      200m (10%)  20Mi (0%)        100Mi (1%)
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  CPU Requests    CPU Limits    Memory Requests    Memory Limits
  ------------    ----------    ---------------    -------------
  680m (34%)      400m (20%)    920Mi (11%)        1070Mi (13%)In the preceding output, you can see that if a Pod requests more than 1.120 CPUs
or more than 6.23Gi of memory, that Pod will not fit on the node.By looking at the “Pods” section, you can see which Pods are taking up space on
the node.The amount of resources available to Pods is less than the node capacity because
system daemons use a portion of the available resources. Within the Kubernetes API,
each Node has a.status.allocatablefield
(seeNodeStatusfor details).The.status.allocatablefield describes the amount of resources that are available
to Pods on that node (for example: 15 virtual CPUs and 7538 MiB of memory).
For more information on node allocatable resources in Kubernetes, seeReserve Compute Resources for System Daemons.You can configureresource quotasto limit the total amount of resources that a namespace can consume.
Kubernetes enforces quotas for objects in particular namespace when there is a
ResourceQuota in that namespace.
For example, if you assign specific namespaces to different teams, you
can add ResourceQuotas into those namespaces. Setting resource quotas helps to
prevent one team from using so much of any resource that this over-use affects other teams.You should also consider what access you grant to that namespace:fullwrite access to a namespace allows someone with that access to remove any
resource, including a configured ResourceQuota.My container is terminatedYour container might get terminated because it is resource-starved. To check
whether a container is being killed because it is hitting a resource limit, callkubectl describe podon the Pod of interest:kubectl describe pod simmemleak-hra99The output is similar to:Name:                           simmemleak-hra99
Namespace:                      default
Image(s):                       saadali/simmemleak
Node:                           kubernetes-node-tf0f/10.240.216.66
Labels:                         name=simmemleak
Status:                         Running
Reason:
Message:
IP:                             10.244.2.75
Containers:
  simmemleak:
    Image:  saadali/simmemleak:latest
    Limits:
      cpu:          100m
      memory:       50Mi
    State:          Running
      Started:      Tue, 07 Jul 2019 12:54:41 -0700
    Last State:     Terminated
      Reason:       OOMKilled
      Exit Code:    137
      Started:      Fri, 07 Jul 2019 12:54:30 -0700
      Finished:     Fri, 07 Jul 2019 12:54:33 -0700
    Ready:          False
    Restart Count:  5
Conditions:
  Type      Status
  Ready     False
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  42s   default-scheduler  Successfully assigned simmemleak-hra99 to kubernetes-node-tf0f
  Normal  Pulled     41s   kubelet            Container image "saadali/simmemleak:latest" already present on machine
  Normal  Created    41s   kubelet            Created container simmemleak
  Normal  Started    40s   kubelet            Started container simmemleak
  Normal  Killing    32s   kubelet            Killing container with id ead3fb35-5cf5-44ed-9ae1-488115be66c6: Need to kill PodIn the preceding example, theRestart Count: 5indicates that thesimmemleakcontainer in the Pod was terminated and restarted five times (so far).
TheOOMKilledreason shows that the container tried to use more memory than its limit.Your next step might be to check the application code for a memory leak. If you
find that the application is behaving how you expect, consider setting a higher
memory limit (and possibly request) for that container.What's nextGet hands-on experienceassigning Memory resources to containers and Pods.Get hands-on experienceassigning CPU resources to containers and Pods.Read how the API reference defines acontainerand itsresource requirementsRead aboutproject quotasin XFSRead more about thekube-scheduler configuration reference (v1)Read more aboutQuality of Service classes for PodsFeedbackWas this page helpful?YesNoThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it onStack Overflow.
Open an issue in theGitHub Repositoryif you want toreport a problemorsuggest an improvement.Last modified August 21, 2024 at 1:18 PM PST:Amend upstream contribution and resource management documentation (f4f6990dcc)Edit this pageCreate child pageCreate documentation issuePrint entire sectionRequests and limitsResource typesResource requests and limits of Pod and containerResource units in KubernetesCPU resource unitsMemory resource unitsContainer resources exampleHow Pods with resource requests are scheduledHow Kubernetes applies resource requests and limitsMonitoring compute & memory resource usageConsiderations for memory backedemptyDirvolumesLocal ephemeral storageConfigurations for local ephemeral storageSetting requests and limits for local ephemeral storageHow Pods with ephemeral-storage requests are scheduledEphemeral storage consumption managementExtended resourcesManaging extended resourcesConsuming extended resourcesPID limitingTroubleshootingMy Pods are pending with event messageFailedSchedulingMy container is terminatedWhat's nextDocumentationBlogTrainingPartnersCommunityCase Studies© 2024 The Kubernetes Authors | Documentation Distributed underCC BY 4.0Copyright © 2024 The Linux Foundation ®. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see ourTrademark Usage pageICP license: 京ICP备17074266号-3