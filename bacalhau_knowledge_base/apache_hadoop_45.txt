URL: https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/CommandsManual.html

Wiki|git|Apache Hadoop| Last Published: 2024-03-04
               | Version: 3.4.0GeneralOverviewSingle Node SetupCluster SetupCommands ReferenceFileSystem ShellCompatibility SpecificationDownstream Developer's GuideAdmin Compatibility GuideInterface ClassificationFileSystem SpecificationCommonCLI Mini ClusterFair Call QueueNative LibrariesProxy UserRack AwarenessSecure ModeService Level AuthorizationHTTP AuthenticationCredential Provider APIHadoop KMSTracingUnix Shell GuideRegistryAsync ProfilerHDFSArchitectureUser GuideCommands ReferenceNameNode HA With QJMNameNode HA With NFSObserver NameNodeFederationViewFsViewFsOverloadSchemeSnapshotsEdits ViewerImage ViewerPermissions and HDFSQuotas and HDFSlibhdfs (C API)WebHDFS (REST API)HttpFSShort Circuit Local ReadsCentralized Cache ManagementNFS GatewayRolling UpgradeExtended AttributesTransparent EncryptionMultihomingStorage PoliciesMemory Storage SupportSynthetic Load GeneratorErasure CodingDisk BalancerUpgrade DomainDataNode AdminRouter FederationProvided StorageMapReduceTutorialCommands ReferenceCompatibility with 1.xEncrypted ShufflePluggable Shuffle/SortDistributed Cache DeploySupport for YARN Shared CacheMapReduce REST APIsMR Application MasterMR History ServerYARNArchitectureCommands ReferenceCapacity SchedulerFair SchedulerResourceManager RestartResourceManager HAResource ModelNode LabelsNode AttributesWeb Application ProxyTimeline ServerTimeline Service V.2Writing YARN ApplicationsYARN Application SecurityNodeManagerRunning Applications in Docker ContainersRunning Applications in runC ContainersUsing CGroupsSecure ContainersReservation SystemGraceful DecommissionOpportunistic ContainersYARN FederationShared CacheUsing GPUUsing FPGAPlacement ConstraintsYARN UI2YARN REST APIsIntroductionResource ManagerNode ManagerTimeline ServerTimeline Service V.2YARN ServiceOverviewQuickStartConceptsYarn Service APIService DiscoverySystem ServicesHadoop Compatible File SystemsAliyun OSSAmazon S3Azure Blob StorageAzure Data Lake StorageTencent COSHuaweicloud OBSAuthOverviewExamplesConfigurationBuildingToolsHadoop StreamingHadoop ArchivesHadoop Archive LogsDistCpHDFS Federation BalanceGridMixRumenResource Estimator ServiceScheduler Load SimulatorHadoop BenchmarkingDynamometerReferenceChangelog and Release NotesJava API docsUnix Shell APIMetricsConfigurationcore-default.xmlhdfs-default.xmlhdfs-rbf-default.xmlmapred-default.xmlyarn-default.xmlkms-default.xmlhttpfs-default.xmlDeprecated PropertiesHadoop Commands GuideOverviewShell OptionsGeneric OptionsUser CommandsarchivechecknativeclasspathconftestcredentialdistchdistcpdtutilfsgridmixjarjnipathkerbnamekdiagkeykmsversionCLASSNAMEenvvarsAdministration CommandsdaemonlogFilesetc/hadoop/hadoop-env.shetc/hadoop/hadoop-user-functions.sh~/.hadooprcOverviewAll of the Hadoop commands and subprojects follow the same basic structure:Usage:shellcommand [SHELL_OPTIONS] [COMMAND] [GENERIC_OPTIONS] [COMMAND_OPTIONS]FIELDDescriptionshellcommandThe command of the project being invoked. For example, Hadoop common useshadoop, HDFS useshdfs, and YARN usesyarn.SHELL_OPTIONSOptions that the shell processes prior to executing Java.COMMANDAction to perform.GENERIC_OPTIONSThe common set of options supported by multiple commands.COMMAND_OPTIONSVarious commands with their options are described in this documention for the Hadoop common sub-project. HDFS and YARN are covered in other documents.Shell OptionsAll of the shell commands will accept a common set of options. For some commands, these options are ignored. For example, passing---hostnameson a command that only executes on a single host will be ignored.SHELL_OPTIONDescription--buildpathsEnables developer versions of jars.--config confdirOverwrites the default Configuration directory. Default is$HADOOP_HOME/etc/hadoop.--daemon modeIf the command supports daemonization (e.g.,hdfs namenode), execute in the appropriate mode. Supported modes arestartto start the process in daemon mode,stopto stop the process, andstatusto determine the active status of the process.statuswill return anLSB-compliantresult code. If no option is provided, commands that support daemonization will run in the foreground. For commands that do not support daemonization, this option is ignored.--debugEnables shell level configuration debugging information--helpShell script usage information.--hostnamesWhen--workersis used, override the workers file with a space delimited list of hostnames where to execute a multi-host subcommand. If--workersis not used, this option is ignored.--hostsWhen--workersis used, override the workers file with another file that contains a list of hostnames where to execute a multi-host subcommand.  If--workersis not used, this option is ignored.--loglevel loglevelOverrides the log level. Valid log levels are FATAL, ERROR, WARN, INFO, DEBUG, and TRACE. Default is INFO.--workersIf possible, execute this command on all hosts in theworkersfile.Generic OptionsMany subcommands honor a common set of configuration options to alter their behavior:GENERIC_OPTIONDescription-archives <comma separated list of archives>Specify comma separated archives to be unarchived on the compute machines. Applies only to job.-conf <configuration file>Specify an application configuration file.-D <property>=<value>Use value for given property.-files <comma separated list of files>Specify comma separated files to be copied to the map reduce cluster. Applies only to job.-fs <file:///> or <hdfs://namenode:port>Specify default filesystem URL to use. Overrides ‘fs.defaultFS’ property from configurations.-jt <local> or <resourcemanager:port>Specify a ResourceManager. Applies only to job.-libjars <comma separated list of jars>Specify comma separated jar files to include in the classpath. Applies only to job.Hadoop Common CommandsAll of these commands are executed from thehadoopshell command. They have been broken up intoUser CommandsandAdministration Commands.User CommandsCommands useful for users of a hadoop cluster.archiveCreates a hadoop archive. More information can be found atHadoop Archives Guide.checknativeUsage:hadoop checknative [-a] [-h]COMMAND_OPTIONDescription-aCheck all libraries are available.-hprint helpThis command checks the availability of the Hadoop native code. SeeNative Libariesfor more information. By default, this command only checks the availability of libhadoop.classpathUsage:hadoop classpath [--glob |--jar <path> |-h |--help]COMMAND_OPTIONDescription--globexpand wildcards--jarpathwrite classpath as manifest in jar namedpath-h,--helpprint helpPrints the class path needed to get the Hadoop jar and the required libraries. If called without arguments, then prints the classpath set up by the command scripts, which is likely to contain wildcards in the classpath entries. Additional options print the classpath after wildcard expansion or write the classpath into the manifest of a jar file. The latter is useful in environments where wildcards cannot be used and the expanded classpath exceeds the maximum supported command line length.conftestUsage:hadoop conftest [-conffile <path>]...COMMAND_OPTIONDescription-conffilePath of a configuration file or directory to validate-h,--helpprint helpValidates configuration XML files. If the-conffileoption is not specified, the files in${HADOOP_CONF_DIR}whose name end with .xml will be verified. If specified, that path will be verified. You can specify either a file or directory, and if a directory specified, the files in that directory whose name end with.xmlwill be verified. You can specify-conffileoption multiple times.The validation is fairly minimal: the XML is parsed and duplicate and empty property names are checked for. The command does not support XInclude; if you using that to pull in configuration items, it will declare the XML file invalid.credentialUsage:hadoop credential <subcommand> [options]COMMAND_OPTIONDescriptioncreatealias[-providerprovider-path] [-strict] [-valuecredential-value]Prompts the user for a credential to be stored as the given alias. Thehadoop.security.credential.provider.pathwithin the core-site.xml file will be used unless a-provideris indicated. The-strictflag will cause the command to fail if the provider uses a default password. Use-valueflag to supply the credential value (a.k.a. the alias password) instead of being prompted.deletealias[-providerprovider-path] [-strict] [-f]Deletes the credential with the provided alias. Thehadoop.security.credential.provider.pathwithin the core-site.xml file will be used unless a-provideris indicated. The-strictflag will cause the command to fail if the provider uses a default password. The command asks for confirmation unless-fis specifiedlist [-providerprovider-path] [-strict]Lists all of the credential aliases Thehadoop.security.credential.provider.pathwithin the core-site.xml file will be used unless a-provideris indicated. The-strictflag will cause the command to fail if the provider uses a default password.checkalias[-providerprovider-path] [-strict]Check the password for the given alias. Thehadoop.security.credential.provider.pathwithin the core-site.xml file will be used unless a-provideris indicated. The-strictflag will cause the command to fail if the provider uses a default password.Command to manage credentials, passwords and secrets within credential providers.The CredentialProvider API in Hadoop allows for the separation of applications and how they store their required passwords/secrets. In order to indicate a particular provider type and location, the user must provide thehadoop.security.credential.provider.pathconfiguration element in core-site.xml or use the command line option-provideron each of the following commands. This provider path is a comma-separated list of URLs that indicates the type and location of a list of providers that should be consulted. For example, the following path:user:///,jceks://file/tmp/test.jceks,jceks://hdfs@nn1.example.com/my/path/test.jceksindicates that the current user’s credentials file should be consulted through the User Provider, that the local file located at/tmp/test.jceksis a Java Keystore Provider and that the file located within HDFS atnn1.example.com/my/path/test.jceksis also a store for a Java Keystore Provider.When utilizing the credential command it will often be for provisioning a password or secret to a particular credential store provider. In order to explicitly indicate which provider store to use the-provideroption should be used. Otherwise, given a path of multiple providers, the first non-transient provider will be used. This may or may not be the one that you intended.Providers frequently require that a password or other secret is supplied. If the provider requires a password and is unable to find one, it will use a default password and emit a warning message that the default password is being used. If the-strictflag is supplied, the warning message becomes an error message and the command returns immediately with an error status.Example:hadoop credential list -provider jceks://file/tmp/test.jceksdistchUsage:hadoop distch [-f urilist_url] [-i] [-log logdir] path:owner:group:permissionsCOMMAND_OPTIONDescription-fList of objects to change-iIgnore failures-logDirectory to log outputChange the ownership and permissions on many files at once.distcpCopy file or directories recursively. More information can be found atHadoop DistCp Guide.dtutilUsage:hadoop dtutil [-keytabkeytab_file-principalprincipal_name]subcommand[-format (java|protobuf)] [-aliasalias] [-renewerrenewer]filename…Utility to fetch and manage hadoop delegation tokens inside credentials files.  It is intended to replace the simpler commandfetchdt.  There are multiple subcommands, each with their own flags and options.For every subcommand that writes out a file, the-formatoption will specify the internal format to use.javais the legacy format that matchesfetchdt.  The default isprotobuf.For every subcommand that connects to a service, convenience flags are provided to specify the kerberos principal name and keytab file to use for auth.SUBCOMMANDDescriptionprint[-aliasalias]filename[filename2...]Print out the fields in the tokens contained infilename(andfilename2…).Ifaliasis specified, print only tokens matchingalias.  Otherwise, print all tokens.getURL[-servicescheme][-format (java|protobuf)][-aliasalias][-renewerrenewer]filenameFetch a token from service atURLand place it infilename.URLis required and must immediately followget.URLis the service URL, e.g.hdfs://localhost:9000.aliaswill overwrite the service field in the token.It is intended for hosts that have external and internal names, e.g.firewall.com:14000.filenameshould come last and is the name of the token file.It will be created if it does not exist.  Otherwise, token(s) are added to existing file.The-serviceflag should only be used with a URL which starts withhttporhttps.The following are equivalent:hdfs://localhost:9000/vs.http://localhost:9000-servicehdfsappend[-format (java|protobuf)]filenamefilename2[filename3...]Append the contents of the first N filenames onto the last filename.When tokens with common service fields are present in multiple files, earlier files’ tokens are overwritten.That is, tokens present in the last file are always preserved.remove -aliasalias[-format (java|protobuf)]filename[filename2...]From each file specified, remove the tokens matchingaliasand write out each file using specified format.aliasmust be specified.cancel -aliasalias[-format (java|protobuf)]filename[filename2...]Just likeremove, except the tokens are also cancelled using the service specified in the token object.aliasmust be specified.renew -aliasalias[-format (java|protobuf)]filename[filename2...]For each file specified, renew the tokens matchingaliasand write out each file using specified format.aliasmust be specified.importbase64[-aliasalias]filenameImport a token from a base64 token.aliaswill overwrite the service field in the token.fsThis command is documented in theFile System Shell Guide. It is a synonym forhdfs dfswhen HDFS is in use.gridmixGridmix is a benchmark tool for Hadoop cluster. More information can be found in theGridmix Guide.jarUsage:hadoop jar <jar> [mainClass] args...Runs a jar file.Useyarn jarto launch YARN applications instead.jnipathUsage:hadoop jnipathPrint the computed java.library.path.kerbnameUsage:hadoop kerbname principalConvert the named principal via the auth_to_local rules to the Hadoop user name.Example:hadoop kerbname user@EXAMPLE.COMkdiagUsage:hadoop kdiagDiagnose Kerberos ProblemskeyUsage:hadoop key <subcommand> [options]COMMAND_OPTIONDescriptioncreatekeyname[-ciphercipher] [-sizesize] [-descriptiondescription] [-attrattribute=value] [-providerprovider] [-strict] [-help]Creates a new key for the name specified by thekeynameargument within the provider specified by the-providerargument. The-strictflag will cause the command to fail if the provider uses a default password. You may specify a cipher with the-cipherargument. The default cipher is currently “AES/CTR/NoPadding”. The default keysize is 128. You may specify the requested key length using the-sizeargument. Arbitrary attribute=value style attributes may be specified using the-attrargument.-attrmay be specified multiple times, once per attribute.rollkeyname[-providerprovider] [-strict] [-help]Creates a new version for the specified key within the provider indicated using the-providerargument. The-strictflag will cause the command to fail if the provider uses a default password.deletekeyname[-providerprovider] [-strict] [-f] [-help]Deletes all versions of the key specified by thekeynameargument from within the provider specified by-provider. The-strictflag will cause the command to fail if the provider uses a default password. The command asks for user confirmation unless-fis specified.list [-providerprovider] [-strict] [-metadata] [-help]Displays the keynames contained within a particular provider as configured in core-site.xml or specified with the-providerargument. The-strictflag will cause the command to fail if the provider uses a default password.-metadatadisplays the metadata.checkkeyname[-providerprovider] [-strict] [-help]Check password of thekeynamecontained within a particular provider as configured in core-site.xml or specified with the-providerargument. The-strictflag will cause the command to fail if the provider uses a default password.| -help | Prints usage of this command |Manage keys via the KeyProvider. For details on KeyProviders, see theTransparent Encryption Guide.Providers frequently require that a password or other secret is supplied. If the provider requires a password and is unable to find one, it will use a default password and emit a warning message that the default password is being used. If the-strictflag is supplied, the warning message becomes an error message and the command returns immediately with an error status.NOTE: Some KeyProviders (e.g. org.apache.hadoop.crypto.key.JavaKeyStoreProvider) do not support uppercase key names.NOTE: Some KeyProviders do not directly execute a key deletion (e.g. performs a soft-delete instead, or delay the actual deletion, to prevent mistake). In these cases, one may encounter errors when creating/deleting a key with the same name after deleting it. Please check the underlying KeyProvider for details.kmsUsage:hadoop kmsRun KMS, the Key Management Server.versionUsage:hadoop versionPrints the version.CLASSNAMEUsage:hadoop CLASSNAMERuns the class namedCLASSNAME. The class must be part of a package.envvarsUsage:hadoop envvarsDisplay computed Hadoop environment variables.Administration CommandsCommands useful for administrators of a hadoop cluster.daemonlogUsage:hadoop daemonlog -getlevel <host:port> <classname> [-protocol (http|https)]
hadoop daemonlog -setlevel <host:port> <classname> <level> [-protocol (http|https)]COMMAND_OPTIONDescription-getlevelhost:portclassname[-protocol (http|https)]Prints the log level of the log identified by a qualifiedclassname, in the daemon running athost:port. The-protocolflag specifies the protocol for connection.-setlevelhost:portclassnamelevel[-protocol (http|https)]Sets the log level of the log identified by a qualifiedclassname, in the daemon running athost:port.  The-protocolflag specifies the protocol for connection.Get/Set the log level for a Log identified by a qualified class name in the daemon dynamically. By default, the command sends a HTTP request, but this can be overridden by using argument-protocol httpsto send a HTTPS request.Example:$ bin/hadoop daemonlog -setlevel 127.0.0.1:9870 org.apache.hadoop.hdfs.server.namenode.NameNode DEBUG
$ bin/hadoop daemonlog -getlevel 127.0.0.1:9871 org.apache.hadoop.hdfs.server.namenode.NameNode -protocol httpsNote that the setting is not permanent and will be reset when the daemon is restarted. This command works by sending a HTTP/HTTPS request to the daemon’s internal Jetty servlet, so it supports the following daemons:Commonkey management serverHDFSname nodesecondary name nodedata nodejournal nodeHttpFS serverYARNresource managernode managerTimeline serverFilesetc/hadoop/hadoop-env.shThis file stores the global settings used by all Hadoop shell commands.etc/hadoop/hadoop-user-functions.shThis file allows for advanced users to override some shell functionality.~/.hadooprcThis stores the personal environment for an individual user. It is processed after the hadoop-env.sh and hadoop-user-functions.sh files and can contain the same settings.©            2008-2024
              Apache Software Foundation
            
                          -Privacy Policy.
        Apache Maven, Maven, Apache, the Apache feather logo, and the Apache Maven project logos are trademarks of The Apache Software Foundation.