URL: https://etcd.io/docs/v3.6/upgrades/upgrade_3_3/

etcdDocsBlogCommunityInstallPlayVersionsv3.6v3.5v3.4v3.3v3.2v3.1v2.3Versionsv3.6-DRAFTQuickstartDemoTutorialsHow to Set Up a Demo etcd ClusterReading from etcdWriting to etcdHow to get keys by prefixHow to delete keysHow to make multiple writes in a transactionHow to watch keysHow to create leaseHow to create locksHow to conduct leader election in etcd clusterHow to check Cluster statusHow to save the databaseHow to migrate etcd from v2 to v3How to Add and Remove MembersInstallFAQLibraries and toolsMetricsReporting bugsTuningDiscovery service protocolLogging conventionsGolang modulesLearningData modeletcd client designetcd learner designetcd v3 authentication designetcd APIetcd persistent storage filesetcd API guaranteesetcd versus other key-value storesGlossaryDeveloper guideDiscovery service protocolSet up a local clusterInteracting with etcdWhy gRPC gatewaygRPC naming and discoverySystem limitsetcd featuresAPI referenceAPI reference: concurrencyOperations guideAuthentication GuidesRole-based access controlAuthenticationConfiguration optionsTransport security modelClustering GuideRun etcd clusters as a Kubernetes StatefulSetRun etcd clusters inside containersFailure modesDisaster recoveryetcd gatewaygRPC proxyHardware recommendationsMaintenanceMonitoring etcdPerformanceDesign of runtime reconfigurationRuntime reconfigurationSupported platformsVersioningData CorruptionBenchmarksStorage Memory Usage BenchmarkWatch Memory Usage BenchmarkBenchmarking etcd v3Benchmarking etcd v2.2.0-rc-memoryBenchmarking etcd v2.2.0-rcBenchmarking etcd v2.2.0Benchmarking etcd v2.1.0UpgradingUpgrading etcd clusters and applicationsUpgrade etcd from 3.4 to 3.5Upgrade etcd from 3.3 to 3.4Upgrade etcd from 3.2 to 3.3Upgrade etcd from 3.1 to 3.2Upgrade etcd from 3.0 to 3.1Upgrade etcd from 2.3 to 3.0TriageIssue triage guidelinesPR managementv3.5QuickstartDemoTutorialsHow to Set Up a Demo etcd ClusterReading from etcdWriting to etcdHow to get keys by prefixHow to delete keysHow to make multiple writes in a transactionHow to watch keysHow to create leaseHow to create locksHow to conduct leader election in etcd clusterHow to check Cluster statusHow to save the databaseHow to migrate etcd from v2 to v3How to Add and Remove MembersInstallFAQLibraries and toolsMetricsReporting bugsTuningDiscovery service protocolLogging conventionsGolang modulesLearningData modeletcd client designetcd learner designetcd v3 authentication designetcd APIetcd persistent storage filesetcd API guaranteesetcd versus other key-value storesGlossaryDeveloper guideDiscovery service protocolSet up a local clusterInteracting with etcdWhy gRPC gatewaygRPC naming and discoverySystem limitsetcd featuresAPI referenceAPI reference: concurrencyOperations guideAuthentication GuidesRole-based access controlAuthenticationConfiguration optionsTransport security modelClustering GuideRun etcd clusters as a Kubernetes StatefulSetRun etcd clusters inside containersFailure modesDisaster recoveryetcd gatewaygRPC proxyHardware recommendationsMaintenanceMonitoring etcdPerformanceDesign of runtime reconfigurationRuntime reconfigurationSupported platformsVersioningData CorruptionBenchmarksStorage Memory Usage BenchmarkWatch Memory Usage BenchmarkBenchmarking etcd v3Benchmarking etcd v2.2.0-rc-memoryBenchmarking etcd v2.2.0-rcBenchmarking etcd v2.2.0Benchmarking etcd v2.1.0DowngradingDowngrading etcd clusters and applicationsDowngrade etcd from 3.5 to 3.4UpgradingUpgrading etcd clusters and applicationsUpgrade etcd from 3.4 to 3.5Upgrade etcd from 3.3 to 3.4Upgrade etcd from 3.2 to 3.3Upgrade etcd from 3.1 to 3.2Upgrade etcd from 3.0 to 3.1Upgrade etcd from 2.3 to 3.0TriageIssue triage guidelinesPR managementv3.4QuickstartOverviewDemoInstallFAQLibraries and toolsMetricsReporting bugsTuningDiscovery service protocolLogging conventionsLearningData modeletcd client designetcd learner designetcd v3 authentication designetcd3 APIetcd API guaranteesetcd versus other key-value storesGlossaryDeveloper guideDiscovery service protocolSet up a local clusterInteracting with etcdWhy gRPC gatewaygRPC naming and discoverySystem limitsetcd featuresAPI referenceAPI reference: concurrencyOperations guideConfiguration optionsRole-based access controlTransport security modelClustering GuideRun etcd clusters inside containersFailure modesDisaster recoveryetcd gatewaygRPC proxyHardware recommendationsMaintenancePerformanceDesign of runtime reconfigurationRuntime reconfigurationSupported platformsMigrate applications from using API v2 to API v3VersioningData CorruptionMonitoring etcdBenchmarksStorage Memory Usage BenchmarkWatch Memory Usage BenchmarkBenchmarking etcd v3Benchmarking etcd v2.2.0-rc-memoryBenchmarking etcd v2.2.0-rcBenchmarking etcd v2.2.0Benchmarking etcd v2.1.0UpgradingUpgrading etcd clusters and applicationsUpgrade etcd from 3.4 to 3.5Upgrade etcd from 3.3 to 3.4Upgrade etcd from 3.2 to 3.3Upgrade etcd from 3.1 to 3.2Upgrade etcd from 3.0 to 3.1Upgrade etcd from 2.3 to 3.0PlatformsAmazon Web ServicesContainer Linux with systemdFreeBSDTriageIssue Triage Guidelinesv3.3InstallLibraries and toolsMetricsBenchmarksBenchmarking etcd v2.1.0Benchmarking etcd v2.2.0Benchmarking etcd v2.2.0-rcBenchmarking etcd v2.2.0-rc-memoryBenchmarking etcd v3Storage Memory Usage BenchmarkWatch Memory Usage BenchmarkDemoDeveloper guideDiscovery service protocoletcd API Referenceetcd concurrency API ReferenceExperimental APIs and featuresgRPC naming and discoveryInteracting with etcdSet up a local clusterSystem limitsWhy gRPC gatewayDiscovery service protocoletcd v3 APIFrequently Asked Questions (FAQ)Learningetcd client architectureClient feature matrixData modeletcd v3 authentication designetcd versus other key-value storesetcd3 APIGlossaryKV API guaranteesLearnerLogging conventionsOperations guideMonitoring etcdVersioningClustering GuideConfiguration flagsDesign of runtime reconfigurationDisaster recoveryetcd gatewayFailure modesgRPC proxyHardware recommendationsMaintenanceMigrate applications from using API v2 to API v3PerformanceRole-based access controlRun etcd clusters inside containersRuntime reconfigurationSupported systemsTransport security modelPlatformsAmazon Web ServicesContainer Linux with systemdFreeBSDProduction usersReporting bugsTuningUpgradingUpgrade etcd from 2.3 to 3.0Upgrade etcd from 3.0 to 3.1Upgrade etcd from 3.1 to 3.2Upgrade etcd from 3.2 to 3.3Upgrade etcd from 3.3 to 3.4Upgrade etcd from 3.4 to 3.5Upgrading etcd clusters and applicationsv3.2BenchmarksBenchmarking etcd v2.1.0Benchmarking etcd v2.2.0Benchmarking etcd v2.2.0-rcBenchmarking etcd v2.2.0-rc-memoryBenchmarking etcd v3-demoStorage Memory Usage BenchmarkWatch Memory Usage BenchmarkData modelDemoDeveloper guideDiscovery service protocoletcd API referenceetcd concurrency API ReferenceExperimental APIs and featuresgRPC gatewaygRPC naming and discoveryInteracting with etcdSet up a local clusterSystem limitsetcd dev internalDiscovery service protocolLogging conventionsetcd operations guideAuthentication GuideClustering GuideConfiguration flagsDesign of runtime reconfigurationDisaster recoveryetcd gatewaygRPC proxyHardware recommendationsMaintenanceMigrate applications from using API v2 to API v3Monitoring etcdPerformanceRun etcd clusters inside containersRuntime reconfigurationSecurity modelSupported platformsUnderstand failuresVersioningetcd upgradesUpgrade etcd from 2.3 to 3.0Upgrade etcd from 3.0 to 3.1Upgrade etcd from 3.1 to 3.2Upgrade etcd from 3.2 to 3.3Upgrade etcd from 3.3 to 3.4Upgrading etcd clusters and applicationsetcd v3 authentication designetcd versus other key-value storesetcd3 APIFrequently Asked Questions (FAQ)GlossaryInstallKV API guaranteesLibraries and toolsMetricsPlatformsAmazon Web ServicesFreeBSDRun etcd on Container Linux with systemdProduction usersReporting bugsRFCetcd v3 APITuningv3.1Data modelDemoetcd benchmarksetcd v2.1.0-alpha benchmarksetcd v2.2.0 benchmarksetcd v2.2.0-rc benchmarksetcd v2.2.0-rc-memory benchmarksetcd v3-demo benchmarksStorage Memory Usage BenchmarkWatch Memory Usage Benchmarketcd developer guideDiscovery service protocoletcd API ReferenceExperimental APIs and featuresgRPC GatewaygRPC naming and discoveryInteracting with etcdSetup a local clusterSystem limitsetcd internal devDiscovery service protocolLogging conventionsetcd operations guideClustering GuideConfiguration flagsDesign of runtime reconfigurationDisaster recoveryetcd gatewaygRPC proxyHardware recommendationsMaintenanceMigrate applications from using API v2 to API v3Monitoring etcdPerformanceRun etcd clusters inside containersRuntime reconfigurationSecurity modelSupported platformsUnderstand failuresVersioningetcd3 APIFrequently Asked Questions (FAQ)GlossaryInstallKV API guaranteesLibraries and toolsMetricsPlatformsFreeBSDProduction usersReporting bugsRFCetcd v3 APITuningUpgrading etcd clusters and applicationsUpgrade etcd from 2.3 to 3.0Upgrade etcd from 3.0 to 3.1Upgrade etcd from 3.1 to 3.2Upgrade etcd from 3.2 to 3.3Upgrade etcd from 3.3 to 3.4Why etcdv2.3AdministrationAuthentication GuideBackward CompatibilityBenchmarksBenchmarking etcd v2.2.0etcd 2.1.0-alpha benchmarksetcd 2.2.0-rc benchmarksetcd 2.2.0-rc memory benchmarksetcd 3 demo benchmarksStorage Memory Usage BenchmarkWatch Memory Usage BenchmarkClustering GuideConfiguration FlagsDesign of Runtime ReconfigurationDevelopmentDiscovery Service ProtocolError Codeetcd APIetcd v3 APIFAQGlossaryLibraries and ToolsMembers APIMetricsMiscellaneous APIsPlatformsFreeBSProduction UsersProxyReporting BugsRunning etcd under DockerRuntime ReconfigurationSecurity ModelSnapshot MigrationTuningUpgrade etcd from 2.1 to 2.2Upgrade etcd from 2.1 to 2.2Upgrade etcd from 2.2 to 2.3v2 Auth and SecurityVersioningView page sourceEdit this pageCreate child pageCreate documentation issueCreate project issueUpgrade checklistsUpgrades to >= v3.3.14Server upgrade checklistsUpgrade procedureVersionsv3.6-DRAFTUpgradingUpgrade etcd from 3.2 to 3.3Upgrade etcd from 3.2 to 3.3Processes, checklists, and notes on upgrading etcd from 3.2 to 3.3In the general case, upgrading from etcd 3.2 to 3.3 can be a zero-downtime, rolling upgrade:one by one, stop the etcd v3.2 processes and replace them with etcd v3.3 processesafter running all v3.3 processes, new features in v3.3 are available to the clusterBeforestarting an upgrade, read through the rest of this guide to prepare.Upgrade checklistsNOTE:Whenmigrating from v2 with no v3 data, etcd server v3.2+ panics when etcd restores from existing snapshots but no v3ETCD_DATA_DIR/member/snap/dbfile. This happens when the server had migrated from v2 with no previous v3 data. This also prevents accidental v3 data loss (e.g.dbfile might have been moved). etcd requires that post v3 migration can only happen with v3 data. Do not upgrade to newer v3 versions until v3.0 server contains v3 data.NOTE:if you enable auth and use lease(lease ttl is small), it has a high probability to encounterissuethat will result in data inconsistency. It is strongly recommended upgrading to 3.2.31+ firstly to fix this problem, and then upgrade to 3.3. In addition, if the user without permission sends aLeaseRevokerequest to the 3.3 node during the upgrade process, it may still cause data corruption, so it is best to ensure that your environment doesn’t exist such abnormal calls before upgrading, see#11691for detail.Highlighted breaking changes in 3.3.Changed value type ofetcd --auto-compaction-retentionflag tostringChanged--auto-compaction-retentionflag toaccept string valueswithfiner granularity. Now that--auto-compaction-retentionaccepts string values, etcd configuration YAML fileauto-compaction-retentionfield must be changed tostringtype. Previously,--config-file etcd.config.yamlcan haveauto-compaction-retention: 24field, now must beauto-compaction-retention: "24"orauto-compaction-retention: "24h". If configured as--auto-compaction-mode periodic --auto-compaction-retention "24h", the time duration value for--auto-compaction-retentionflag must be valid fortime.ParseDurationfunction in Go.# etcd.config.yaml+auto-compaction-mode: periodic-auto-compaction-retention: 24+auto-compaction-retention: "24"+# Or+auto-compaction-retention: "24h"Changedetcdserver.EtcdServer.ServerConfigto*etcdserver.EtcdServer.ServerConfigetcdserver.EtcdServerhas changed the type of its member field*etcdserver.ServerConfigtoetcdserver.ServerConfig. Andetcdserver.NewServernow takesetcdserver.ServerConfig, instead of*etcdserver.ServerConfig.Before and after (e.g.k8s.io/kubernetes/test/e2e_node/services/etcd.go)import "github.com/coreos/etcd/etcdserver"type EtcdServer struct {*etcdserver.EtcdServer-	config *etcdserver.ServerConfig+	config etcdserver.ServerConfig}func NewEtcd(dataDir string) *EtcdServer {-	config := &etcdserver.ServerConfig{+	config := etcdserver.ServerConfig{DataDir: dataDir,...}return &EtcdServer{config: config}}func (e *EtcdServer) Start() error {var err errore.EtcdServer, err = etcdserver.NewServer(e.config)...Addedembed.Config.LogOutputstructNote that this field has been renamed toembed.Config.LogOutputsin[]stringtype in v3.4. Please seev3.4 upgrade guidefor more details.FieldLogOutputis added toembed.Config:package embedtype Config struct {Debug bool `json:"debug"`LogPkgLevels string `json:"log-package-levels"`+	LogOutput string `json:"log-output"`...Before gRPC server warnings were logged in etcdserver.WARNING: 2017/11/02 11:35:51 grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp: operation was canceled"; Reconnecting to {localhost:2379 <nil>}
WARNING: 2017/11/02 11:35:51 grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp: operation was canceled"; Reconnecting to {localhost:2379 <nil>}From v3.3, gRPC server logs are disabled by default.Note thatembed.Config.SetupLoggingmethod has been deprecated in v3.4. Please seev3.4 upgrade guidefor more details.import"github.com/coreos/etcd/embed"cfg:=&embed.Config{Debug:false}cfg.SetupLogging()Setembed.Config.Debugfield totrueto enable gRPC server logs.Changed/healthendpoint responsePreviously,[endpoint]:[client-port]/healthreturned manually marshaled JSON value. 3.3 now definesetcdhttp.Healthstruct.Note that in v3.3.0-rc.0, v3.3.0-rc.1, and v3.3.0-rc.2,etcdhttp.Healthhas boolean type"health"and"errors"fields. For backward compatibilities, we reverted"health"field tostringtype and removed"errors"field. Further health information will be provided in separate APIs.$ curl http://localhost:2379/health{"health":"true"}Changed gRPC gateway HTTP endpoints (replaced/v3alphawith/v3beta)Beforecurl -L http://localhost:2379/v3alpha/kv/put\-X POST -d'{"key": "Zm9v", "value": "YmFy"}'Aftercurl -L http://localhost:2379/v3beta/kv/put\-X POST -d'{"key": "Zm9v", "value": "YmFy"}'Requests to/v3alphaendpoints will redirect to/v3beta, and/v3alphawill be removed in 3.4 release.Changed maximum request size limits3.3 now allows custom request size limits for both server andclient side. In previous versions(v3.2.10, v3.2.11), client response size was limited to only 4 MiB.Server-side request limits can be configured with--max-request-bytesflag:# limits request size to 1.5 KiBetcd --max-request-bytes1536# client writes exceeding 1.5 KiB will be rejectedetcdctl put foo[LARGE VALUE...]# etcdserver: request is too largeOr configureembed.Config.MaxRequestBytesfield:import"github.com/coreos/etcd/embed"import"github.com/coreos/etcd/etcdserver/api/v3rpc/rpctypes"// limit requests to 5 MiBcfg:=embed.NewConfig()cfg.MaxRequestBytes=5*1024*1024// client writes exceeding 5 MiB will be rejected_,err:=cli.Put(ctx,"foo",[LARGEVALUE...])err==rpctypes.ErrRequestTooLargeIf not specified, server-side limit defaults to 1.5 MiB.Client-side request limits must be configured based on server-side limits.# limits request size to 1 MiBetcd --max-request-bytes1048576import"github.com/coreos/etcd/clientv3"cli,_:=clientv3.New(clientv3.Config{Endpoints:[]string{"127.0.0.1:2379"},MaxCallSendMsgSize:2*1024*1024,MaxCallRecvMsgSize:3*1024*1024,})// client writes exceeding "--max-request-bytes" will be rejected from etcd server_,err:=cli.Put(ctx,"foo",strings.Repeat("a",1*1024*1024+5))err==rpctypes.ErrRequestTooLarge// client writes exceeding "MaxCallSendMsgSize" will be rejected from client-side_,err=cli.Put(ctx,"foo",strings.Repeat("a",5*1024*1024))err.Error()=="rpc error: code = ResourceExhausted desc = grpc: trying to send message larger than max (5242890 vs. 2097152)"// some writes under limitsfori:=range[]int{0,1,2,3,4}{_,err=cli.Put(ctx,fmt.Sprintf("foo%d",i),strings.Repeat("a",1*1024*1024-500))iferr!=nil{panic(err)}}// client reads exceeding "MaxCallRecvMsgSize" will be rejected from client-side_,err=cli.Get(ctx,"foo",clientv3.WithPrefix())err.Error()=="rpc error: code = ResourceExhausted desc = grpc: received message larger than max (5240509 vs. 3145728)"If not specified, client-side send limit defaults to 2 MiB (1.5 MiB + gRPC overhead bytes) and receive limit tomath.MaxInt32. Please seeclientv3 godocfor more detail.Changed raw gRPC client wrapper function signatures3.3 changes the function signatures ofclientv3gRPC client wrapper. This change was needed to supportcustomgrpc.CallOptionon message size limits.Before and after-func NewKVFromKVClient(remote pb.KVClient) KV {+func NewKVFromKVClient(remote pb.KVClient, c *Client) KV {-func NewClusterFromClusterClient(remote pb.ClusterClient) Cluster {+func NewClusterFromClusterClient(remote pb.ClusterClient, c *Client) Cluster {-func NewLeaseFromLeaseClient(remote pb.LeaseClient, keepAliveTimeout time.Duration) Lease {+func NewLeaseFromLeaseClient(remote pb.LeaseClient, c *Client, keepAliveTimeout time.Duration) Lease {-func NewMaintenanceFromMaintenanceClient(remote pb.MaintenanceClient) Maintenance {+func NewMaintenanceFromMaintenanceClient(remote pb.MaintenanceClient, c *Client) Maintenance {-func NewWatchFromWatchClient(wc pb.WatchClient) Watcher {+func NewWatchFromWatchClient(wc pb.WatchClient, c *Client) Watcher {Changed clientv3SnapshotAPI error typePreviously, clientv3SnapshotAPI returned raw [grpc/*status.statusError] type error. v3.3 now translates those errors to corresponding public error types, to be consistent with other APIs.Beforeimport"context"// reading snapshot with canceled context should error outctx,cancel:=context.WithCancel(context.Background())rc,_:=cli.Snapshot(ctx)cancel()_,err:=io.Copy(f,rc)err.Error()=="rpc error: code = Canceled desc = context canceled"// reading snapshot with deadline exceeded should error outctx,cancel=context.WithTimeout(context.Background(),time.Second)defercancel()rc,_=cli.Snapshot(ctx)time.Sleep(2*time.Second)_,err=io.Copy(f,rc)err.Error()=="rpc error: code = DeadlineExceeded desc = context deadline exceeded"Afterimport"context"// reading snapshot with canceled context should error outctx,cancel:=context.WithCancel(context.Background())rc,_:=cli.Snapshot(ctx)cancel()_,err:=io.Copy(f,rc)err==context.Canceled// reading snapshot with deadline exceeded should error outctx,cancel=context.WithTimeout(context.Background(),time.Second)defercancel()rc,_=cli.Snapshot(ctx)time.Sleep(2*time.Second)_,err=io.Copy(f,rc)err==context.DeadlineExceededChangedetcdctl lease timetolivecommand outputPreviously,lease timetolive LEASE_IDcommand on expired lease prints-1sfor remaining seconds. 3.3 now outputs clearer messages.Beforelease 2d8257079fa1bc0c granted with TTL(0s), remaining(-1s)Afterlease 2d8257079fa1bc0c already expiredChangedgolang.org/x/net/contextimportsclientv3has deprecatedgolang.org/x/net/context. If a project vendorsgolang.org/x/net/contextin other code (e.g. etcd generated protocol buffer code) and importsgithub.com/coreos/etcd/clientv3, it requires Go 1.9+ to compile.Beforeimport"golang.org/x/net/context"cli.Put(context.Background(),"f","v")Afterimport"context"cli.Put(context.Background(),"f","v")Changed gRPC dependency3.3 now requiresgrpc/grpc-gov1.7.5.Deprecatedgrpclog.Loggergrpclog.Loggerhas been deprecated in favor ofgrpclog.LoggerV2.clientv3.Loggeris nowgrpclog.LoggerV2.Beforeimport"github.com/coreos/etcd/clientv3"clientv3.SetLogger(log.New(os.Stderr,"grpc: ",0))Afterimport"github.com/coreos/etcd/clientv3"import"google.golang.org/grpc/grpclog"clientv3.SetLogger(grpclog.NewLoggerV2(os.Stderr,os.Stderr,os.Stderr))// log.New above cannot be used (not implement grpclog.LoggerV2 interface)Deprecatedgrpc.ErrClientConnTimeoutPreviously,grpc.ErrClientConnTimeouterror is returned on client dial time-outs. 3.3 instead returnscontext.DeadlineExceeded(see#8504).Before// expect dial time-out on ipv4 blackhole_,err:=clientv3.New(clientv3.Config{Endpoints:[]string{"http://254.0.0.1:12345"},DialTimeout:2*time.Second})iferr==grpc.ErrClientConnTimeout{// handle errors}After_,err:=clientv3.New(clientv3.Config{Endpoints:[]string{"http://254.0.0.1:12345"},DialTimeout:2*time.Second})iferr==context.DeadlineExceeded{// handle errors}Changed official container registryetcd now usesgcr.io/etcd-development/etcdas a primary container registry, andquay.io/coreos/etcdas secondary.Beforedocker pull quay.io/coreos/etcd:v3.2.5Afterdocker pull gcr.io/etcd-development/etcd:v3.3.0Upgrades to >= v3.3.14v3.3.14had to include some features from 3.4, while trying to minimize the difference between client balancer implementation. This release fixes“kube-apiserver 1.13.x refuses to work when first etcd-server is not available” (kubernetes#72102).grpc.ErrClientConnClosinghas beendeprecated in gRPC >= 1.10.import (+	"go.etcd.io/etcd/clientv3""google.golang.org/grpc"+	"google.golang.org/grpc/codes"+	"google.golang.org/grpc/status")_, err := kvc.Get(ctx, "a")-if err == grpc.ErrClientConnClosing {+if clientv3.IsConnCanceled(err) {// or+s, ok := status.FromError(err)+if ok {+  if s.Code() == codes.CanceledThe new client balanceruses an asynchronous resolver to pass endpoints to the gRPC dial function. As a result,v3.3.14or later requiresgrpc.WithBlockdial option to wait until the underlying connection is up.import ("time""go.etcd.io/etcd/clientv3"+	"google.golang.org/grpc")+// "grpc.WithBlock()" to block until the underlying connection is upccfg := clientv3.Config{Endpoints:            []string{"localhost:2379"},DialTimeout:          time.Second,+ DialOptions:          []grpc.DialOption{grpc.WithBlock()},DialKeepAliveTime:    time.Second,DialKeepAliveTimeout: 500 * time.Millisecond,}Please seeCHANGELOGfor a full list of changes.Server upgrade checklistsUpgrade requirementsTo upgrade an existing etcd deployment to 3.3, the running cluster must be 3.2 or greater. If it’s before 3.2, pleaseupgrade to 3.2before upgrading to 3.3.Also, to ensure a smooth rolling upgrade, the running cluster must be healthy. Check the health of the cluster by using theetcdctl endpoint healthcommand before proceeding.PreparationBefore upgrading etcd, always test the services relying on etcd in a staging environment before deploying the upgrade to the production environment.Before beginning,backup the etcd data. Should something go wrong with the upgrade, it is possible to use this backup todowngradeback to existing etcd version. Please note that thesnapshotcommand only backs up the v3 data. For v2 data, seebacking up v2 datastore.Mixed versionsWhile upgrading, an etcd cluster supports mixed versions of etcd members, and operates with the protocol of the lowest common version. The cluster is only considered upgraded once all of its members are upgraded to version 3.3. Internally, etcd members negotiate with each other to determine the overall cluster version, which controls the reported version and the supported features.LimitationsNote: If the cluster only has v3 data and no v2 data, it is not subject to this limitation.If the cluster is serving a v2 data set larger than 50MB, each newly upgraded member may take up to two minutes to catch up with the existing cluster. Check the size of a recent snapshot to estimate the total data size. In other words, it is safest to wait for 2 minutes between upgrading each member.For a much larger total data size, 100MB or more , this one-time process might take even more time. Administrators of very large etcd clusters of this magnitude can feel free to contact theetcd teambefore upgrading, and we’ll be happy to provide advice on the procedure.DowngradeIf all members have been upgraded to v3.3, the cluster will be upgraded to v3.3, and downgrade from this completed state isnot possible. If any single member is still v3.2, however, the cluster and its operations remains “v3.2”, and it is possible from this mixed cluster state to return to using a v3.2 etcd binary on all members.Pleasebackup the data directoryof all etcd members to make downgrading the cluster possible even after it has been completely upgraded.Upgrade procedureThis example shows how to upgrade a 3-member v3.2 etcd cluster running on a local machine.1. Check upgrade requirementsIs the cluster healthy and running v3.2.x?$ ETCDCTL_API=3 etcdctl endpoint health --endpoints=localhost:2379,localhost:22379,localhost:32379
localhost:2379 is healthy: successfully committed proposal: took = 6.600684ms
localhost:22379 is healthy: successfully committed proposal: took = 8.540064ms
localhost:32379 is healthy: successfully committed proposal: took = 8.763432ms

$ curl http://localhost:2379/version
{"etcdserver":"3.2.7","etcdcluster":"3.2.0"}2. Stop the existing etcd processWhen each etcd process is stopped, expected errors will be logged by other cluster members. This is normal since a cluster member connection has been (temporarily) broken:14:13:31.491746 I | raft: c89feb932daef420 [term 3] received MsgTimeoutNow from 6d4f535bae3ab960 and starts an election to get leadership.
14:13:31.491769 I | raft: c89feb932daef420 became candidate at term 4
14:13:31.491788 I | raft: c89feb932daef420 received MsgVoteResp from c89feb932daef420 at term 4
14:13:31.491797 I | raft: c89feb932daef420 [logterm: 3, index: 9] sent MsgVote request to 6d4f535bae3ab960 at term 4
14:13:31.491805 I | raft: c89feb932daef420 [logterm: 3, index: 9] sent MsgVote request to 9eda174c7df8a033 at term 4
14:13:31.491815 I | raft: raft.node: c89feb932daef420 lost leader 6d4f535bae3ab960 at term 4
14:13:31.524084 I | raft: c89feb932daef420 received MsgVoteResp from 6d4f535bae3ab960 at term 4
14:13:31.524108 I | raft: c89feb932daef420 [quorum:2] has received 2 MsgVoteResp votes and 0 vote rejections
14:13:31.524123 I | raft: c89feb932daef420 became leader at term 4
14:13:31.524136 I | raft: raft.node: c89feb932daef420 elected leader c89feb932daef420 at term 4
14:13:31.592650 W | rafthttp: lost the TCP streaming connection with peer 6d4f535bae3ab960 (stream MsgApp v2 reader)
14:13:31.592825 W | rafthttp: lost the TCP streaming connection with peer 6d4f535bae3ab960 (stream Message reader)
14:13:31.693275 E | rafthttp: failed to dial 6d4f535bae3ab960 on stream Message (dial tcp [::1]:2380: getsockopt: connection refused)
14:13:31.693289 I | rafthttp: peer 6d4f535bae3ab960 became inactive
14:13:31.936678 W | rafthttp: lost the TCP streaming connection with peer 6d4f535bae3ab960 (stream Message writer)It’s a good idea at this point tobackup the etcd datato provide a downgrade path should any problems occur:$ etcdctl snapshot save backup.db3. Drop-in etcd v3.3 binary and start the new etcd processThe new v3.3 etcd will publish its information to the cluster:14:14:25.363225 I | etcdserver: published {Name:s1 ClientURLs:[http://localhost:2379]} to cluster a9ededbffcb1b1f1Verify that each member, and then the entire cluster, becomes healthy with the new v3.3 etcd binary:$ ETCDCTL_API=3 /etcdctl endpoint health --endpoints=localhost:2379,localhost:22379,localhost:32379
localhost:22379 is healthy: successfully committed proposal: took = 5.540129ms
localhost:32379 is healthy: successfully committed proposal: took = 7.321771ms
localhost:2379 is healthy: successfully committed proposal: took = 10.629901msUpgraded members will log warnings like the following until the entire cluster is upgraded. This is expected and will cease after all etcd cluster members are upgraded to v3.3:14:15:17.071804 W | etcdserver: member c89feb932daef420 has a higher version 3.3.0
14:15:21.073110 W | etcdserver: the local etcd version 3.2.7 is not up-to-date
14:15:21.073142 W | etcdserver: member 6d4f535bae3ab960 has a higher version 3.3.0
14:15:21.073157 W | etcdserver: the local etcd version 3.2.7 is not up-to-date
14:15:21.073164 W | etcdserver: member c89feb932daef420 has a higher version 3.3.04. Repeat step 2 to step 3 for all other members5. FinishWhen all members are upgraded, the cluster will report upgrading to 3.3 successfully:14:15:54.536901 N | etcdserver/membership: updated the cluster version from 3.2 to 3.3
14:15:54.537035 I | etcdserver/api: enabled capabilities for version 3.3$ ETCDCTL_API=3 /etcdctl endpoint health --endpoints=localhost:2379,localhost:22379,localhost:32379
localhost:2379 is healthy: successfully committed proposal: took = 2.312897ms
localhost:22379 is healthy: successfully committed proposal: took = 2.553476ms
localhost:32379 is healthy: successfully committed proposal: took = 2.517902msFeedbackWas this page helpful?YesNoGlad to hear it! Pleasetell us how we can improve.Sorry to hear that. Pleasetell us how we can improve.Last modified August 19, 2023:etcd-io/website#479 Use new and better canonical link to Google Groups (cd8b01f)©
2013–2024etcd AuthorsTerms|Privacy|Trademarks|LicenseAll Rights Reserved