URL: https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SecureMode.html

Wiki|git|Apache Hadoop| Last Published: 2024-03-04
               | Version: 3.4.0GeneralOverviewSingle Node SetupCluster SetupCommands ReferenceFileSystem ShellCompatibility SpecificationDownstream Developer's GuideAdmin Compatibility GuideInterface ClassificationFileSystem SpecificationCommonCLI Mini ClusterFair Call QueueNative LibrariesProxy UserRack AwarenessSecure ModeService Level AuthorizationHTTP AuthenticationCredential Provider APIHadoop KMSTracingUnix Shell GuideRegistryAsync ProfilerHDFSArchitectureUser GuideCommands ReferenceNameNode HA With QJMNameNode HA With NFSObserver NameNodeFederationViewFsViewFsOverloadSchemeSnapshotsEdits ViewerImage ViewerPermissions and HDFSQuotas and HDFSlibhdfs (C API)WebHDFS (REST API)HttpFSShort Circuit Local ReadsCentralized Cache ManagementNFS GatewayRolling UpgradeExtended AttributesTransparent EncryptionMultihomingStorage PoliciesMemory Storage SupportSynthetic Load GeneratorErasure CodingDisk BalancerUpgrade DomainDataNode AdminRouter FederationProvided StorageMapReduceTutorialCommands ReferenceCompatibility with 1.xEncrypted ShufflePluggable Shuffle/SortDistributed Cache DeploySupport for YARN Shared CacheMapReduce REST APIsMR Application MasterMR History ServerYARNArchitectureCommands ReferenceCapacity SchedulerFair SchedulerResourceManager RestartResourceManager HAResource ModelNode LabelsNode AttributesWeb Application ProxyTimeline ServerTimeline Service V.2Writing YARN ApplicationsYARN Application SecurityNodeManagerRunning Applications in Docker ContainersRunning Applications in runC ContainersUsing CGroupsSecure ContainersReservation SystemGraceful DecommissionOpportunistic ContainersYARN FederationShared CacheUsing GPUUsing FPGAPlacement ConstraintsYARN UI2YARN REST APIsIntroductionResource ManagerNode ManagerTimeline ServerTimeline Service V.2YARN ServiceOverviewQuickStartConceptsYarn Service APIService DiscoverySystem ServicesHadoop Compatible File SystemsAliyun OSSAmazon S3Azure Blob StorageAzure Data Lake StorageTencent COSHuaweicloud OBSAuthOverviewExamplesConfigurationBuildingToolsHadoop StreamingHadoop ArchivesHadoop Archive LogsDistCpHDFS Federation BalanceGridMixRumenResource Estimator ServiceScheduler Load SimulatorHadoop BenchmarkingDynamometerReferenceChangelog and Release NotesJava API docsUnix Shell APIMetricsConfigurationcore-default.xmlhdfs-default.xmlhdfs-rbf-default.xmlmapred-default.xmlyarn-default.xmlkms-default.xmlhttpfs-default.xmlDeprecated PropertiesHadoop in Secure ModeIntroductionAuthenticationEnd User AccountsUser Accounts for Hadoop DaemonsKerberos principals for Hadoop DaemonsHDFSYARNMapReduce JobHistory ServerMapping from Kerberos principals to OS user accountsExample rulesMapping from user to groupProxy userSecure DataNodeData confidentialityData Encryption on RPCData Encryption on Block data transfer.Data Encryption on HTTPConfigurationPermissions for both HDFS and local fileSystem pathsCommon ConfigurationsNameNodeSecondary NameNodeJournalNodeDataNodeWebHDFSResourceManagerNodeManagerConfiguration for WebAppProxyLinuxContainerExecutorMapReduce JobHistory ServerMultihomingTroubleshootingTroubleshooting with KDiagUsage--jaas: Require a JAAS file to be defined in java.security.auth.login.config.--keylen <length>: Require a minimum size for encryption keys supported by the JVM".--keytab <keytab> --principal <principal>: Log in from a keytab.--nofail : Do not fail on the first problem--nologin: Do not attempt to log in.--out outfile: Write output to file.--resource <resource> : XML configuration resource to load.--secure: Fail if the command is not executed on a secure cluster.--verifyshortname <principal>: validate the short name of a principalExampleReferencesIntroductionIn its default configuration, we expect you to make sure attackers don’t have access to your Hadoop cluster by restricting all network access. If you want any restrictions on who can remotely access data or submit work, you MUST secure authentication and access for your Hadoop cluster as described in this document.When Hadoop is configured to run in secure mode, each Hadoop service and each user must be authenticated by Kerberos.Forward and reverse host lookup for all service hosts must be configured correctly to allow services to authenticate with each other. Host lookups may be configured using either DNS or/etc/hostsfiles. Working knowledge of Kerberos and DNS is recommended before attempting to configure Hadoop services in Secure Mode.Security features of Hadoop consist ofAuthentication,Service Level Authorization,Authentication for Web ConsolesandData Confidentiality.AuthenticationEnd User AccountsWhen service level authentication is turned on, end users must authenticate themselves before interacting with Hadoop services. The simplest way is for a user to authenticate interactively using theKerberoskinitcommand. Programmatic authentication using Kerberos keytab files may be used when interactive login withkinitis infeasible.User Accounts for Hadoop DaemonsEnsure that HDFS and YARN daemons run as different Unix users, e.g.hdfsandyarn. Also, ensure that the MapReduce JobHistory server runs as different user such asmapred.It’s recommended to have them share a Unix group, e.g.hadoop. See also “Mapping from user to group” for group management.User:GroupDaemonshdfs:hadoopNameNode, Secondary NameNode, JournalNode, DataNodeyarn:hadoopResourceManager, NodeManagermapred:hadoopMapReduce JobHistory ServerKerberos principals for Hadoop DaemonsEach Hadoop Service instance must be configured with its Kerberos principal and keytab file location.The general format of a Service principal isServiceName/_HOST@REALM.TLD. e.g.dn/_HOST@EXAMPLE.COM.Hadoop simplifies the deployment of configuration files by allowing the hostname component of the service principal to be specified as the_HOSTwildcard. Each service instance will substitute_HOSTwith its own fully qualified hostname at runtime. This allows administrators to deploy the same set of configuration files on all nodes. However, the keytab files will be different.HDFSThe NameNode keytab file, on each NameNode host, should look like the following:$ klist -e -k -t /etc/security/keytab/nn.service.keytab
Keytab name: FILE:/etc/security/keytab/nn.service.keytab
KVNO Timestamp         Principal
   4 07/18/11 21:08:09 nn/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 nn/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 nn/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5)
   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5)The Secondary NameNode keytab file, on that host, should look like the following:$ klist -e -k -t /etc/security/keytab/sn.service.keytab
Keytab name: FILE:/etc/security/keytab/sn.service.keytab
KVNO Timestamp         Principal
   4 07/18/11 21:08:09 sn/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 sn/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 sn/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5)
   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5)The DataNode keytab file, on each host, should look like the following:$ klist -e -k -t /etc/security/keytab/dn.service.keytab
Keytab name: FILE:/etc/security/keytab/dn.service.keytab
KVNO Timestamp         Principal
   4 07/18/11 21:08:09 dn/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 dn/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 dn/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5)
   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5)YARNThe ResourceManager keytab file, on the ResourceManager host, should look like the following:$ klist -e -k -t /etc/security/keytab/rm.service.keytab
Keytab name: FILE:/etc/security/keytab/rm.service.keytab
KVNO Timestamp         Principal
   4 07/18/11 21:08:09 rm/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 rm/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 rm/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5)
   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5)The NodeManager keytab file, on each host, should look like the following:$ klist -e -k -t /etc/security/keytab/nm.service.keytab
Keytab name: FILE:/etc/security/keytab/nm.service.keytab
KVNO Timestamp         Principal
   4 07/18/11 21:08:09 nm/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 nm/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 nm/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5)
   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5)MapReduce JobHistory ServerThe MapReduce JobHistory Server keytab file, on that host, should look like the following:$ klist -e -k -t /etc/security/keytab/jhs.service.keytab
Keytab name: FILE:/etc/security/keytab/jhs.service.keytab
KVNO Timestamp         Principal
   4 07/18/11 21:08:09 jhs/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 jhs/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 jhs/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5)
   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-256 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (AES-128 CTS mode with 96-bit SHA-1 HMAC)
   4 07/18/11 21:08:09 host/full.qualified.domain.name@REALM.TLD (ArcFour with HMAC/md5)Mapping from Kerberos principals to OS user accountsHadoop maps Kerberos principals to OS user (system) accounts using rules specified byhadoop.security.auth_to_local. How Hadoop evaluates these rules is determined by the setting ofhadoop.security.auth_to_local.mechanism.In the defaulthadoopmode a Kerberos principalmustbe matched against a rule that transforms the principal to a simple form, i.e. a user account name without ‘@’ or ‘/’, otherwise a principal will not be authorized and a error will be logged.  In case of theMITmode the rules work in the same way as theauth_to_localinKerberos configuration file (krb5.conf)and the restrictions ofhadoopmode donotapply. If you useMITmode it is suggested to use the sameauth_to_localrules that are specified in your /etc/krb5.conf as part of your default realm and keep them in sync. In bothhadoopandMITmode the rules are being applied (with the exception ofDEFAULT) toallprincipals regardless of their specified realm. Also, note you shouldnotrely on theauth_to_localrules as an ACL and use proper (OS) mechanisms.Possible values forauth_to_localare:RULE:expThe local name will be formulated from exp. The format for exp is[n:string](regexp)s/pattern/replacement/g. The integer n indicates how many components the target principal should have. If this matches, then a string will be formed from string, substituting the realm of the principal for$0and the n’th component of the principal for$n(e.g., if the principal was johndoe/admin then[2:$2$1foo]would result in the stringadminjohndoefoo). If this string matches regexp, then thes//[g]substitution command will be run over the string. The optional g will cause the substitution to be global over the string, instead of replacing only the first match in the string. As an extension to MIT, Hadoopauth_to_localmapping supports the/Lflag that lowercases the returned name.DEFAULTPicks the first component of the principal name as the system user name if and only if the realm matches thedefault_realm(usually defined in /etc/krb5.conf). e.g. The default rule maps the principalhost/full.qualified.domain.name@MYREALM.TLDto system userhostif the default realm isMYREALM.TLD.In case no rules are specified Hadoop defaults to usingDEFAULT, which is probablynot suitableto most of the clusters.Please note that Hadoop does not support multiple default realms (e.g like Heimdal does). Also, Hadoop does not do a verification on mapping whether a local system account exists.Example rulesIn a typical cluster HDFS and YARN services will be launched as the systemhdfsandyarnusers respectively.hadoop.security.auth_to_localcan be configured as follows:<property>
  <name>hadoop.security.auth_to_local</name>
  <value>
    RULE:[2:$1/$2@$0]([ndj]n/.*@REALM.\TLD)s/.*/hdfs/
    RULE:[2:$1/$2@$0]([rn]m/.*@REALM\.TLD)s/.*/yarn/
    RULE:[2:$1/$2@$0](jhs/.*@REALM\.TLD)s/.*/mapred/
    DEFAULT
  </value>
</property>This would map any principalnn, dn, jnon anyhostfrom realmREALM.TLDto the local system accounthdfs. Secondly it would map any principalrm, nmon anyhostfromREALM.TLDto the local system accountyarn. Thirdly, it would map the principaljhson anyhostfrom realmREALM.TLDto the local system accountmapred. Finally, any principal on any host from the default realm will be mapped to the user component of that principal.Custom rules can be tested using thehadoop kerbnamecommand.  This command allows one to specify a principal and apply Hadoop’s currentauth_to_localruleset.Mapping from user to groupThe system user to system group mapping mechanism can be configured viahadoop.security.group.mapping. SeeHadoop Groups Mappingfor details.Practically you need to manage SSO environment using Kerberos with LDAP for Hadoop in secure mode.Proxy userSome products such as Apache Oozie which access the services of Hadoop on behalf of end users need to be able to impersonate end users. Seethe doc of proxy userfor details.Secure DataNodeBecause the DataNode data transfer protocol does not use the Hadoop RPC framework, DataNodes must authenticate themselves using privileged ports which are specified bydfs.datanode.addressanddfs.datanode.http.address. This authentication is based on the assumption that the attacker won’t be able to get root privileges on DataNode hosts.When you execute thehdfs datanodecommand as root, the server process binds privileged ports at first, then drops privilege and runs as the user account specified byHDFS_DATANODE_SECURE_USER. This startup process usesthe jsvc programinstalled toJSVC_HOME. You must specifyHDFS_DATANODE_SECURE_USERandJSVC_HOMEas environment variables on start up (inhadoop-env.sh).As of version 2.6.0, SASL can be used to authenticate the data transfer protocol. In this configuration, it is no longer required for secured clusters to start the DataNode as root usingjsvcand bind to privileged ports. To enable SASL on data transfer protocol, setdfs.data.transfer.protectionin hdfs-site.xml. A SASL enabled DataNode can be started in secure mode in following two ways: 1. Set a non-privileged port fordfs.datanode.address. 1. Setdfs.http.policytoHTTPS_ONLYor setdfs.datanode.http.addressto a privileged port and make sure theHDFS_DATANODE_SECURE_USERandJSVC_HOMEenvironment variables are specified properly as environment variables on start up (inhadoop-env.sh).In order to migrate an existing cluster that used root authentication to start using SASL instead, first ensure that version 2.6.0 or later has been deployed to all cluster nodes as well as any external applications that need to connect to the cluster. Only versions 2.6.0 and later of the HDFS client can connect to a DataNode that uses SASL for authentication of data transfer protocol, so it is vital that all callers have the correct version before migrating. After version 2.6.0 or later has been deployed everywhere, update configuration of any external applications to enable SASL. If an HDFS client is enabled for SASL, then it can connect successfully to a DataNode running with either root authentication or SASL authentication. Changing configuration for all clients guarantees that subsequent configuration changes on DataNodes will not disrupt the applications. Finally, each individual DataNode can be migrated by changing its configuration and restarting. It is acceptable to have a mix of some DataNodes running with root authentication and some DataNodes running with SASL authentication temporarily during this migration period, because an HDFS client enabled for SASL can connect to both.Data confidentialityData Encryption on RPCThe data transfered between hadoop services and clients can be encrypted on the wire. Settinghadoop.rpc.protectiontoprivacyincore-site.xmlactivates data encryption.Data Encryption on Block data transfer.You need to setdfs.encrypt.data.transfertotruein the hdfs-site.xml in order to activate data encryption for data transfer protocol of DataNode.Optionally, you may setdfs.encrypt.data.transfer.algorithmto either3desorrc4to choose the specific encryption algorithm. If unspecified, then the configured JCE default on the system is used, which is usually 3DES.Settingdfs.encrypt.data.transfer.cipher.suitestoAES/CTR/NoPaddingactivates AES encryption. By default, this is unspecified, so AES is not used. When AES is used, the algorithm specified indfs.encrypt.data.transfer.algorithmis still used during an initial key exchange. The AES key bit length can be configured by settingdfs.encrypt.data.transfer.cipher.key.bitlengthto 128, 192 or 256. The default is 128.AES offers the greatest cryptographic strength and the best performance. At this time, 3DES and RC4 have been used more often in Hadoop clusters.You can also setdfs.encrypt.data.transfer.cipher.suitestoSM4/CTR/NoPaddingto activates SM4 encryption. By default, this is unspecified. The SM4 key bit length can be configured by settingdfs.encrypt.data.transfer.cipher.key.bitlengthto 128, 192 or 256. The default is 128.Data Encryption on HTTPData transfer between Web-console and clients are protected by using SSL(HTTPS). SSL configuration is recommended but not required to configure Hadoop security with Kerberos.To enable SSL for web console of HDFS daemons, setdfs.http.policyto eitherHTTPS_ONLYorHTTP_AND_HTTPSin hdfs-site.xml. Note KMS and HttpFS do not respect this parameter. SeeHadoop KMSandHadoop HDFS over HTTP - Server Setupfor instructions on enabling KMS over HTTPS and HttpFS over HTTPS, respectively.To enable SSL for web console of YARN daemons, setyarn.http.policytoHTTPS_ONLYin yarn-site.xml.To enable SSL for web console of MapReduce JobHistory server, setmapreduce.jobhistory.http.policytoHTTPS_ONLYin mapred-site.xml.ConfigurationPermissions for both HDFS and local fileSystem pathsThe following table lists various paths on HDFS and local filesystems (on all nodes) and recommended permissions:FilesystemPathUser:GroupPermissionslocaldfs.namenode.name.dirhdfs:hadoopdrwx------localdfs.datanode.data.dirhdfs:hadoopdrwx------local$HADOOP_LOG_DIRhdfs:hadoopdrwxrwxr-xlocal$YARN_LOG_DIRyarn:hadoopdrwxrwxr-xlocalyarn.nodemanager.local-dirsyarn:hadoopdrwxr-xr-xlocalyarn.nodemanager.log-dirsyarn:hadoopdrwxr-xr-xlocalcontainer-executorroot:hadoop--Sr-s--*localconf/container-executor.cfgroot:hadoopr-------*hdfs/hdfs:hadoopdrwxr-xr-xhdfs/tmphdfs:hadoopdrwxrwxrwxthdfs/userhdfs:hadoopdrwxr-xr-xhdfsyarn.nodemanager.remote-app-log-diryarn:hadoopdrwxrwxrwxthdfsmapreduce.jobhistory.intermediate-done-dirmapred:hadoopdrwxrwxrwxthdfsmapreduce.jobhistory.done-dirmapred:hadoopdrwxr-x---Common ConfigurationsIn order to turn on RPC authentication in hadoop, set the value ofhadoop.security.authenticationproperty to"kerberos", and set security related settings listed below appropriately.The following properties should be in thecore-site.xmlof all the nodes in the cluster.ParameterValueNoteshadoop.security.authenticationkerberossimple: No authentication. (default)kerberos: Enable authentication by Kerberos.hadoop.security.authorizationtrueEnableRPC service-level authorization.hadoop.rpc.protectionauthenticationauthentication: authentication only (default);integrity: integrity check in addition to authentication;privacy: data encryption in addition to integrityhadoop.security.auth_to_localRULE:exp1RULE:exp2…DEFAULTThe value is string containing new line characters. SeeKerberos documentationfor the format ofexp.hadoop.proxyuser.superuser.hostscomma separated hosts from whichsuperuseraccess are allowed to impersonation.*means wildcard.hadoop.proxyuser.superuser.groupscomma separated groups to which users impersonated bysuperuserbelong.*means wildcard.NameNodeParameterValueNotesdfs.block.access.token.enabletrueEnable HDFS block access tokens for secure operations.dfs.namenode.kerberos.principalnn/_HOST@REALM.TLDKerberos principal name for the NameNode.dfs.namenode.keytab.file/etc/security/keytab/nn.service.keytabKerberos keytab file for the NameNode.dfs.namenode.kerberos.internal.spnego.principalHTTP/_HOST@REALM.TLDThe server principal used by the NameNode for web UI SPNEGO authentication. The SPNEGO server principal begins with the prefixHTTP/by convention. If the value is'*', the web server will attempt to login with every principal specified in the keytab filedfs.web.authentication.kerberos.keytab. For most deployments this can be set to${dfs.web.authentication.kerberos.principal}i.e use the value ofdfs.web.authentication.kerberos.principal.dfs.web.authentication.kerberos.keytab/etc/security/keytab/spnego.service.keytabSPNEGO keytab file for the NameNode. In HA clusters this setting is shared with the Journal Nodes.The following settings allow configuring SSL access to the NameNode web UI (optional).ParameterValueNotesdfs.http.policyHTTP_ONLYorHTTPS_ONLYorHTTP_AND_HTTPSHTTPS_ONLYturns off http access. If using SASL to authenticate data transfer protocol instead of running DataNode as root and using privileged ports, then this property must be set toHTTPS_ONLYto guarantee authentication of HTTP servers. (Seedfs.data.transfer.protection.)dfs.namenode.https-address0.0.0.0:9871This parameter is used in non-HA mode and without federation. SeeHDFS High AvailabilityandHDFS Federationfor details.Secondary NameNodeParameterValueNotesdfs.namenode.secondary.http-address0.0.0.0:9868HTTP web UI address for the Secondary NameNode.dfs.namenode.secondary.https-address0.0.0.0:9869HTTPS web UI address for the Secondary NameNode.dfs.secondary.namenode.keytab.file/etc/security/keytab/sn.service.keytabKerberos keytab file for the Secondary NameNode.dfs.secondary.namenode.kerberos.principalsn/_HOST@REALM.TLDKerberos principal name for the Secondary NameNode.dfs.secondary.namenode.kerberos.internal.spnego.principalHTTP/_HOST@REALM.TLDThe server principal used by the Secondary NameNode for web UI SPNEGO authentication. The SPNEGO server principal begins with the prefixHTTP/by convention. If the value is'*', the web server will attempt to login with every principal specified in the keytab filedfs.web.authentication.kerberos.keytab. For most deployments this can be set to${dfs.web.authentication.kerberos.principal}i.e use the value ofdfs.web.authentication.kerberos.principal.JournalNodeParameterValueNotesdfs.journalnode.kerberos.principaljn/_HOST@REALM.TLDKerberos principal name for the JournalNode.dfs.journalnode.keytab.file/etc/security/keytab/jn.service.keytabKerberos keytab file for the JournalNode.dfs.journalnode.kerberos.internal.spnego.principalHTTP/_HOST@REALM.TLDThe server principal used by the JournalNode for web UI SPNEGO authentication when Kerberos security is enabled. The SPNEGO server principal begins with the prefixHTTP/by convention. If the value is'*', the web server will attempt to login with every principal specified in the keytab filedfs.web.authentication.kerberos.keytab. For most deployments this can be set to${dfs.web.authentication.kerberos.principal}i.e use the value ofdfs.web.authentication.kerberos.principal.dfs.web.authentication.kerberos.keytab/etc/security/keytab/spnego.service.keytabSPNEGO keytab file for the JournalNode. In HA clusters this setting is shared with the Name Nodes.dfs.journalnode.https-address0.0.0.0:8481HTTPS web UI address for the JournalNode.DataNodeParameterValueNotesdfs.datanode.data.dir.perm700dfs.datanode.address0.0.0.0:1004Secure DataNode must use privileged port in order to assure that the server was started securely. This means that the server must be started via jsvc. Alternatively, this must be set to a non-privileged port if using SASL to authenticate data transfer protocol. (Seedfs.data.transfer.protection.)dfs.datanode.http.address0.0.0.0:1006Secure DataNode must use privileged port in order to assure that the server was started securely. This means that the server must be started via jsvc.dfs.datanode.https.address0.0.0.0:9865HTTPS web UI address for the Data Node.dfs.datanode.kerberos.principaldn/_HOST@REALM.TLDKerberos principal name for the DataNode.dfs.datanode.keytab.file/etc/security/keytab/dn.service.keytabKerberos keytab file for the DataNode.dfs.encrypt.data.transferfalseset totruewhen using data encryptiondfs.encrypt.data.transfer.algorithmoptionally set to3desorrc4when using data encryption to control encryption algorithmdfs.encrypt.data.transfer.cipher.suitesoptionally set toAES/CTR/NoPaddingto activate AES encryption when using data encryptiondfs.encrypt.data.transfer.cipher.key.bitlengthoptionally set to128,192or256to control key bit length when using AES with data encryptiondfs.data.transfer.protectionauthentication: authentication only;integrity: integrity check in addition to authentication;privacy: data encryption in addition to integrity This property is unspecified by default. Setting this property enables SASL for authentication of data transfer protocol. If this is enabled, thendfs.datanode.addressmust use a non-privileged port,dfs.http.policymust be set toHTTPS_ONLYand theHDFS_DATANODE_SECURE_USERenvironment variable must be undefined when starting the DataNode process.WebHDFSParameterValueNotesdfs.web.authentication.kerberos.principalhttp/_HOST@REALM.TLDKerberos principal name for the WebHDFS. In HA clusters this setting is commonly used by the JournalNodes for securing access to the JournalNode HTTP server with SPNEGO.dfs.web.authentication.kerberos.keytab/etc/security/keytab/http.service.keytabKerberos keytab file for WebHDFS. In HA clusters this setting is commonly used the JournalNodes for securing access to the JournalNode HTTP server with SPNEGO.ResourceManagerParameterValueNotesyarn.resourcemanager.principalrm/_HOST@REALM.TLDKerberos principal name for the ResourceManager.yarn.resourcemanager.keytab/etc/security/keytab/rm.service.keytabKerberos keytab file for the ResourceManager.yarn.resourcemanager.webapp.https.address${yarn.resourcemanager.hostname}:8090The https adddress of the RM web application for non-HA. In HA clusters, useyarn.resourcemanager.webapp.https.address.rm-idfor each ResourceManager. SeeResourceManager High Availabilityfor details.NodeManagerParameterValueNotesyarn.nodemanager.principalnm/_HOST@REALM.TLDKerberos principal name for the NodeManager.yarn.nodemanager.keytab/etc/security/keytab/nm.service.keytabKerberos keytab file for the NodeManager.yarn.nodemanager.container-executor.classorg.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutorUse LinuxContainerExecutor.yarn.nodemanager.linux-container-executor.grouphadoopUnix group of the NodeManager.yarn.nodemanager.linux-container-executor.path/path/to/bin/container-executorThe path to the executable of Linux container executor.yarn.nodemanager.webapp.https.address0.0.0.0:8044The https adddress of the NM web application.Configuration for WebAppProxyTheWebAppProxyprovides a proxy between the web applications exported by an application and an end user. If security is enabled it will warn users before accessing a potentially unsafe web application. Authentication and authorization using the proxy is handled just like any other privileged web application.ParameterValueNotesyarn.web-proxy.addressWebAppProxyhost:port for proxy to AM web apps.host:portif this is the same asyarn.resourcemanager.webapp.addressor it is not defined then theResourceManagerwill run the proxy otherwise a standalone proxy server will need to be launched.yarn.web-proxy.keytab/etc/security/keytab/web-app.service.keytabKerberos keytab file for the WebAppProxy.yarn.web-proxy.principalwap/_HOST@REALM.TLDKerberos principal name for the WebAppProxy.LinuxContainerExecutorAContainerExecutorused by YARN framework which define how anycontainerlaunched and controlled.The following are the available in Hadoop YARN:ContainerExecutorDescriptionDefaultContainerExecutorThe default executor which YARN uses to manage container execution. The container process has the same Unix user as the NodeManager.LinuxContainerExecutorSupported only on GNU/Linux, this executor runs the containers as either the YARN user who submitted the application (when full security is enabled) or as a dedicated user (defaults to nobody) when full security is not enabled. When full security is enabled, this executor requires all user accounts to be created on the cluster nodes where the containers are launched. It uses asetuidexecutable that is included in the Hadoop distribution. The NodeManager uses this executable to launch and kill containers. The setuid executable switches to the user who has submitted the application and launches or kills the containers. For maximum security, this executor sets up restricted permissions and user/group ownership of local files and directories used by the containers such as the shared objects, jars, intermediate files, log files etc. Particularly note that, because of this, except the application owner and NodeManager, no other user can access any of the local files/directories including those localized as part of the distributed cache.To build the LinuxContainerExecutor executable run:$ mvn package -Dcontainer-executor.conf.dir=/etc/hadoop/The path passed in-Dcontainer-executor.conf.dirshould be the path on the cluster nodes where a configuration file for the setuid executable should be located. The executable should be installed in$HADOOP_YARN_HOME/bin.The executable must have specific permissions: 6050 or--Sr-s---permissions user-owned byroot(super-user) and group-owned by a special group (e.g.hadoop) of which the NodeManager Unix user is the group member and no ordinary application user is. If any application user belongs to this special group, security will be compromised. This special group name should be specified for the configuration propertyyarn.nodemanager.linux-container-executor.groupin bothconf/yarn-site.xmlandconf/container-executor.cfg.For example, let’s say that the NodeManager is run as useryarnwho is part of the groupsusersandhadoop, any of them being the primary group. Let also be thatusershas bothyarnand another user (application submitter)aliceas its members, andalicedoes not belong tohadoop. Going by the above description, the setuid/setgid executable should be set 6050 or--Sr-s---with user-owner asyarnand group-owner ashadoopwhich hasyarnas its member (and notuserswhich hasalicealso as its member besidesyarn).The LinuxTaskController requires that paths including and leading up to the directories specified inyarn.nodemanager.local-dirsandyarn.nodemanager.log-dirsto be set 755 permissions as described above in the table on permissions on directories.conf/container-executor.cfgThe executable requires a configuration file calledcontainer-executor.cfgto be present in the configuration directory passed to the mvn target mentioned above.The configuration file must be owned by the user running NodeManager (useryarnin the above example), group-owned by anyone and should have the permissions 0400 orr--------.The executable requires following configuration items to be present in theconf/container-executor.cfgfile. The items should be mentioned as simple key=value pairs, one per-line:ParameterValueNotesyarn.nodemanager.linux-container-executor.grouphadoopUnix group of the NodeManager. The group owner of thecontainer-executorbinary should be this group. Should be same as the value with which the NodeManager is configured. This configuration is required for validating the secure access of thecontainer-executorbinary.banned.usershdfs,yarn,mapred,binBanned users.allowed.system.usersfoo,barAllowed system users.min.user.id1000Prevent other super-users.To re-cap, here are the local file-sysytem permissions required for the various paths related to theLinuxContainerExecutor:FilesystemPathUser:GroupPermissionslocalcontainer-executorroot:hadoop--Sr-s--*localconf/container-executor.cfgroot:hadoopr-------*localyarn.nodemanager.local-dirsyarn:hadoopdrwxr-xr-xlocalyarn.nodemanager.log-dirsyarn:hadoopdrwxr-xr-xMapReduce JobHistory ServerParameterValueNotesmapreduce.jobhistory.addressMapReduce JobHistory Serverhost:portDefault port is 10020.mapreduce.jobhistory.keytab/etc/security/keytab/jhs.service.keytabKerberos keytab file for the MapReduce JobHistory Server.mapreduce.jobhistory.principaljhs/_HOST@REALM.TLDKerberos principal name for the MapReduce JobHistory Server.MultihomingMultihomed setups where each host has multiple hostnames in DNS (e.g. different hostnames corresponding to public and private network interfaces) may require additional configuration to get Kerberos authentication working. SeeHDFS Support for Multihomed NetworksTroubleshootingKerberos is hard to set up —and harder to debug. Common problems areNetwork and DNS configuration.Kerberos configuration on hosts (/etc/krb5.conf).Keytab creation and maintenance.Environment setup: JVM, user login, system clocks, etc.The fact that the error messages from the JVM are essentially meaningless does not aid in diagnosing and fixing such problems.Extra debugging information can be enabled for the client and for any serviceSet the environment variableHADOOP_JAAS_DEBUGtotrue.export HADOOP_JAAS_DEBUG=trueEdit thelog4j.propertiesfile to log Hadoop’s security package atDEBUGlevel.log4j.logger.org.apache.hadoop.security=DEBUGEnable JVM-level debugging by setting some system properties.export HADOOP_OPTS="-Djava.net.preferIPv4Stack=true -Dsun.security.krb5.debug=true -Dsun.security.spnego.debug"Troubleshooting withKDiagHadoop has a tool to aid validating setup:KDiagIt contains a series of probes for the JVM’s configuration and the environment, dumps out some system files (/etc/krb5.conf,/etc/ntp.conf), prints out some system state and then attempts to log in to Kerberos as the current user, or a specific principal in a named keytab.The output of the command can be used for local diagnostics, or forwarded to whoever supports the cluster.TheKDiagcommand has its own entry point; It is invoked by passingkdiagtobin/hadoopcommand. Accordingly, it will display the kerberos client state of the command used to invoke it.hadoop kdiagThe command returns a status code of 0 for a successful diagnostics run. This does not imply that Kerberos is working —merely that the KDiag command did not identify any problem from its limited set of probes. In particular, as it does not attempt to connect to any remote service, it does not verify that the client is trusted by any service.If unsuccessful, exit codes are-1: the command failed for an unknown reason41: Unauthorized (== HTTP’s 401). KDiag detected a condition which causes Kerberos to not work. Examine the output to identify the issue.UsageKDiag: Diagnose Kerberos Problems
  [-D key=value] : Define a configuration option.
  [--jaas] : Require a JAAS file to be defined in java.security.auth.login.config.
  [--keylen <keylen>] : Require a minimum size for encryption keys supported by the JVM. Default value : 256.
  [--keytab <keytab> --principal <principal>] : Login from a keytab as a specific principal.
  [--nofail] : Do not fail on the first problem.
  [--nologin] : Do not attempt to log in.
  [--out <file>] : Write output to a file.
  [--resource <resource>] : Load an XML configuration resource.
  [--secure] : Require the hadoop configuration to be secure.
  [--verifyshortname <principal>]: Verify the short name of the specific principal does not contain '@' or '/'--jaas: Require a JAAS file to be defined injava.security.auth.login.config.If--jaasis set, the Java system propertyjava.security.auth.login.configmust be set to a JAAS file; this file must exist, be a simple file of non-zero bytes, and readable by the current user. More detailed validation is not performed.JAAS files are not needed by Hadoop itself, but some services (such as Zookeeper) do require them for secure operation.--keylen <length>: Require a minimum size for encryption keys supported by the JVM".If the JVM does not support this length, the command will fail.The default value is to 256, as needed for theAES256encryption scheme. A JVM without the Java Cryptography Extensions installed does not support such a key length. Kerberos will not work unless configured to use an encryption scheme with a shorter key length.--keytab <keytab> --principal <principal>: Log in from a keytab.Log in from a keytab as the specific principal.The file must contain the specific principal, including any named host. That is, there is no mapping from_HOSTto the current hostname.KDiag will log out and attempt to log back in again. This catches JVM compatibility problems which have existed in the past. (Hadoop’s Kerberos support requires use of/introspection into JVM-specific classes).--nofail: Do not fail on the first problemKDiag will make a best-effort attempt to diagnose all Kerberos problems, rather than stop at the first one.This is somewhat limited; checks are made in the order which problems surface (e.g keylength is checked first), so an early failure can trigger many more problems. But it does produce a more detailed report.--nologin: Do not attempt to log in.Skip trying to log in. This takes precedence over the--keytaboption, and also disables trying to log in to kerberos as the current kinited user.This is useful when the KDiag command is being invoked within an application, as it does not set up Hadoop’s static security state —merely check for some basic Kerberos preconditions.--out outfile: Write output to file.hadoop kdiag --out out.txtMuch of the diagnostics information comes from the JRE (tostderr) and from Log4j (tostdout). To get all the output, it is best to redirect both these output streams to the same file, and omit the--outoption.hadoop kdiag --keytab zk.service.keytab --principal zookeeper/devix.example.org@REALM > out.txt 2>&1Even there, the output of the two streams, emitted across multiple threads, can be a bit confusing. It will get easier with practise. Looking at the thread name in the Log4j output to distinguish background threads from the main thread helps at the hadoop level, but doesn’t assist in JVM-level logging.--resource <resource>: XML configuration resource to load.To load XML configuration files, this option can be used. As by default, thecore-defaultandcore-siteXML resources are only loaded. This will help, when additional configuration files has any Kerberos related configurations.hadoop kdiag --resource hbase-default.xml --resource hbase-site.xmlFor extra logging during the operation, set the logging andHADOOP_JAAS_DEBUGenvironment variable to the values listed in “Troubleshooting”. The JVM options are automatically set in KDiag.--secure: Fail if the command is not executed on a secure cluster.That is: if the authentication mechanism of the cluster is explicitly or implicitly set to “simple”:<property>
  <name>hadoop.security.authentication</name>
  <value>simple</value>
</property>Needless to say, an application so configured cannot talk to a secure Hadoop cluster.--verifyshortname <principal>: validate the short name of a principalThis verifies that the short name of a principal contains neither the"@"nor"/"characters.Examplehadoop kdiag \
  --nofail \
  --resource hdfs-site.xml --resource yarn-site.xml \
  --keylen 1024 \
  --keytab zk.service.keytab --principal zookeeper/devix.example.org@REALMThis attempts to perform all diagnostics without failing early, load in the HDFS and YARN XML resources, require a minimum key length of 1024 bytes, and log in as the principalzookeeper/devix.example.org@REALM, whose key must be in the keytabzk.service.keytabReferencesO’Malley O et al.Hadoop Security DesignO’Malley O,Hadoop Security ArchitectureTroubleshooting Kerberos on Java 7Troubleshooting Kerberos on Java 8Java 7 Kerberos RequirementsJava 8 Kerberos RequirementsLoughran S.,Hadoop and Kerberos: The Madness beyond the Gate©            2008-2024
              Apache Software Foundation
            
                          -Privacy Policy.
        Apache Maven, Maven, Apache, the Apache feather logo, and the Apache Maven project logos are trademarks of The Apache Software Foundation.