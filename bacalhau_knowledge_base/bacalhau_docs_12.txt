URL: https://docs.bacalhau.org/getting-started/workload-onboarding/docker-workload-onboarding

Bacalhau Docsv.1.4.0v.1.3.0v.1.3.1v.1.3.2v.1.4.0GitHubSlackContactMoreGitHubSlackContactAsk or SearchCtrl+ KWelcomeGetting StartedHow Bacalhau WorksInstallationCreate NetworkHardware SetupContainer OnboardingDocker WorkloadsWebAssembly (Wasm) WorkloadsSetting UpRunning NodesNode OnboardingGPU InstallationJob selection policyAccess ManagementNode persistenceConnect StorageConfiguration ManagementConfiguring Transport Level SecurityLimits and TimeoutsTest Network LocallyBacalhau WebUIWorkload OnboardingContainerDocker Workload OnboardingWebAssembly (Wasm) WorkloadsBacalhau Docker ImageHow To Work With Custom Containers in BacalhauPythonBuilding and Running Custom Python ContainerRunning Pandas on BacalhauRunning a Python ScriptRunning Jupyter Notebooks on BacalhauScripting Bacalhau with PythonR (language)Building and Running your Custom R Containers on BacalhauRunning a Simple R Script on BacalhauRun CUDA programs on BacalhauRunning a Prolog ScriptReading Data from Multiple S3 Buckets using BacalhauRunning Rust programs as WebAssembly (WASM)Generate Synthetic Data using Sparkov Data Generation techniqueData IngestionCopy Data from URL to Public StoragePinning DataRunning a Job over S3 dataNetworking InstructionsAccessing the Internet from JobsUtilizing NATS.io within BacalhauGPU Workloads SetupAutomatic Update CheckingMarketplace DeploymentsGoogle Cloud MarketplaceGuidesWrite a config.yamlWrite a SpecConfigExamplesData EngineeringUsing Bacalhau with DuckDBEthereum Blockchain Analysis with Ethereum-ETL and BacalhauConvert CSV To Parquet Or AvroSimple Image ProcessingOceanography - Data ConversionVideo ProcessingModel InferenceEasyOCR (Optical Character Recognition) on BacalhauRunning Inference on Dolly 2.0 Model with Hugging FaceSpeech Recognition using WhisperStable Diffusion on a GPUStable Diffusion on a CPUObject Detection with YOLOv5 on BacalhauGenerate Realistic Images using StyleGAN3 and BacalhauStable Diffusion Checkpoint InferenceRunning Inference on a Model stored on S3Model TrainingTraining Pytorch Model with BacalhauTraining Tensorflow ModelStable Diffusion Dreambooth (Finetuning)Molecular DynamicsRunning BIDS Apps on BacalhauCoresets On BacalhauGenomics Data GenerationGromacs for AnalysisMolecular Simulation with OpenMM and BacalhauReferencesJobs GuideJob SpecificationJob TypesTask SpecificationEnginesDocker Engine SpecificationWebAssembly (WASM) Engine SpecificationPublishersIPFS Publisher SpecificationLocal Publisher SpecificationS3 Publisher SpecificationSourcesIPFS Source SpecificationLocal Source SpecificationS3 Source SpecificationURL Source SpecificationNetwork SpecificationInput Source SpecificationResources SpecificationResultPath SpecificationConstraint SpecificationLabels SpecificationMeta SpecificationJob TemplatesQueuing & TimeoutsJob QueuingTimeouts SpecificationJob ResultsStateCLI GuideSingle CLI commandsAgentAgent OverviewAgent AliveAgent NodeAgent VersionConfigConfig OverviewConfig Auto-ResourcesConfig DefaultConfig ListConfig SetJobJob OverviewJob DescribeJob ExecJob ExecutionsJob HistoryJob ListJob LogsJob RunJob StopNodeNode OverviewNode ApproveNode DeleteNode ListNode DescribeNode RejectCLI Commands OverviewCommand MigrationAPI GuideBacalhau API overviewBest PracticesAgent EndpointOrchestrator EndpointMigration APINode ManagementAuthentication & AuthorizationDatabase IntegrationDebuggingDebugging Failed JobsDebugging LocallyOpen Telemetry in BacalhauRunning locally in 'devstack'Setting up Dev EnvironmentHelp & FAQBacalhau FAQsRelease NotesGlossaryIntegrationsApache Airflow Provider for BacalhauLilypadBacalhau Python SDKObservability for WebAssembly WorkloadsCommunitySocial MediaStyle GuideWays to ContributePowered by GitBookDocker WorkloadsHow to use docker containers with BacalhauDocker WorkloadsBacalhau executes jobs by running them within containers. Bacalhau employs a syntax closely resembling Docker, allowing you to utilize the same containers. The key distinction lies in how input and output data are transmitted to the container via IPFS, enabling scalability on a global level.This section describes how to migrate a workload based on a Docker container into a format that will work with the Bacalhau client.You can check out this example tutorial onhow to work with custom containers in Bacalhauto see how we used all these steps together.RequirementsHere are few things to note before getting started:Container Registry: Ensure that the container is published to a public container registry that is accessible from the Bacalhau network.Architecture Compatibility: Bacalhau supports only images that match the host node's architecture. Typically, most nodes run onlinux/amd64, so containers inarm64format are not able to run.Input Flags: The--input ipfs://...flag supports onlydirectoriesand does not support CID subpaths. The--input https://...flag supports onlysingle filesand does not support URL directories. The--input s3://...flag supports S3 keys and prefixes. For example,s3://bucket/logs-2023-04*includes all logs for April 2023.You can check to see alist of example public containersused by the Bacalhau teamNote: Only about a third of examples have their containers here. If you can't find one, feel free to contact the team.Runtime RestrictionsTo help provide a safe, secure network for all users, we add the following runtime restrictions:Limited Ingress/Egress Networking:All ingress/egress networking is limited as described in thenetworkingdocumentation. You won't be able to pulldata/code/weights/etc. from an external source.Data Passing with Docker Volumes:A job includes the concept of input and output volumes, and the Docker executor implements support for these. This means you can specify your CIDs, URLs, and/or S3 objects asinputpaths and also write results to anoutputvolume. This can be seen in the following example:Copybacalhaudockerrun\-is3://mybucket/logs-2023-04*:/input\-oapples:/output_folder\ubuntu\bash-c'ls /input > /output_folder/file.txt'The above example demonstrates an input volume flag-i s3://mybucket/logs-2023-04*, which mounts all S3 objects in bucketmybucketwithlogs-2023-04prefix within the docker container at location/input(root).Output volumes are mounted to the Docker container at the location specified. In the example above, any content written to/output_folderwill be made available within theapplesfolder in the job results CID.Once the job has run on the executor, the contents ofstdoutandstderrwill be added to any named output volumes the job has used (in this caseapples), and all those entities will be packaged into the results folder which is then published to a remote location by the publisher.Onboarding Your WorkloadStep 1 - Read Data From Your DirectoryIf you need to pass data into your container you will do this through a Docker volume. You'll need to modify your code to read from a local directory.We make the assumption that you are reading from a directory called/inputs, which is set as the default.You can specify which directory the data is written to with the--inputCLI flag.Step 2 - Write Data to the Your DirectoryIf you need to return data from your container you will do this through a Docker volume. You'll need to modify your code to write to a local directory.We make the assumption that you are writing to a directory called/outputs, which is set as the default.You can specify which directory the data is written to with the--output-volumesCLI flag.Step 3 - Build and Push Your Image To a RegistryAt this step, you create (or update) a Docker image that Bacalhau will use to perform your task. Youbuild your imagefrom your code and dependencies, thenpush itto a public registry so that Bacalhau can access it. This is necessary for other Bacalhau nodes to run your container and execute the task.Most Bacalhau nodes are of anx86_64architecture, therefore containers should be built forx86_64systems.For example:CopyexportIMAGE=myuser/myimage:latestdockerbuild-t${IMAGE}.dockerimagepush${IMAGE}Step 4 - Test Your ContainerTo test your docker image locally, you'll need to execute the following command, changing the environment variables as necessary:CopyexportLOCAL_INPUT_DIR=$PWDexportLOCAL_OUTPUT_DIR=$PWDexportCMD=(sh-c'ls /inputs; echo do something useful > /outputs/stdout')dockerrun--rm\-v${LOCAL_INPUT_DIR}:/inputs\-v${LOCAL_OUTPUT_DIR}:/outputs\${IMAGE} \${CMD}Let's see what each command will be used for:CopyexportLOCAL_INPUT_DIR=$PWDExports the current working directory of the host system to theLOCAL_INPUT_DIRvariable. This variable will be used for binding a volume and transferring data into the container.CopyexportLOCAL_OUTPUT_DIR=$PWDExports the current working directory of the host system to the LOCAL_OUTPUT_DIR variable. Similarly, this variable will be used for binding a volume and transferring data from the container.CopyexportCMD=(sh-c'ls /inputs; echo do something useful > /outputs/stdout')Creates an array of commands CMD that will be executed inside the container. In this case, it is a simple command executing 'ls' in the /inputs directory and writing text to the /outputs/stdout file.Copydockerrun...${IMAGE} ${CMD}Launches a Docker container using the specified variables and commands. It binds volumes to facilitate data exchange between the host and the container.Bacalhau will use thedefault ENTRYPOINTif your image contains one. If you need to specify another entrypoint, use the--entrypointflag tobacalhau docker run.For example:CopyexportLOCAL_INPUT_DIR=$PWDexportLOCAL_OUTPUT_DIR=$PWDexportCMD=(sh-c'ls /inputs; echo "do something useful" > /outputs/stdout')exportIMAGE=ubuntudockerrun--rm\-v${LOCAL_INPUT_DIR}:/inputs\-v${LOCAL_OUTPUT_DIR}:/outputs\${IMAGE} \${CMD}catstdoutThe result of the commands' execution is shown below:Copydo something usefulStep 5 - Upload the Input DataData is identified by its content identifier (CID) and can be accessed by anyone who knows the CID. You can use either of these methods to upload your data:You can choose toCopy data from a URL to public storagePin Data to public storageCopy Data from S3 Bucket to public storage.You can mount your data anywhere on your machine, and Bacalhau will be able to run against that dataStep 6 - Run the Workload on BacalhauTo launch your workload in a Docker container, using the specified image and working withinputdata specified via IPFS CID, run the following command.Copybacalhaudockerrun--inputipfs://${CID} ${IMAGE} ${CMD}To check the status of your job, run the following command.Copybacalhaujoblist--id-filterJOB_IDTo get more information on your job, you can run the following command.CopybacalhaujobdescribeJOB_IDTo download your job, run.CopybacalhaujobgetJOB_IDTo put this all together into one would look like the following.CopyJOB_ID=$(bacalhaudockerrunubuntuechohello|grep'Job ID:'|sed's/.*Job ID: \([^ ]*\).*/\1/')echo"The job ID is: $JOB_ID"bacalhaujoblist--id-filter$JOB_IDsleep5bacalhaujoblist--id-filter$JOB_IDbacalhauget$JOB_IDlsshardsThis outputs the following.CopyCREATEDIDJOBSTATEVERIFIEDPUBLISHED10:26:0024440f0dDockerubuntuechoh...VerifyingCREATEDIDJOBSTATEVERIFIEDPUBLISHED10:26:0024440f0dDockerubuntuechoh...Published/ipfs/bafybeiflj3kha...11:26:09.107|INFbacalhau/get.go:67>Fetchingresultsofjob'24440f0d-3c06-46af-9adf-cb524aa43961'...11:26:10.528|INFipfs/downloader.go:115>Found1resultshards,downloadingtotemporaryfolder.11:26:13.144 | INF ipfs/downloader.go:195 > Combining shard from output volume 'outputs' to final location: '/Users/phil/source/filecoin-project/docs.bacalhau.org'job-24440f0d-3c06-46af-9adf-cb524aa43961-shard-0-host-QmYgxZiySj3MRkwLSL4X2MF5F9f2PMhAE3LV49XkfNL1o3The--inputflag does not support CID subpaths foripfs://content.Alternatively, you can run your workload with a publicly accessible http(s) URL, which will download the data temporarily into your public storage:CopyexportURL=https://download.geofabrik.de/antarctica-latest.osm.pbfbacalhaudockerrun--input${URL} ${IMAGE} ${CMD}bacalhaujoblistbacalhaujobgetJOB_IDThe--inputflag does not support URL directories.TroubleshootingIf you run into this compute error while running your docker imageCopyCreating job for submission ... done ✅Finding node(s) for the job ... done ✅Node accepted the job ... done ✅Error while executing the job.This can often be resolved by re-tagging your docker imageSupportIf you have questions or need support or guidance, please reach out to theBacalhau team via Slack(#generalchannel).PreviousContainer OnboardingNextWebAssembly (Wasm) WorkloadsLast updated1 month agoOn this pageDocker WorkloadsRequirementsRuntime RestrictionsOnboarding Your WorkloadTroubleshootingSupportWas this helpful?Edit on GitHubExport as PDFGet SupportExpansoSupportUse CasesDistributed ETLEdge MLDistributed Data WarehousingFlett ManagementAbout UsWho we areWhat we valueNews & BlogBlogNewsExpanso (2024). All Rights Reserved.