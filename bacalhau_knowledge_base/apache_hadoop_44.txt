URL: https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/ClusterSetup.html

Wiki|git|Apache Hadoop| Last Published: 2024-03-04
               | Version: 3.4.0GeneralOverviewSingle Node SetupCluster SetupCommands ReferenceFileSystem ShellCompatibility SpecificationDownstream Developer's GuideAdmin Compatibility GuideInterface ClassificationFileSystem SpecificationCommonCLI Mini ClusterFair Call QueueNative LibrariesProxy UserRack AwarenessSecure ModeService Level AuthorizationHTTP AuthenticationCredential Provider APIHadoop KMSTracingUnix Shell GuideRegistryAsync ProfilerHDFSArchitectureUser GuideCommands ReferenceNameNode HA With QJMNameNode HA With NFSObserver NameNodeFederationViewFsViewFsOverloadSchemeSnapshotsEdits ViewerImage ViewerPermissions and HDFSQuotas and HDFSlibhdfs (C API)WebHDFS (REST API)HttpFSShort Circuit Local ReadsCentralized Cache ManagementNFS GatewayRolling UpgradeExtended AttributesTransparent EncryptionMultihomingStorage PoliciesMemory Storage SupportSynthetic Load GeneratorErasure CodingDisk BalancerUpgrade DomainDataNode AdminRouter FederationProvided StorageMapReduceTutorialCommands ReferenceCompatibility with 1.xEncrypted ShufflePluggable Shuffle/SortDistributed Cache DeploySupport for YARN Shared CacheMapReduce REST APIsMR Application MasterMR History ServerYARNArchitectureCommands ReferenceCapacity SchedulerFair SchedulerResourceManager RestartResourceManager HAResource ModelNode LabelsNode AttributesWeb Application ProxyTimeline ServerTimeline Service V.2Writing YARN ApplicationsYARN Application SecurityNodeManagerRunning Applications in Docker ContainersRunning Applications in runC ContainersUsing CGroupsSecure ContainersReservation SystemGraceful DecommissionOpportunistic ContainersYARN FederationShared CacheUsing GPUUsing FPGAPlacement ConstraintsYARN UI2YARN REST APIsIntroductionResource ManagerNode ManagerTimeline ServerTimeline Service V.2YARN ServiceOverviewQuickStartConceptsYarn Service APIService DiscoverySystem ServicesHadoop Compatible File SystemsAliyun OSSAmazon S3Azure Blob StorageAzure Data Lake StorageTencent COSHuaweicloud OBSAuthOverviewExamplesConfigurationBuildingToolsHadoop StreamingHadoop ArchivesHadoop Archive LogsDistCpHDFS Federation BalanceGridMixRumenResource Estimator ServiceScheduler Load SimulatorHadoop BenchmarkingDynamometerReferenceChangelog and Release NotesJava API docsUnix Shell APIMetricsConfigurationcore-default.xmlhdfs-default.xmlhdfs-rbf-default.xmlmapred-default.xmlyarn-default.xmlkms-default.xmlhttpfs-default.xmlDeprecated PropertiesHadoop Cluster SetupPurposePrerequisitesInstallationConfiguring Hadoop in Non-Secure ModeConfiguring Environment of Hadoop DaemonsConfiguring the Hadoop DaemonsMonitoring Health of NodeManagersSlaves FileHadoop Rack AwarenessLoggingOperating the Hadoop ClusterHadoop StartupHadoop ShutdownWeb InterfacesPurposeThis document describes how to install and configure Hadoop clusters ranging from a few nodes to extremely large clusters with thousands of nodes. To play with Hadoop, you may first want to install it on a single machine (seeSingle Node Setup).This document does not cover advanced topics such as High Availability.Important: all production Hadoop clusters use Kerberos to authenticate callers and secure access to HDFS data as well as restriction access to computation services (YARN etc.).These instructions do not cover integration with any Kerberos services, -everyone bringing up a production cluster should include connecting to their organisation’s Kerberos infrastructure as a key part of the deployment.SeeSecurityfor details on how to secure a cluster.PrerequisitesInstall Java. See theHadoop Wikifor known good versions.Download a stable version of Hadoop from Apache mirrors.InstallationInstalling a Hadoop cluster typically involves unpacking the software on all the machines in the cluster or installing it via a packaging system as appropriate for your operating system. It is important to divide up the hardware into functions.Typically one machine in the cluster is designated as the NameNode and another machine as the ResourceManager, exclusively. These are the masters. Other services (such as Web App Proxy Server and MapReduce Job History server) are usually run either on dedicated hardware or on shared infrastructure, depending upon the load.The rest of the machines in the cluster act as both DataNode and NodeManager. These are the workers.Configuring Hadoop in Non-Secure ModeHadoop’s Java configuration is driven by two types of important configuration files:Read-only default configuration -core-default.xml,hdfs-default.xml,yarn-default.xmlandmapred-default.xml.Site-specific configuration -etc/hadoop/core-site.xml,etc/hadoop/hdfs-site.xml,etc/hadoop/yarn-site.xmlandetc/hadoop/mapred-site.xml.Additionally, you can control the Hadoop scripts found in the bin/ directory of the distribution, by setting site-specific values via theetc/hadoop/hadoop-env.shandetc/hadoop/yarn-env.sh.To configure the Hadoop cluster you will need to configure theenvironmentin which the Hadoop daemons execute as well as theconfiguration parametersfor the Hadoop daemons.HDFS daemons are NameNode, SecondaryNameNode, and DataNode. YARN daemons are ResourceManager, NodeManager, and WebAppProxy. If MapReduce is to be used, then the MapReduce Job History Server will also be running. For large installations, these are generally running on separate hosts.Configuring Environment of Hadoop DaemonsAdministrators should use theetc/hadoop/hadoop-env.shand optionally theetc/hadoop/mapred-env.shandetc/hadoop/yarn-env.shscripts to do site-specific customization of the Hadoop daemons’ process environment.At the very least, you must specify theJAVA_HOMEso that it is correctly defined on each remote node.Administrators can configure individual daemons using the configuration options shown below in the table:DaemonEnvironment VariableNameNodeHDFS_NAMENODE_OPTSDataNodeHDFS_DATANODE_OPTSSecondary NameNodeHDFS_SECONDARYNAMENODE_OPTSResourceManagerYARN_RESOURCEMANAGER_OPTSNodeManagerYARN_NODEMANAGER_OPTSWebAppProxyYARN_PROXYSERVER_OPTSMap Reduce Job History ServerMAPRED_HISTORYSERVER_OPTSFor example, To configure Namenode to use parallelGC and a 4GB Java Heap, the following statement should be added in hadoop-env.sh :export HDFS_NAMENODE_OPTS="-XX:+UseParallelGC -Xmx4g"Seeetc/hadoop/hadoop-env.shfor other examples.Other useful configuration parameters that you can customize include:HADOOP_PID_DIR- The directory where the daemons’ process id files are stored.HADOOP_LOG_DIR- The directory where the daemons’ log files are stored. Log files are automatically created if they don’t exist.HADOOP_HEAPSIZE_MAX- The maximum amount of memory to use for the Java heapsize. Units supported by the JVM are also supported here. If no unit is present, it will be assumed the number is in megabytes. By default, Hadoop will let the JVM determine how much to use. This value can be overriden on a per-daemon basis using the appropriate_OPTSvariable listed above. For example, settingHADOOP_HEAPSIZE_MAX=1gandHADOOP_NAMENODE_OPTS="-Xmx5g"will configure the NameNode with 5GB heap.In most cases, you should specify theHADOOP_PID_DIRandHADOOP_LOG_DIRdirectories such that they can only be written to by the users that are going to run the hadoop daemons. Otherwise there is the potential for a symlink attack.It is also traditional to configureHADOOP_HOMEin the system-wide shell environment configuration. For example, a simple script inside/etc/profile.d:HADOOP_HOME=/path/to/hadoop
  export HADOOP_HOMEConfiguring the Hadoop DaemonsThis section deals with important parameters to be specified in the given configuration files:etc/hadoop/core-site.xmlParameterValueNotesfs.defaultFSNameNode URIhdfs://host:port/io.file.buffer.size131072Size of read/write buffer used in SequenceFiles.etc/hadoop/hdfs-site.xmlConfigurations for NameNode:ParameterValueNotesdfs.namenode.name.dirPath on the local filesystem where the NameNode stores the namespace and transactions logs persistently.If this is a comma-delimited list of directories then the name table is replicated in all of the directories, for redundancy.dfs.hosts/dfs.hosts.excludeList of permitted/excluded DataNodes.If necessary, use these files to control the list of allowable datanodes.dfs.blocksize268435456HDFS blocksize of 256MB for large file-systems.dfs.namenode.handler.count100More NameNode server threads to handle RPCs from large number of DataNodes.Configurations for DataNode:ParameterValueNotesdfs.datanode.data.dirComma separated list of paths on the local filesystem of aDataNodewhere it should store its blocks.If this is a comma-delimited list of directories, then data will be stored in all named directories, typically on different devices.etc/hadoop/yarn-site.xmlConfigurations for ResourceManager and NodeManager:ParameterValueNotesyarn.acl.enabletrue/falseEnable ACLs? Defaults tofalse.yarn.admin.aclAdmin ACLACL to set admins on the cluster. ACLs are of forcomma-separated-usersspacecomma-separated-groups. Defaults to special value of*which meansanyone. Special value of justspacemeans no one has access.yarn.log-aggregation-enablefalseConfiguration to enable or disable log aggregationConfigurations for ResourceManager:ParameterValueNotesyarn.resourcemanager.addressResourceManagerhost:port for clients to submit jobs.host:portIf set, overrides the hostname set inyarn.resourcemanager.hostname.yarn.resourcemanager.scheduler.addressResourceManagerhost:port for ApplicationMasters to talk to Scheduler to obtain resources.host:portIf set, overrides the hostname set inyarn.resourcemanager.hostname.yarn.resourcemanager.resource-tracker.addressResourceManagerhost:port for NodeManagers.host:portIf set, overrides the hostname set inyarn.resourcemanager.hostname.yarn.resourcemanager.admin.addressResourceManagerhost:port for administrative commands.host:portIf set, overrides the hostname set inyarn.resourcemanager.hostname.yarn.resourcemanager.webapp.addressResourceManagerweb-ui host:port.host:portIf set, overrides the hostname set inyarn.resourcemanager.hostname.yarn.resourcemanager.hostnameResourceManagerhost.hostSingle hostname that can be set in place of setting allyarn.resourcemanager*addressresources. Results in default ports for ResourceManager components.yarn.resourcemanager.scheduler.classResourceManagerScheduler class.CapacityScheduler(recommended),FairScheduler(also recommended), orFifoScheduler. Use a fully qualified class name, e.g.,org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.yarn.scheduler.minimum-allocation-mbMinimum limit of memory to allocate to each container request at theResource Manager.In MBsyarn.scheduler.maximum-allocation-mbMaximum limit of memory to allocate to each container request at theResource Manager.In MBsyarn.resourcemanager.nodes.include-path/yarn.resourcemanager.nodes.exclude-pathList of permitted/excluded NodeManagers.If necessary, use these files to control the list of allowable NodeManagers.Configurations for NodeManager:ParameterValueNotesyarn.nodemanager.resource.memory-mbResource i.e. available physical memory, in MB, for givenNodeManagerDefines total available resources on theNodeManagerto be made available to running containersyarn.nodemanager.vmem-pmem-ratioMaximum ratio by which virtual memory usage of tasks may exceed physical memoryThe virtual memory usage of each task may exceed its physical memory limit by this ratio. The total amount of virtual memory used by tasks on the NodeManager may exceed its physical memory usage by this ratio.yarn.nodemanager.local-dirsComma-separated list of paths on the local filesystem where intermediate data is written.Multiple paths help spread disk i/o.yarn.nodemanager.log-dirsComma-separated list of paths on the local filesystem where logs are written.Multiple paths help spread disk i/o.yarn.nodemanager.log.retain-seconds10800Default time (in seconds) to retain log files on the NodeManager Only applicable if log-aggregation is disabled.yarn.nodemanager.remote-app-log-dir/logsHDFS directory where the application logs are moved on application completion. Need to set appropriate permissions. Only applicable if log-aggregation is enabled.yarn.nodemanager.remote-app-log-dir-suffixlogsSuffix appended to the remote log dir. Logs will be aggregated to ${yarn.nodemanager.remote-app-log-dir}/${user}/${thisParam} Only applicable if log-aggregation is enabled.yarn.nodemanager.aux-servicesmapreduce_shuffleShuffle service that needs to be set for Map Reduce applications.yarn.nodemanager.env-whitelistEnvironment properties to be inherited by containers from NodeManagersFor mapreduce application in addition to the default values HADOOP_MAPRED_HOME should to be added. Property value should JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_HOME,PATH,LANG,TZ,HADOOP_MAPRED_HOMEConfigurations for History Server (Needs to be moved elsewhere):ParameterValueNotesyarn.log-aggregation.retain-seconds-1How long to keep aggregation logs before deleting them. -1 disables. Be careful, set this too small and you will spam the name node.yarn.log-aggregation.retain-check-interval-seconds-1Time between checks for aggregated log retention. If set to 0 or a negative value then the value is computed as one-tenth of the aggregated log retention time. Be careful, set this too small and you will spam the name node.etc/hadoop/mapred-site.xmlConfigurations for MapReduce Applications:ParameterValueNotesmapreduce.framework.nameyarnExecution framework set to Hadoop YARN.mapreduce.map.memory.mb1536Larger resource limit for maps.mapreduce.map.java.opts-Xmx1024MLarger heap-size for child jvms of maps.mapreduce.reduce.memory.mb3072Larger resource limit for reduces.mapreduce.reduce.java.opts-Xmx2560MLarger heap-size for child jvms of reduces.mapreduce.task.io.sort.mb512Higher memory-limit while sorting data for efficiency.mapreduce.task.io.sort.factor100More streams merged at once while sorting files.mapreduce.reduce.shuffle.parallelcopies50Higher number of parallel copies run by reduces to fetch outputs from very large number of maps.Configurations for MapReduce JobHistory Server:ParameterValueNotesmapreduce.jobhistory.addressMapReduce JobHistory Serverhost:portDefault port is 10020.mapreduce.jobhistory.webapp.addressMapReduce JobHistory Server Web UIhost:portDefault port is 19888.mapreduce.jobhistory.intermediate-done-dir/mr-history/tmpDirectory where history files are written by MapReduce jobs.mapreduce.jobhistory.done-dir/mr-history/doneDirectory where history files are managed by the MR JobHistory Server.Monitoring Health of NodeManagersHadoop provides a mechanism by which administrators can configure the NodeManager to run an administrator supplied script periodically to determine if a node is healthy or not.Administrators can determine if the node is in a healthy state by performing any checks of their choice in the script. If the script detects the node to be in an unhealthy state, it must print a line to standard output beginning with the string ERROR. The NodeManager spawns the script periodically and checks its output. If the script’s output contains the string ERROR, as described above, the node’s status is reported asunhealthyand the node is black-listed by the ResourceManager. No further tasks will be assigned to this node. However, the NodeManager continues to run the script, so that if the node becomes healthy again, it will be removed from the blacklisted nodes on the ResourceManager automatically. The node’s health along with the output of the script, if it is unhealthy, is available to the administrator in the ResourceManager web interface. The time since the node was healthy is also displayed on the web interface.The following parameters can be used to control the node health monitoring script inetc/hadoop/yarn-site.xml.ParameterValueNotesyarn.nodemanager.health-checker.script.pathNode health scriptScript to check for node’s health status.yarn.nodemanager.health-checker.script.optsNode health script optionsOptions for script to check for node’s health status.yarn.nodemanager.health-checker.interval-msNode health script intervalTime interval for running health script.yarn.nodemanager.health-checker.script.timeout-msNode health script timeout intervalTimeout for health script execution.The health checker script is not supposed to give ERROR if only some of the local disks become bad. NodeManager has the ability to periodically check the health of the local disks (specifically checks nodemanager-local-dirs and nodemanager-log-dirs) and after reaching the threshold of number of bad directories based on the value set for the config property yarn.nodemanager.disk-health-checker.min-healthy-disks, the whole node is marked unhealthy and this info is sent to resource manager also. The boot disk is either raided or a failure in the boot disk is identified by the health checker script.Slaves FileList all worker hostnames or IP addresses in youretc/hadoop/workersfile, one per line. Helper scripts (described below) will use theetc/hadoop/workersfile to run commands on many hosts at once. It is not used for any of the Java-based Hadoop configuration. In order to use this functionality, ssh trusts (via either passphraseless ssh or some other means, such as Kerberos) must be established for the accounts used to run Hadoop.Hadoop Rack AwarenessMany Hadoop components are rack-aware and take advantage of the network topology for performance and safety. Hadoop daemons obtain the rack information of the workers in the cluster by invoking an administrator configured module. See theRack Awarenessdocumentation for more specific information.It is highly recommended configuring rack awareness prior to starting HDFS.LoggingHadoop uses theApache log4jvia the Apache Commons Logging framework for logging. Edit theetc/hadoop/log4j.propertiesfile to customize the Hadoop daemons’ logging configuration (log-formats and so on).Operating the Hadoop ClusterOnce all the necessary configuration is complete, distribute the files to theHADOOP_CONF_DIRdirectory on all the machines. This should be the same directory on all machines.In general, it is recommended that HDFS and YARN run as separate users. In the majority of installations, HDFS processes execute as ‘hdfs’. YARN is typically using the ‘yarn’ account.Hadoop StartupTo start a Hadoop cluster you will need to start both the HDFS and YARN cluster.The first time you bring up HDFS, it must be formatted. Format a new distributed filesystem ashdfs:[hdfs]$ $HADOOP_HOME/bin/hdfs namenode -formatStart the HDFS NameNode with the following command on the designated node ashdfs:[hdfs]$ $HADOOP_HOME/bin/hdfs --daemon start namenodeStart a HDFS DataNode with the following command on each designated node ashdfs:[hdfs]$ $HADOOP_HOME/bin/hdfs --daemon start datanodeIfetc/hadoop/workersand ssh trusted access is configured (seeSingle Node Setup), all of the HDFS processes can be started with a utility script. Ashdfs:[hdfs]$ $HADOOP_HOME/sbin/start-dfs.shStart the YARN with the following command, run on the designated ResourceManager asyarn:[yarn]$ $HADOOP_HOME/bin/yarn --daemon start resourcemanagerRun a script to start a NodeManager on each designated host asyarn:[yarn]$ $HADOOP_HOME/bin/yarn --daemon start nodemanagerStart a standalone WebAppProxy server. Run on the WebAppProxy server asyarn. If multiple servers are used with load balancing it should be run on each of them:[yarn]$ $HADOOP_HOME/bin/yarn --daemon start proxyserverIfetc/hadoop/workersand ssh trusted access is configured (seeSingle Node Setup), all of the YARN processes can be started with a utility script. Asyarn:[yarn]$ $HADOOP_HOME/sbin/start-yarn.shStart the MapReduce JobHistory Server with the following command, run on the designated server asmapred:[mapred]$ $HADOOP_HOME/bin/mapred --daemon start historyserverHadoop ShutdownStop the NameNode with the following command, run on the designated NameNode ashdfs:[hdfs]$ $HADOOP_HOME/bin/hdfs --daemon stop namenodeRun a script to stop a DataNode ashdfs:[hdfs]$ $HADOOP_HOME/bin/hdfs --daemon stop datanodeIfetc/hadoop/workersand ssh trusted access is configured (seeSingle Node Setup), all of the HDFS processes may be stopped with a utility script. Ashdfs:[hdfs]$ $HADOOP_HOME/sbin/stop-dfs.shStop the ResourceManager with the following command, run on the designated ResourceManager asyarn:[yarn]$ $HADOOP_HOME/bin/yarn --daemon stop resourcemanagerRun a script to stop a NodeManager on a worker asyarn:[yarn]$ $HADOOP_HOME/bin/yarn --daemon stop nodemanagerIfetc/hadoop/workersand ssh trusted access is configured (seeSingle Node Setup), all of the YARN processes can be stopped with a utility script. Asyarn:[yarn]$ $HADOOP_HOME/sbin/stop-yarn.shStop the WebAppProxy server. Run on the WebAppProxy server asyarn. If multiple servers are used with load balancing it should be run on each of them:[yarn]$ $HADOOP_HOME/bin/yarn stop proxyserverStop the MapReduce JobHistory Server with the following command, run on the designated server asmapred:[mapred]$ $HADOOP_HOME/bin/mapred --daemon stop historyserverWeb InterfacesOnce the Hadoop cluster is up and running check the web-ui of the components as described below:DaemonWeb InterfaceNotesNameNodehttp://nn_host:port/Default HTTP port is 9870.ResourceManagerhttp://rm_host:port/Default HTTP port is 8088.MapReduce JobHistory Serverhttp://jhs_host:port/Default HTTP port is 19888.©            2008-2024
              Apache Software Foundation
            
                          -Privacy Policy.
        Apache Maven, Maven, Apache, the Apache feather logo, and the Apache Maven project logos are trademarks of The Apache Software Foundation.