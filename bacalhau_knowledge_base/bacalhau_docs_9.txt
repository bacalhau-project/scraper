URL: https://docs.bacalhau.org/getting-started/create-private-network

Bacalhau Docsv.1.4.0v.1.3.0v.1.3.1v.1.3.2v.1.4.0GitHubSlackContactMoreGitHubSlackContactAsk or SearchCtrl+â€†KWelcomeGetting StartedHow Bacalhau WorksInstallationCreate NetworkHardware SetupContainer OnboardingDocker WorkloadsWebAssembly (Wasm) WorkloadsSetting UpRunning NodesNode OnboardingGPU InstallationJob selection policyAccess ManagementNode persistenceConnect StorageConfiguration ManagementConfiguring Transport Level SecurityLimits and TimeoutsTest Network LocallyBacalhau WebUIWorkload OnboardingContainerDocker Workload OnboardingWebAssembly (Wasm) WorkloadsBacalhau Docker ImageHow To Work With Custom Containers in BacalhauPythonBuilding and Running Custom Python ContainerRunning Pandas on BacalhauRunning a Python ScriptRunning Jupyter Notebooks on BacalhauScripting Bacalhau with PythonR (language)Building and Running your Custom R Containers on BacalhauRunning a Simple R Script on BacalhauRun CUDA programs on BacalhauRunning a Prolog ScriptReading Data from Multiple S3 Buckets using BacalhauRunning Rust programs as WebAssembly (WASM)Generate Synthetic Data using Sparkov Data Generation techniqueData IngestionCopy Data from URL to Public StoragePinning DataRunning a Job over S3 dataNetworking InstructionsAccessing the Internet from JobsUtilizing NATS.io within BacalhauGPU Workloads SetupAutomatic Update CheckingMarketplace DeploymentsGoogle Cloud MarketplaceGuidesWrite a config.yamlWrite a SpecConfigExamplesData EngineeringUsing Bacalhau with DuckDBEthereum Blockchain Analysis with Ethereum-ETL and BacalhauConvert CSV To Parquet Or AvroSimple Image ProcessingOceanography - Data ConversionVideo ProcessingModel InferenceEasyOCR (Optical Character Recognition) on BacalhauRunning Inference on Dolly 2.0 Model with Hugging FaceSpeech Recognition using WhisperStable Diffusion on a GPUStable Diffusion on a CPUObject Detection with YOLOv5 on BacalhauGenerate Realistic Images using StyleGAN3 and BacalhauStable Diffusion Checkpoint InferenceRunning Inference on a Model stored on S3Model TrainingTraining Pytorch Model with BacalhauTraining Tensorflow ModelStable Diffusion Dreambooth (Finetuning)Molecular DynamicsRunning BIDS Apps on BacalhauCoresets On BacalhauGenomics Data GenerationGromacs for AnalysisMolecular Simulation with OpenMM and BacalhauReferencesJobs GuideJob SpecificationJob TypesTask SpecificationEnginesDocker Engine SpecificationWebAssembly (WASM) Engine SpecificationPublishersIPFS Publisher SpecificationLocal Publisher SpecificationS3 Publisher SpecificationSourcesIPFS Source SpecificationLocal Source SpecificationS3 Source SpecificationURL Source SpecificationNetwork SpecificationInput Source SpecificationResources SpecificationResultPath SpecificationConstraint SpecificationLabels SpecificationMeta SpecificationJob TemplatesQueuing & TimeoutsJob QueuingTimeouts SpecificationJob ResultsStateCLI GuideSingle CLI commandsAgentAgent OverviewAgent AliveAgent NodeAgent VersionConfigConfig OverviewConfig Auto-ResourcesConfig DefaultConfig ListConfig SetJobJob OverviewJob DescribeJob ExecJob ExecutionsJob HistoryJob ListJob LogsJob RunJob StopNodeNode OverviewNode ApproveNode DeleteNode ListNode DescribeNode RejectCLI Commands OverviewCommand MigrationAPI GuideBacalhau API overviewBest PracticesAgent EndpointOrchestrator EndpointMigration APINode ManagementAuthentication & AuthorizationDatabase IntegrationDebuggingDebugging Failed JobsDebugging LocallyOpen Telemetry in BacalhauRunning locally in 'devstack'Setting up Dev EnvironmentHelp & FAQBacalhau FAQsRelease NotesGlossaryIntegrationsApache Airflow Provider for BacalhauLilypadBacalhau Python SDKObservability for WebAssembly WorkloadsCommunitySocial MediaStyle GuideWays to ContributePowered by GitBookCreate NetworkIn this tutorial you are setting up your own networkIntroductionBacalhau allows you to create your own private network so you can securely run private workloads without the risks inherent in working on public nodes or inadvertently distributing data outside your organization.This tutorial describes the process of creating your own private network from multiple nodes, configuring the nodes and running demo jobs.â€‹TLDRInstall Bacalhaucurl -sL https://get.bacalhau.org/install.sh | bashon every hostStart theRequester node:bacalhau serve --node-type requesterCopy and paste the command it outputs under the "To connect a compute node to this orchestrator, run the following command in your shell" line toother hostsCopy and paste the environment variables it outputs under the "To connect to this node from the client, run the following commands in your shell" line to aclient machineDone! You can run an example, like:CopybacalhaudockerrunaplineechohelloPrerequisitesPrepare the hosts on which the nodes are going to be set up. They could be:Physical HostsCloud VMs (AWS,GCP,Azureor any other provider)Local Hypervisor VMsDocker ContainersInstall Bacalhauon each hostEnsure that all nodes are connected to the same network and that the necessary ports are open for communication between them.Ensure your nodes have an internet connection in case you have to download or upload any data (docker images, input data, results)Ensure thatDocker Engineis installed in case you are going to run Docker Workloadsâ€‹Bacalhau is designed to be versatile in its deployment, capable of running on various environments: physical hosts, virtual machines or cloud instances. Its resource requirements are modest, ensuring compatibility with a wide range of hardware configurations. However, for certain workloads, such as machine learning, it's advisable to consider hardware configurations optimized for computational tasks, includingGPUs.Start Initial Requestor NodeThe Bacalhau network consists of nodes of two types: compute and requester. Compute Node is responsible for executing jobs and producing results. Requester Node is responsible for handling user requests, forwarding jobs to compute nodes and monitoring the job lifecycle.The first step is to start up the initialRequesternode. This node will connect to nothing but will listen for connections.Start by creating a secure token. This token will be used for authentication between the orchestrator and compute nodes during their communications. Any string can be used as a token, preferably not easy to guess or brute-force. In addition, new authentication methods will be introduced in future releases.â€‹Create and Set Up a TokenLet's use theuuidgentool to create our token, then add it to the Bacalhau configuration and run the requester node:Copy# Create token and write it into the 'my_token' fileuuidgen>my_token#Add token to the Bacalhau configurationbacalhauconfigset"node.network.authsecret"my_tokenCopy#Start the Requester nodebacalhauserve--node-typerequester--peernoneThis will produce output similar to this, indicating that the node is up and running:Copy15:09:58.711|INFpkg/nats/logger.go:47>Startingnats-server[Server:n-1134cdf3-a974-4c0b-b9c9-61858a856bda]...15:09:58.719|INFpkg/nats/logger.go:47>Serverisready[Server:n-1134cdf3-a974-4c0b-b9c9-61858a856bda]15:09:58.739 | INF pkg/nats/server.go:48 > NATS server NAN464RLFLYVA7GYZ6QN3RSH6UAKFJHMQWON4K4VVIRE3O3C7RKU3V7D listening on nats://0.0.0.0:4222 [NodeID:n-1134cdf3]15:10:02.81|INFpkg/config/setters.go:84>Writingtoconfigfile/home/username/.bacalhau/config.yaml:Node.Compute.ExecutionStore:{BoltDB/home/username/.bacalhau/compute_store/executions.db}Node.Requester.JobStore:{BoltDB/home/username/.bacalhau/orchestrator_store/jobs.db}Node.Name:n-1134cdf3-a974-4c0b-b9c9-61858a856bdaToconnectacomputenodetothisorchestrator,runthefollowingcommandinyourshell:bacalhau serve --node-type=compute --network=nats --orchestrators=nats://127.0.0.1:4222 --private-internal-ipfs --ipfs-swarm-addrs=/ip4/127.0.0.1/tcp/39311/p2p/QmdUmWyEUHK3Zfnno4x3Ct89AjtQ75Tr3WGaEsgh1nGGj1Toconnecttothisnodefromtheclient,runthefollowingcommandsinyourshell:exportBACALHAU_NODE_CLIENTAPI_HOST=0.0.0.0exportBACALHAU_NODE_CLIENTAPI_PORT=1234exportBACALHAU_NODE_NETWORK_TYPE=natsexportBACALHAU_NODE_NETWORK_ORCHESTRATORS=nats://127.0.0.1:4222exportBACALHAU_NODE_IPFS_SWARMADDRESSES=/ip4/127.0.0.1/tcp/39311/p2p/QmdUmWyEUHK3Zfnno4x3Ct89AjtQ75Tr3WGaEsgh1nGGj1Note that for security reasons, the output of the command contains the localhost127.0.0.1address instead of your real IP. To connect to this node, you should replace it with your real public IP address yourself. The method for obtaining your public IP address may vary depending on the type of instance you're using. Windows and Linux instances can be queried for their public IP using the following command:Copycurlhttps://api.ipify.orgIf you are using a cloud deployment, you can find your public IP through their console, e.g.AWSandGoogle Cloud.â€‹Create and Connect Compute NodeNow let's move to another host from the preconditions, start a compute node on it and connect to the requester node. Here you will also need to add the same token to the configuration as on the requester.Copy#Add token to the Bacalhau configurationbacalhauconfigset"node.network.authsecret"my_tokenThen execute theservecommand to connect to the requester node:Copybacalhauserve--node-type=compute--orchestrators=<Public-IP-of-Requester-Node>This will produce output similar to this, indicating that the node is up and running:Copy15:51:02.534 | INF pkg/publisher/local/server.go:52 > Running local publishing server on 0.0.0.0:6001 [NodeID:n-ef98aa76]Toconnecttothisnodefromtheclient,runthefollowingcommandsinyourshell:exportBACALHAU_NODE_CLIENTAPI_HOST=0.0.0.0exportBACALHAU_NODE_CLIENTAPI_PORT=1234Acopyofthesevariableshavebeenwrittento:/home/username/.bacalhau/bacalhau.runTo ensure that the nodes are connected to the network, run the following command, specifying the public IP of the requester node:Copybacalhau--api-host<Public-IP-of-Requester-Node>nodelistThis will produce output similar to this, indicating that the nodes belong to the same network:Copybacalhau--api-host10.0.2.15nodelistID          TYPE       STATUS    LABELS                                              CPU     MEMORY      DISK         GPUn-550ee0db  Compute              Architecture=amd64 Operating-System=linux           0.8 /   1.5 GB /    12.3 GB /    0 /git-lfs=true                                        0.8     1.5 GB      12.3 GB      0n-b2ab8483  Requester  APPROVED  Architecture=amd64 Operating-System=linuxSubmitting JobsTo connect to the requester node find the following lines in the requester node logs:CopyToconnecttothisnodefromtheclient,runthefollowingcommandsinyourshell:exportBACALHAU_NODE_CLIENTAPI_HOST=<Public-IP-of-the-Requester-Node>exportBACALHAU_NODE_CLIENTAPI_PORT=1234exportBACALHAU_NODE_NETWORK_TYPE=natsexportBACALHAU_NODE_NETWORK_ORCHESTRATORS=nats://<Public-IP-of-the-Requester-Node>:4222export BACALHAU_NODE_IPFS_SWARMADDRESSES=/ip4/<Public-IP-of-the-Requester-Node>/tcp/43919/p2p/QmehkJQ9BN4QMvv7nFTzsWSBk13coaxEZh4N5YmumtJQDbThe exact commands list will be different for each node and is outputted by thebacalhau servecommand.Note that by default such command contains127.0.0.1or0.0.0.0instead of actual public IP. Make sure to replace it before executing the command.Now you can submit your jobs using thebacalhau docker run,bacalhau wasm runandbacalhau job runcommands. For example submit a hello-world jobbacalhau docker run alpine echo hello:CopybacalhaudockerrunalpineechohelloUsingdefaulttag:latest.Pleasespecifyatag/digestforbetterreproducibility.Jobsuccessfullysubmitted.JobID:ddbfa358-d663-4f54-804e-598c53dbb969Checkingjobstatus...(EnterCtrl+Ctoexitatanytime,yourjobwillcontinuerunning):Communicatingwiththenetwork................doneâœ…0.0sCreatingjobforsubmission................doneâœ…0.5sJobinprogress................doneâœ…0.0sTodownloadtheresults,execute:bacalhaujobgetddbfa358-d663-4f54-804e-598c53dbb969Togetmoredetailsabouttherun,execute:bacalhaujobdescribeddbfa358-d663-4f54-804e-598c53dbb969You will be able to see the job execution logs on the compute node:Copy15:42:06.32 | INF pkg/executor/docker/executor.go:116 > starting execution [NodeID:n-550ee0db] [execution:e-f79b74aa-82c3-4fbe-ac71-476f0d596161] [executionID:e-f79b74aa-82c3-4fbe-ac71-476f0d596161] [job:ddbfa358-d663-4f54-804e-598c53dbb969] [jobID:ddbfa358-d663-4f54-804e-598c53dbb969]...15:42:06.665 | INF pkg/executor/docker/executor.go:217 > received results from execution [executionID:e-f79b74aa-82c3-4fbe-ac71-476f0d596161]15:42:06.676 | INF pkg/compute/executor.go:195 > cleaning up execution [NodeID:n-550ee0db] [execution:e-f79b74aa-82c3-4fbe-ac71-476f0d596161] [job:ddbfa358-d663-4f54-804e-598c53dbb969]Publishers and Sources ConfigurationBy default, IPFS & Local publishers and URL & IPFS sources are available on the compute node. The following describes how to configure the appropriate sources and publishers:S3IPFSLocalTo set upS3 publisheryou need to specify environment variables such asAWS_ACCESS_KEY_IDandAWS_SECRET_ACCESS_KEY, populating a credentials file to be located on your compute node, i.e.~/.aws/credentials, or creating anIAM rolefor your compute nodes if you are utilizing cloud instances.Your chosen publisher can be set for your Bacalhau compute nodes declaratively or imperatively using either configuration yaml file:CopyPublisher:Type:"s3"Params:Bucket:"my-task-results"Key:"task123/result.tar.gz"Endpoint:"https://s3.us-west-2.amazonaws.com"Or within your imperative job execution commands:Copybacalhaudockerrun-ps3://bucket/key,opt=endpoint=http://s3.example.com,opt=region=us-east-1ubuntuâ€¦S3 compatible publishers can also be used asinput sourcesfor your jobs, with a similar configuration.CopyInputSources:-Source:Type:"s3"Params:Bucket:"my-bucket"Key:"data/"Endpoint:"https://storage.googleapis.com"-Target:"/data"By default, bacalhau creates its own in-process IPFS node that will attempt to discover other IPFS nodes, including public nodes, on its own. If you specify the--private-internal-ipfsflag when starting the node, the node will not attempt to discover other nodes. Note, that such an IPFS node exists only with the compute node and will be shut down along with it. Alternatively, you can create your own private IPFS network and connect to it using theappropriate flags.IPFS publishercan be set for your Bacalhau compute nodes declaratively or imperatively using either configuration yaml file:CopyPublisher:Type:ipfsOr within your imperative job execution commands:Copybacalhaudockerrun--publisheripfsubuntu...Data pinned to the IPFS network can be used asinput source. To do this, you will need to specify the CID in declarative:CopyInputSources:-Source:Type:"ipfs"Params:CID:"QmY7Yh4UquoXHLPFo2XbhXkhBvFoPwmQUSa92pxnxjY3fZ"-Target:"/data"Or imperative format:Copybacalhaudockerrun--inputQmY7Yh4UquoXHLPFo2XbhXkhBvFoPwmQUSa92pxnxjY3fZ:/data...Bacalhau allows to publish job results directly to the compute node. Please note that this method is not a reliable storage option and is recommended to be used mainly for introductory purposes.Local publishercan be set for your Bacalhau compute nodes declaratively or imperatively using configuration yaml file:CopyPublisher:Type:localOr within your imperative job execution commands:Copybacalhaudockerrun--publisherlocalubuntu...TheLocal input sourceallows Bacalhau jobs to access files and directories that are already present on the compute node. To allow jobs to access local files when starting a node, the--allow-listed-local-pathsflag should be used, specifying the path to the data and access mode:rwfor Read-Write access or:rofor Read-Only (used by default). For example:Copybacalhauserve--allow-listed-local-paths"/etc/config:rw,/etc/*.conf:ro"Further, the path to local data in declarative or imperative form must be specified in the job. Declarative example of the local input source:CopyInputSources:-Source:Type:"localDirectory"Params:SourcePath:"/etc/config"ReadWrite:trueTarget:"/config"Imperative example of the local input source:Copybacalhaudockerrun-inputfile:///etc/config:/configubuntu...Best Practices for Production Use CasesYour private cluster can be quickly set up for testing packaged jobs and tweaking data processing pipelines. However, when using a private cluster in production, here are a few considerations to note.Ensure you are running the Bacalhau process from a dedicated system user with limited permissions. This enhances security and reduces the risk of unauthorized access to critical system resources. If you are using an orchestrator such asTerraform, utilize a service file to manage the Bacalhau process, ensuring the correct user is specified and consistently used. Hereâ€™s asample service fileCreate an authentication file for your clients. Adedicated authentication file or policycan ease the process of maintaining secure data transmission within your network. With this, clients can authenticate themselves, and you can limit the Bacalhau API endpoints unauthorized users have access to.Consistency is a key consideration when deploying decentralized tools such as Bacalhau. You can use aninstallation scriptto affix a specific version of Bacalhau or specify deployment actions, ensuring that each host instance has all the necessary resources for efficient operations.Ensure separation of concerns in your cloud deployments by mounting the Bacalhau repository on a separate non-boot disk. This prevents instability on shutdown or restarts and improves performance within your host instances.That's all folks! ðŸŽ‰ Please contact us onSlack#bacalhauchannel for questions and feedback!PreviousInstallationNextHardware SetupLast updated1 month agoOn this pageIntroductionTLDRPrerequisitesStart Initial Requestor NodeCreate and Set Up a TokenCreate and Connect Compute NodeSubmitting JobsPublishers and Sources ConfigurationBest Practices for Production Use CasesWas this helpful?Edit on GitHubExport as PDFGet SupportExpansoSupportUse CasesDistributed ETLEdge MLDistributed Data WarehousingFlett ManagementAbout UsWho we areWhat we valueNews & BlogBlogNewsExpanso (2024). All Rights Reserved.