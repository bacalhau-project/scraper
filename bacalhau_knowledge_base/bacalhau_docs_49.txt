URL: https://docs.bacalhau.org/setting-up/data-ingestion/s3

Bacalhau Docsv.1.4.0v.1.3.0v.1.3.1v.1.3.2v.1.4.0GitHubSlackContactMoreGitHubSlackContactAsk or SearchCtrl+ KWelcomeGetting StartedHow Bacalhau WorksInstallationCreate NetworkHardware SetupContainer OnboardingDocker WorkloadsWebAssembly (Wasm) WorkloadsSetting UpRunning NodesNode OnboardingGPU InstallationJob selection policyAccess ManagementNode persistenceConnect StorageConfiguration ManagementConfiguring Transport Level SecurityLimits and TimeoutsTest Network LocallyBacalhau WebUIWorkload OnboardingContainerDocker Workload OnboardingWebAssembly (Wasm) WorkloadsBacalhau Docker ImageHow To Work With Custom Containers in BacalhauPythonBuilding and Running Custom Python ContainerRunning Pandas on BacalhauRunning a Python ScriptRunning Jupyter Notebooks on BacalhauScripting Bacalhau with PythonR (language)Building and Running your Custom R Containers on BacalhauRunning a Simple R Script on BacalhauRun CUDA programs on BacalhauRunning a Prolog ScriptReading Data from Multiple S3 Buckets using BacalhauRunning Rust programs as WebAssembly (WASM)Generate Synthetic Data using Sparkov Data Generation techniqueData IngestionCopy Data from URL to Public StoragePinning DataRunning a Job over S3 dataNetworking InstructionsAccessing the Internet from JobsUtilizing NATS.io within BacalhauGPU Workloads SetupAutomatic Update CheckingMarketplace DeploymentsGoogle Cloud MarketplaceGuidesWrite a config.yamlWrite a SpecConfigExamplesData EngineeringUsing Bacalhau with DuckDBEthereum Blockchain Analysis with Ethereum-ETL and BacalhauConvert CSV To Parquet Or AvroSimple Image ProcessingOceanography - Data ConversionVideo ProcessingModel InferenceEasyOCR (Optical Character Recognition) on BacalhauRunning Inference on Dolly 2.0 Model with Hugging FaceSpeech Recognition using WhisperStable Diffusion on a GPUStable Diffusion on a CPUObject Detection with YOLOv5 on BacalhauGenerate Realistic Images using StyleGAN3 and BacalhauStable Diffusion Checkpoint InferenceRunning Inference on a Model stored on S3Model TrainingTraining Pytorch Model with BacalhauTraining Tensorflow ModelStable Diffusion Dreambooth (Finetuning)Molecular DynamicsRunning BIDS Apps on BacalhauCoresets On BacalhauGenomics Data GenerationGromacs for AnalysisMolecular Simulation with OpenMM and BacalhauReferencesJobs GuideJob SpecificationJob TypesTask SpecificationEnginesDocker Engine SpecificationWebAssembly (WASM) Engine SpecificationPublishersIPFS Publisher SpecificationLocal Publisher SpecificationS3 Publisher SpecificationSourcesIPFS Source SpecificationLocal Source SpecificationS3 Source SpecificationURL Source SpecificationNetwork SpecificationInput Source SpecificationResources SpecificationResultPath SpecificationConstraint SpecificationLabels SpecificationMeta SpecificationJob TemplatesQueuing & TimeoutsJob QueuingTimeouts SpecificationJob ResultsStateCLI GuideSingle CLI commandsAgentAgent OverviewAgent AliveAgent NodeAgent VersionConfigConfig OverviewConfig Auto-ResourcesConfig DefaultConfig ListConfig SetJobJob OverviewJob DescribeJob ExecJob ExecutionsJob HistoryJob ListJob LogsJob RunJob StopNodeNode OverviewNode ApproveNode DeleteNode ListNode DescribeNode RejectCLI Commands OverviewCommand MigrationAPI GuideBacalhau API overviewBest PracticesAgent EndpointOrchestrator EndpointMigration APINode ManagementAuthentication & AuthorizationDatabase IntegrationDebuggingDebugging Failed JobsDebugging LocallyOpen Telemetry in BacalhauRunning locally in 'devstack'Setting up Dev EnvironmentHelp & FAQBacalhau FAQsRelease NotesGlossaryIntegrationsApache Airflow Provider for BacalhauLilypadBacalhau Python SDKObservability for WebAssembly WorkloadsCommunitySocial MediaStyle GuideWays to ContributePowered by GitBookRunning a Job over S3 dataHere is a quick tutorial on how to copy Data from S3 to a public storage. In this tutorial, we will scrape all the links from a public AWS S3 buckets and then copy the data to IPFS using Bacalhau.Prerequisite​To get started, you need to install the Bacalhau client, see more informationhereRunning a Bacalhau Job​Copybacalhaudockerrun\-i"s3://noaa-goes16/ABI-L1b-RadC/2000/001/12/OR_ABI-L1b-RadC-M3C01*:/inputs,opt=region=us-east-1"\--id-only\--wait\alpine\--sh-c"cp -r /inputs/* /outputs/"Structure of the Command​Let's look closely at the command above:bacalhau docker run: call to bacalhau-i "s3://noaa-goes16/ABI-L1b-RadC/2000/001/12/OR_ABI-L1b-RadC-M3C01*:/inputs,opt=region=us-east-1: defines S3 objects as inputs to the job. In this case, it will download all objects that match the prefixABI-L1b-RadC/2000/001/12/OR_ABI-L1b-RadC-M3C01from the bucketnoaa-goes16inus-east-1region, and mount the objects under/inputspath inside the docker job.-- sh -c "cp -r /inputs/* /outputs/": copies all files under/inputsto/outputs, which is by default the result output directory which all of its content will be published to the specified destination, which is IPFS by defaultWhen a job is submitted, Bacalhau prints out the relatedjob_id. We store that in an environment variable so that we can reuse it later on.This works either with datasets that are publicly available or with private datasets, provided that the nodes have the necessary credentials to access. See theS3 Source Specificationfor more details.Checking the State of your Jobs​Job status: You can check the status of the job usingbacalhau job list.Copybacalhaujoblist--id-filter${JOB_ID}--wideWhen it saysPublishedorCompleted, that means the job is done, and we can get the results.Job information: You can find out more information about your job by usingbacalhau job describe.Copybacalhaujobdescribe${JOB_ID}Job download: You can download your job results directly by usingbacalhau job get. Alternatively, you can choose to create a directory to store your results. In the command below, we remove the results directory if it exists, create it again and download our job output to be stored in that directory.Copyrm-rfresults&&mkdir-presults# Temporary directory to store the resultsbacalhaujobget$JOB_ID--output-dirresults# Download the resultsViewing your Job Output​When the download is completed, the results of the job will be present in the directory. To view them, run the following command:Copyls-1results/outputs{"NextToken":"","Results":[{"Type":"s3PreSigned","Params":{"PreSignedURL": "https://bacalhau-test-datasets.s3.eu-west-1.amazonaws.com/integration-tests-publisher/walid-manual-test-j-46a23fe7-e063-4ba6-8879-aac62af732b0.tar.gz?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAUEMPQ7JFSLGEPHJG%2F20240129%2Feu-west-1%2Fs3%2Faws4_request&X-Amz-Date=20240129T060142Z&X-Amz-Expires=1800&X-Amz-SignedHeaders=host&x-id=GetObject&X-Amz-Signature=cea00578ae3b03a1b52dba2d65a1bab40f1901fb7cd4ee1a0a974dc05b595f2e","SourceSpec":{"Bucket":"bacalhau-test-datasets","ChecksumSHA256":"1tlbgo+q0TlQhJi8vkiWnwTwPu1zenfvTO4qW1D5yvI=","Endpoint":"","Filter":"","Key":"integration-tests-publisher/walid-manual-test-j-46a23fe7-e063-4ba6-8879-aac62af732b0.tar.gz","Region":"eu-west-1","VersionID":"oS7n.lY5BYHPMNOfbBS1n5VLl4ppVS4h"}}}]}Extract Result CID​First you need to installjq(if it is not already installed) to process JSON:CopysudoaptupdatesudoaptinstalljqTo extract the CIDs from output JSON, execute following:Copybacalhaujobdescribe${JOB_ID}--json\|jq-r'.State.Executions[].PublishedResults.CID | select (. != null)'The extracted CID will look like this:CopyQmYFhG668yJZmtk84SMMdbrz5Uvuh78Q8nLxTgLDWShkhRPublishing Results to S3-Compatible Destinations​You can publish your results to Amazon s3 or other S3-compatible destinations like MinIO, Ceph, or SeaweedFS to conveniently store and share your outputs.Publisher Spec​To facilitate publishing results, define publishers and their configurations using the PublisherSpec structure.For S3-compatible destinations, the configuration is as follows:CopytypePublisherSpecstruct{TypePublisher`json:"Type,omitempty"`Paramsmap[string]interface{}`json:"Params,omitempty"`}For Amazon S3, you can specify thePublisherSpecconfiguration as shown below:CopyPublisherSpec:Type:S3Params:Bucket:<bucket># Specify the bucket where results will be storedKey:<object-key># Define the object key (supports dynamic naming using placeholders)Compress:<true/false># Specify whether to publish results as a single gzip file (default: false)Endpoint:<optional># Optionally specify the S3 endpointRegion:<optional># Optionally specify the S3 regionExample Usage​Let's explore some examples to illustrate how you can use this:Publishing results to S3 using default settingsCopybacalhaudockerrun-ps3://<bucket>/<object-key>ubuntu...Publishing results to S3 with a custom endpoint and region:Copybacalhaudockerrun\-ps3://<bucket>/<object-key>,opt=endpoint=http://s3.example.com,opt=region=us-east-1\ubuntu...Publishing results to S3 as a single compressed fileCopybacalhaudockerrun-ps3://<bucket>/<object-key>,opt=compress=trueubuntu...Utilizing naming placeholders in the object keyCopybacalhaudockerrun-ps3://<bucket>/result-{date}-{jobID}ubuntu...Content Identification​Tracking content identification and maintaining lineage across different jobs' inputs and outputs can be challenging. To address this, the publisher encodes the SHA-256 checksum of the published results, specifically when publishing a single compressed file.Here's an example of a sample result:Copy{"NodeID":"QmYJ9QN9Pbi6gBKNrXVk5J36KSDGL5eUT6LMLF5t7zyaA7","Data":{"StorageSource":"S3","Name":"s3://<bucket>/run3.tar.gz","S3":{"Bucket":"<bucket>","Key":"run3.tar.gz","Checksum":"e0uDqmflfT9b+rMfoCnO5G+cy+8WVTOPUtAqDMnXWbw=","VersionID":"hZoNdqJsZxE_bFm3UGJuJ0RqkITe9dQ1"}}}Support for the S3-compatible storage provider​To enable support for the S3-compatible storage provider, no additional dependencies are required. However, valid AWS credentials are necessary to sign the requests. The storage provider uses the default credentials chain, which checks the following sources for credentials:Environment variables, such asAWS_ACCESS_KEY_IDandAWS_SECRET_ACCESS_KEYCredentials file~/.aws/credentialsIAM Roles for Amazon EC2 InstancesNeed Support?​For questions, feedback, please reach out in ourSlackPreviousPinning DataNextNetworking InstructionsLast updated2 months agoOn this pagePrerequisite​Running a Bacalhau Job​Checking the State of your Jobs​Viewing your Job Output​Extract Result CID​Publishing Results to S3-Compatible Destinations​Content Identification​Support for the S3-compatible storage provider​Need Support?​Was this helpful?Edit on GitHubExport as PDFGet SupportExpansoSupportUse CasesDistributed ETLEdge MLDistributed Data WarehousingFlett ManagementAbout UsWho we areWhat we valueNews & BlogBlogNewsExpanso (2024). All Rights Reserved.