URL: https://en.wikipedia.org/wiki/Concurrent_computing

Jump to contentMain menuMain menumove to sidebarhideNavigationMain pageContentsCurrent eventsRandom articleAbout WikipediaContact usDonateContributeHelpLearn to editCommunity portalRecent changesUpload fileSearchSearchAppearanceCreate accountLog inPersonal toolsCreate accountLog inPages for logged out editorslearn moreContributionsTalkContentsmove to sidebarhide(Top)1IntroductionToggle Introduction subsection1.1Coordinating access to shared resources1.2Advantages2ModelsToggle Models subsection2.1Consistency models3ImplementationToggle Implementation subsection3.1Interaction and communication4History5Prevalence6Languages supporting concurrent programming7See also8Notes9References10Sources11Further reading12External linksToggle the table of contentsConcurrent computing23 languagesالعربيةتۆرکجهБългарскиCatalàČeštinaEspañolEuskaraفارسیFrançais한국어हिन्दीNederlands日本語Norsk bokmålPolskiPortuguêsRomânăРусскийСрпски / srpskiSuomiУкраїнська粵語中文Edit linksArticleTalkEnglishReadEditView historyToolsToolsmove to sidebarhideActionsReadEditView historyGeneralWhat links hereRelated changesUpload fileSpecial pagesPermanent linkPage informationCite this pageGet shortened URLDownload QR codeWikidata itemPrint/exportDownload as PDFPrintable versionIn other projectsWikimedia CommonsAppearancemove to sidebarhideFrom Wikipedia, the free encyclopediaExecuting several computations during overlapping time periodsFor the American computer company, seeConcurrent Computer Corporation. For a more theoretical discussion, seeConcurrency (computer science).This articleneeds additional citations forverification.Please helpimprove this articlebyadding citations to reliable sources. Unsourced material may be challenged and removed.Find sources:"Concurrent computing"–news·newspapers·books·scholar·JSTOR(February 2014)(Learn how and when to remove this message)Concurrent computingis a form ofcomputingin which severalcomputationsare executedconcurrently—during overlapping time periods—instead ofsequentially—with one completing before the next starts.This is a property of a system—whether aprogram,computer, or anetwork—where there is a separate execution point or "thread of control" for each process. Aconcurrent systemis one where a computation can advance without waiting for all other computations to complete.[1]Concurrent computing is a form ofmodular programming. In itsparadigman overall computation isfactoredinto subcomputations that may be executed concurrently. Pioneers in the field of concurrent computing includeEdsger Dijkstra,Per Brinch Hansen, andC.A.R. Hoare.[2]Introduction[edit]See also:Parallel computingThis section has multiple issues.Please helpimprove itor discuss these issues on thetalk page.(Learn how and when to remove these template messages)This sectionneeds additional citations forverification.Please helpimprove this articlebyadding citations to reliable sourcesin this section. Unsourced material may be challenged and removed.(December 2016)(Learn how and when to remove this message)This sectionpossibly containsoriginal research.Pleaseimprove itbyverifyingthe claims made and addinginline citations. Statements consisting only of original research should be removed.(December 2016)(Learn how and when to remove this message)(Learn how and when to remove this message)The concept of concurrent computing is frequently confused with the related but distinct concept ofparallel computing,[3][4]although both can be described as "multiple processes executingduring the same period of time". In parallel computing, execution occurs at the same physical instant: for example, on separateprocessorsof amulti-processormachine, with the goal of speeding up computations—parallel computing is impossible on a (one-core) single processor, as only one computation can occur at any instant (during any single clock cycle).[a]By contrast, concurrent computing consists of processlifetimesoverlapping, but execution need not happen at the same instant. The goal here is to model processes in the outside world that happen concurrently, such as multiple clients accessing a server at the same time. Structuring software systems as composed of multiple concurrent, communicating parts can be useful for tackling complexity, regardless of whether the parts can be executed in parallel.[5]: 1For example, concurrent processes can be executed on one core by interleaving the execution steps of each process viatime-sharingslices: only one process runs at a time, and if it does not complete during its time slice, it ispaused, another process begins or resumes, and then later the original process is resumed. In this way, multiple processes are part-way through execution at a single instant, but only one process is being executed at that instant.[citation needed]Concurrent computationsmaybe executed in parallel,[3][6]for example, by assigning each process to a separate processor or processor core, ordistributinga computation across a network.The exact timing of when tasks in a concurrent system are executed depends on thescheduling, and tasks need not always be executed concurrently. For example, given two tasks, T1 and T2:[citation needed]T1 may be executed and finished before T2 orvice versa(serialandsequential)T1 and T2 may be executed alternately (serialandconcurrent)T1 and T2 may be executed simultaneously at the same instant of time (parallelandconcurrent)The word "sequential" is used as an antonym for both "concurrent" and "parallel"; when these are explicitly distinguished,concurrent/sequentialandparallel/serialare used as opposing pairs.[7]A schedule in which tasks execute one at a time (serially, no parallelism), without interleaving (sequentially, no concurrency: no task begins until the prior task ends) is called aserial schedule. A set of tasks that can be scheduled serially isserializable, which simplifiesconcurrency control.[citation needed]Coordinating access to shared resources[edit]The main challenge in designing concurrent programs isconcurrency control: ensuring the correct sequencing of the interactions or communications between different computational executions, and coordinating access to resources that are shared among executions.[6]Potential problems includerace conditions,deadlocks, andresource starvation. For example, consider the following algorithm to make withdrawals from a checking account represented by the shared resourcebalance:boolwithdraw(intwithdrawal){if(balance>=withdrawal){balance-=withdrawal;returntrue;}returnfalse;}Supposebalance = 500, and two concurrentthreadsmake the callswithdraw(300)andwithdraw(350). If line 3 in both operations executes before line 5 both operations will find thatbalance >= withdrawalevaluates totrue, and execution will proceed to subtracting the withdrawal amount. However, since both processes perform their withdrawals, the total amount withdrawn will end up being more than the original balance. These sorts of problems with shared resources benefit from the use of concurrency control, ornon-blocking algorithms.Advantages[edit]This sectiondoes notciteanysources.Please helpimprove this sectionbyadding citations to reliable sources. Unsourced material may be challenged andremoved.(December 2006)(Learn how and when to remove this message)The advantages of concurrent computing include:Increased program throughput—parallel execution of a concurrent program allows the number of tasks completed in a given time to increase proportionally to the number of processors according toGustafson's lawHigh responsiveness for input/output—input/output-intensive programs mostly wait for input or output operations to complete. Concurrent programming allows the time that would be spent waiting to be used for another task.[citation needed]More appropriate program structure—some problems and problem domains are well-suited to representation as concurrent tasks or processes.[citation needed]Models[edit]Introduced in 1962,Petri netswere an early attempt to codify the rules of concurrent execution. Dataflow theory later built upon these, andDataflow architectureswere created to physically implement the ideas of dataflow theory. Beginning in the late 1970s,process calculisuch asCalculus of Communicating Systems(CCS) andCommunicating Sequential Processes(CSP) were developed to permit algebraic reasoning about systems composed of interacting components. Theπ-calculusadded the capability for reasoning about dynamic topologies.Input/output automatawere introduced in 1987.Logics such as Lamport'sTLA+, and mathematical models such astracesandActor event diagrams, have also been developed to describe the behavior of concurrent systems.Software transactional memoryborrows fromdatabase theorythe concept ofatomic transactionsand applies them to memory accesses.Consistency models[edit]Main article:Consistency modelConcurrent programming languages and multiprocessor programs must have aconsistency model(also known as a memory model). The consistency model defines rules for how operations oncomputer memoryoccur and how results are produced.One of the first consistency models wasLeslie Lamport'ssequential consistencymodel. Sequential consistency is the property of a program that its execution produces the same results as a sequential program. Specifically, a program is sequentially consistent if "the results of any execution is the same as if the operations of all the processors were executed in some sequential order, and the operations of each individual processor appear in this sequence in the order specified by its program".[8]See also:Relaxed sequentialImplementation[edit]This sectionneeds expansion. You can help byadding to it.(February 2014)A number of different methods can be used to implement concurrent programs, such as implementing each computational execution as anoperating system process, or implementing the computational processes as a set ofthreadswithin a single operating system process.Interaction and communication[edit]In some concurrent computing systems, communication between the concurrent components is hidden from the programmer (e.g., by usingfutures), while in others it must be handled explicitly. Explicit communication can be divided into two classes:Shared memory communicationConcurrent components communicate by altering the contents ofshared memorylocations (exemplified byJavaandC#). This style of concurrent programming usually needs the use of some form of locking (e.g.,mutexes,semaphores, ormonitors) to coordinate between threads. A program that properly implements any of these is said to bethread-safe.Message passing communicationConcurrent components communicate byexchanging messages(exemplified byMPI,Go,Scala,Erlangandoccam). The exchange of messages may be carried out asynchronously, or may use a synchronous "rendezvous" style in which the sender blocks until the message is received. Asynchronous message passing may be reliable or unreliable (sometimes referred to as "send and pray"). Message-passing concurrency tends to be far easier to reason about than shared-memory concurrency, and is typically considered a more robust form of concurrent programming.[citation needed]A wide variety of mathematical theories to understand and analyze message-passing systems are available, including theactor model, and variousprocess calculi. Message passing can be efficiently implemented viasymmetric multiprocessing, with or without shared memorycache coherence.Shared memory and message passing concurrency have different performance characteristics. Typically (although not always), the per-process memory overhead and task switching overhead is lower in a message passing system, but the overhead of message passing is greater than for a procedure call. These differences are often overwhelmed by other performance factors.History[edit]Concurrent computing developed out of earlier work on railroads andtelegraphy, from the 19th and early 20th century, and some terms date to this period, such as semaphores. These arose to address the question of how to handle multiple trains on the same railroad system (avoiding collisions and maximizing efficiency) and how to handle multiple transmissions over a given set of wires (improving efficiency), such as viatime-division multiplexing(1870s).The academic study of concurrent algorithms started in the 1960s, withDijkstra (1965)credited with being the first paper in this field, identifying and solvingmutual exclusion.[9]Prevalence[edit]Concurrency is pervasive in computing, occurring from low-level hardware on a single chip to worldwide networks. Examples follow.At the programming language level:ChannelCoroutineFutures and promisesAt the operating system level:Computer multitasking, including bothcooperative multitaskingandpreemptive multitaskingTime-sharing, which replaced sequentialbatch processingof jobs with concurrent use of a systemProcessThreadAt the network level, networked systems are generally concurrent by their nature, as they consist of separate devices.Languages supporting concurrent programming[edit]Concurrent programming languagesare programming languages that use language constructs forconcurrency. These constructs may involvemulti-threading, support fordistributed computing,message passing,shared resources(includingshared memory) orfutures and promises. Such languages are sometimes described asconcurrency-oriented languagesorconcurrency-oriented programming languages(COPL).[10]Today, the most commonly used programming languages that have specific constructs for concurrency areJavaandC#. Both of these languages fundamentally use a shared-memory concurrency model, with locking provided bymonitors(although message-passing models can and have been implemented on top of the underlying shared-memory model). Of the languages that use a message-passing concurrency model,Erlangis probably the most widely used in industry at present.[citation needed]Many concurrent programming languages have been developed more as research languages (e.g.Pict) rather than as languages for production use. However, languages such asErlang,Limbo, andoccamhave seen industrial use at various times in the last 20 years. A non-exhaustive list of languages which use or provide concurrent programming facilities:Ada—general purpose, with native support for message passing and monitor based concurrencyAlef—concurrent, with threads and message passing, for system programming in early versions ofPlan 9 from Bell LabsAlice—extension toStandard ML, adds support for concurrency via futuresAteji PX—extension toJavawith parallel primitives inspired fromπ-calculusAxum—domain specific, concurrent, based on actor model and .NET Common Language Runtime using a C-like syntaxBMDFM—Binary Modular DataFlow MachineC++—thread and coroutine support libraries[11][12]Cω(C omega)—for research, extends C#, uses asynchronous communicationC#—supports concurrent computing using lock, yield, also since version 5.0 async and await keywords introducedClojure—modern,functionaldialect ofLispon theJavaplatformConcurrent Clean—functional programming, similar toHaskellConcurrent Collections(CnC)—Achieves implicit parallelism independent of memory model by explicitly defining flow of data and controlConcurrent Haskell—lazy, pure functional language operating concurrent processes on shared memoryConcurrent ML—concurrent extension ofStandard MLConcurrent Pascal—byPer Brinch HansenCurryD—multi-paradigmsystem programming languagewith explicit support for concurrent programming (actor model)E—uses promises to preclude deadlocksECMAScript—uses promises for asynchronous operationsEiffel—through itsSCOOPmechanism based on the concepts of Design by ContractElixir—dynamic and functional meta-programming aware language running on the Erlang VM.Erlang—uses synchronous or asynchronous message passing with no shared memoryFAUST—real-time functional, for signal processing, compiler provides automatic parallelization viaOpenMPor a specificwork-stealingschedulerFortran—coarraysanddo concurrentare part of Fortran 2008 standardGo—for system programming, with a concurrent programming model based onCSPHaskell—concurrent, and parallel functional programming language[13]Hume—functional, concurrent, for bounded space and time environments where automata processes are described by synchronous channels patterns and message passingIo—actor-based concurrencyJanus—features distinctaskersandtellersto logical variables, bag channels; is purely declarativeJava—thread class or Runnable interfaceJulia—"concurrent programming primitives: Tasks, async-wait, Channels."[14]JavaScript—viaweb workers, in a browser environment,promises, andcallbacks.JoCaml—concurrent and distributed channel based, extension ofOCaml, implements thejoin-calculusof processesJoin Java—concurrent, based onJavalanguageJoule—dataflow-based, communicates by message passingJoyce—concurrent, teaching, built onConcurrent Pascalwith features fromCSPbyPer Brinch HansenLabVIEW—graphical, dataflow, functions are nodes in a graph, data is wires between the nodes; includes object-oriented languageLimbo—relative ofAlef, for system programming inInferno (operating system)Locomotive BASIC—Amstrad variant of BASIC contains EVERY and AFTER commands for concurrent subroutinesMultiLisp—Schemevariant extended to support parallelismModula-2—for system programming, by N. Wirth as a successor to Pascal with native support for coroutinesModula-3—modern member of Algol family with extensive support for threads, mutexes, condition variablesNewsqueak—for research, with channels as first-class values; predecessor ofAlefoccam—influenced heavily bycommunicating sequential processes(CSP)occam-π—a modern variant ofoccam, which incorporates ideas from Milner'sπ-calculusOrc—heavily concurrent, nondeterministic, based onKleene algebraOz-Mozart—multiparadigm, supports shared-state and message-passing concurrency, and futuresParaSail—object-oriented, parallel, free of pointers, race conditionsPict—essentially an executable implementation of Milner'sπ-calculusRakuincludes classes for threads, promises and channels by default[15]Python— uses thread-based parallelism and process-based parallelism[16]Reia—uses asynchronous message passing between shared-nothing objectsRed/System—for system programming, based onRebolRust—for system programming, using message-passing with move semantics, shared immutable memory, and shared mutable memory.[17]Scala—general purpose, designed to express common programming patterns in a concise, elegant, and type-safe waySequenceL—general purpose functional, main design objectives are ease of programming, code clarity-readability, and automatic parallelization for performance on multicore hardware, and provably free ofrace conditionsSR—for researchSuperPascal—concurrent, for teaching, built onConcurrent PascalandJoycebyPer Brinch HansenSwift—built-in support for writing asynchronous and parallel code in a structured way[18]Unicon—for researchTNSDL—for developing telecommunication exchanges, uses asynchronous message passingVHSIC Hardware Description Language (VHDL)—IEEE STD-1076XC—concurrency-extended subset of C language developed byXMOS, based oncommunicating sequential processes, built-in constructs for programmable I/OMany other languages provide support for concurrency in the form of libraries, at levels roughly comparable with the above list.See also[edit]Asynchronous I/OChu spaceFlow-based programmingJava ConcurrentMapPtolemy ProjectRace condition § ComputingStructured concurrencyTransaction processingNotes[edit]^This is discounting parallelism internal to a processor core, such as pipelining or vectorized instructions. A one-core, one-processormachinemay be capable of some parallelism, such as with acoprocessor, but the processor alone is not.References[edit]^Operating System Concepts9th edition, Abraham Silberschatz. "Chapter 4: Threads"^Hansen, Per Brinch, ed. (2002).The Origin of Concurrent Programming.doi:10.1007/978-1-4757-3472-0.ISBN978-1-4419-2986-0.S2CID44909506.^abPike, Rob(2012-01-11). "Concurrency is not Parallelism".Waza conference, 11 January 2012. Retrieved fromhttp://talks.golang.org/2012/waza.slide(slides) andhttp://vimeo.com/49718712(video).^"Parallelism vs. Concurrency".Haskell Wiki.^Schneider, Fred B. (1997-05-06).On Concurrent Programming. Springer.ISBN9780387949420.^abBen-Ari, Mordechai (2006).Principles of Concurrent and Distributed Programming(2nd ed.). Addison-Wesley.ISBN978-0-321-31283-9.^Patterson & Hennessy 2013, p. 503.^Lamport, Leslie (1 September 1979). "How to Make a Multiprocessor Computer That Correctly Executes Multiprocess Programs".IEEE Transactions on Computers.C-28(9): 690–691.doi:10.1109/TC.1979.1675439.S2CID5679366.^"PODC Influential Paper Award: 2002",ACM Symposium on Principles of Distributed Computing, retrieved2009-08-24^Armstrong, Joe (2003)."Making reliable distributed systems in the presence of software errors"(PDF).^https://en.cppreference.com/w/cpp/header/thread[bare URL]^https://en.cppreference.com/w/cpp/header/coroutine[bare URL]^Marlow, Simon (2013) Parallel and Concurrent Programming in Haskell : Techniques for Multicore and Multithreaded ProgrammingISBN9781449335946^https://juliacon.talkfunnel.com/2015/21-concurrent-and-parallel-programming-in-juliaConcurrent and Parallel programming in Julia^"Concurrency".docs.perl6.org. Retrieved2017-12-24.^Documentation » The Python Standard Library » Concurrent Execution^Blum, Ben (2012)."Typesafe Shared Mutable State". Retrieved2012-11-14.^"Concurrency". 2022. Retrieved2022-12-15.Sources[edit]Patterson, David A.; Hennessy, John L. (2013).Computer Organization and Design: The Hardware/Software Interface. The Morgan Kaufmann Series in Computer Architecture and Design (5 ed.). Morgan Kaufmann.ISBN978-0-12407886-4.Further reading[edit]Dijkstra, E. W.(1965)."Solution of a problem in concurrent programming control".Communications of the ACM.8(9): 569.doi:10.1145/365559.365617.S2CID19357737.Herlihy, Maurice (2008) [2008].The Art of Multiprocessor Programming. Morgan Kaufmann.ISBN978-0123705914.Downey, Allen B. (2005) [2005].The Little Book of Semaphores(PDF). Green Tea Press.ISBN978-1-4414-1868-5. Archived fromthe original(PDF)on 2016-03-04. Retrieved2009-11-21.Filman, Robert E.; Daniel P. Friedman (1984).Coordinated Computing: Tools and Techniques for Distributed Software. New York: McGraw-Hill. p.370.ISBN978-0-07-022439-1.Leppäjärvi, Jouni (2008).A pragmatic, historically oriented survey on the universality of synchronization primitives(PDF). University of Oulu.Taubenfeld, Gadi (2006).Synchronization Algorithms and Concurrent Programming. Pearson / Prentice Hall. p. 433.ISBN978-0-13-197259-9.External links[edit]Media related toConcurrent programmingat Wikimedia CommonsConcurrent Systems Virtual LibraryvteConcurrent computingGeneralConcurrencyConcurrency controlConcurrent data structuresConcurrent hash tablesConcurrent usersIndeterminacyLinearizabilityProcess calculiCSPCCSACPLOTOSπ-calculusAmbient calculusAPI-CalculusPEPAJoin-calculusClassic problemsABA problemCigarette smokers problemDeadlockDining philosophers problemProducer–consumer problemRace conditionReaders–writers problemSleeping barber problemCategory: Concurrent computingvteProgramming paradigms(Comparison by language)ImperativeStructuredJackson structuresBlock-structuredModularNon-structuredProceduralProgramming in the large and in the smallDesign by contractInvariant-basedNested functionObject-oriented(comparison,list)Class-based,Prototype-based,Object-basedAgentImmutable objectPersistentUniform Function Call SyntaxDeclarativeFunctional(comparison)RecursiveAnonymous function(Partial application)Higher-orderPurely functionalTotalStrictGADTsDependent typesFunctional logicPoint-free styleExpression-orientedApplicative,ConcatenativeFunction-level,Value-levelDataflowFlow-basedReactive(Functional reactive)SignalsStreamsSynchronousLogicAbductive logicAnswer setConstraint(Constraint logic)Inductive logicNondeterministicOntologyProbabilistic logicQueryDSLAlgebraic modelingArrayAutomata-based(Action)Command(Spacecraft)DifferentiableEnd-userGrammar-orientedInterface descriptionLanguage-orientedList comprehensionLow-codeModelingNatural languageNon-English-basedPage descriptionPipesandfiltersProbabilisticQuantumScientificScriptingSet-theoreticSimulationStack-basedSystemTactileTemplatingTransformation(Graph rewriting,Production,Pattern)VisualConcurrent,distributed,parallelActor-basedAutomatic mutual exclusionChoreographic programmingConcurrent logic(Concurrent constraint logic)Concurrent OOMacroprogrammingMultitier programmingOrganic computingParallel programming modelsPartitioned global address spaceProcess-orientedRelativistic programmingService-orientedStructured concurrencyMetaprogrammingAttribute-orientedAutomatic(Inductive)DynamicExtensibleGenericHomoiconicityInteractiveMacro(Hygienic)Metalinguistic abstractionMulti-stageProgram synthesis(Bayesian,Inferential,by demonstration,by example)ReflectiveSelf-modifying codeSymbolicTemplateSeparationof concernsAspectsComponentsData-drivenData-orientedEvent-drivenFeaturesIntentionalLiterateRolesSubjectsvteTypes of programming languagesLevelMachineAssemblyCompiledInterpretedLow-levelHigh-levelVery high-levelEsotericGenerationFirstSecondThirdFourthFifthRetrieved from "https://en.wikipedia.org/w/index.php?title=Concurrent_computing&oldid=1241386672"Categories:Concurrent computingOperating system technologyHidden categories:All articles with bare URLs for citationsArticles with bare URLs for citations from August 2024Articles with short descriptionShort description is different from WikidataArticles needing additional references from February 2014All articles needing additional referencesArticles needing additional references from December 2016Articles that may contain original research from December 2016All articles that may contain original researchArticles with multiple maintenance issuesAll articles with unsourced statementsArticles with unsourced statements from December 2016Articles needing additional references from December 2006Articles to be expanded from February 2014All articles to be expandedArticles with unsourced statements from May 2013Articles with unsourced statements from August 2010Commons category link is on WikidataThis page was last edited on 20 August 2024, at 21:22(UTC).Text is available under theCreative Commons Attribution-ShareAlike License 4.0;
additional terms may apply. By using this site, you agree to theTerms of UseandPrivacy Policy. Wikipedia® is a registered trademark of theWikimedia Foundation, Inc., a non-profit organization.Privacy policyAbout WikipediaDisclaimersContact WikipediaCode of ConductDevelopersStatisticsCookie statementMobile view