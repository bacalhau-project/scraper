URL: https://docs.bacalhau.org/references/jobs/job/task/sources/s3

Bacalhau Docsv.1.4.0v.1.3.0v.1.3.1v.1.3.2v.1.4.0GitHubSlackContactMoreGitHubSlackContactAsk or SearchCtrl+â€†KWelcomeGetting StartedHow Bacalhau WorksInstallationCreate NetworkHardware SetupContainer OnboardingDocker WorkloadsWebAssembly (Wasm) WorkloadsSetting UpRunning NodesNode OnboardingGPU InstallationJob selection policyAccess ManagementNode persistenceConnect StorageConfiguration ManagementConfiguring Transport Level SecurityLimits and TimeoutsTest Network LocallyBacalhau WebUIWorkload OnboardingContainerDocker Workload OnboardingWebAssembly (Wasm) WorkloadsBacalhau Docker ImageHow To Work With Custom Containers in BacalhauPythonBuilding and Running Custom Python ContainerRunning Pandas on BacalhauRunning a Python ScriptRunning Jupyter Notebooks on BacalhauScripting Bacalhau with PythonR (language)Building and Running your Custom R Containers on BacalhauRunning a Simple R Script on BacalhauRun CUDA programs on BacalhauRunning a Prolog ScriptReading Data from Multiple S3 Buckets using BacalhauRunning Rust programs as WebAssembly (WASM)Generate Synthetic Data using Sparkov Data Generation techniqueData IngestionCopy Data from URL to Public StoragePinning DataRunning a Job over S3 dataNetworking InstructionsAccessing the Internet from JobsUtilizing NATS.io within BacalhauGPU Workloads SetupAutomatic Update CheckingMarketplace DeploymentsGoogle Cloud MarketplaceGuidesWrite a config.yamlWrite a SpecConfigExamplesData EngineeringUsing Bacalhau with DuckDBEthereum Blockchain Analysis with Ethereum-ETL and BacalhauConvert CSV To Parquet Or AvroSimple Image ProcessingOceanography - Data ConversionVideo ProcessingModel InferenceEasyOCR (Optical Character Recognition) on BacalhauRunning Inference on Dolly 2.0 Model with Hugging FaceSpeech Recognition using WhisperStable Diffusion on a GPUStable Diffusion on a CPUObject Detection with YOLOv5 on BacalhauGenerate Realistic Images using StyleGAN3 and BacalhauStable Diffusion Checkpoint InferenceRunning Inference on a Model stored on S3Model TrainingTraining Pytorch Model with BacalhauTraining Tensorflow ModelStable Diffusion Dreambooth (Finetuning)Molecular DynamicsRunning BIDS Apps on BacalhauCoresets On BacalhauGenomics Data GenerationGromacs for AnalysisMolecular Simulation with OpenMM and BacalhauReferencesJobs GuideJob SpecificationJob TypesTask SpecificationEnginesDocker Engine SpecificationWebAssembly (WASM) Engine SpecificationPublishersIPFS Publisher SpecificationLocal Publisher SpecificationS3 Publisher SpecificationSourcesIPFS Source SpecificationLocal Source SpecificationS3 Source SpecificationURL Source SpecificationNetwork SpecificationInput Source SpecificationResources SpecificationResultPath SpecificationConstraint SpecificationLabels SpecificationMeta SpecificationJob TemplatesQueuing & TimeoutsJob QueuingTimeouts SpecificationJob ResultsStateCLI GuideSingle CLI commandsAgentAgent OverviewAgent AliveAgent NodeAgent VersionConfigConfig OverviewConfig Auto-ResourcesConfig DefaultConfig ListConfig SetJobJob OverviewJob DescribeJob ExecJob ExecutionsJob HistoryJob ListJob LogsJob RunJob StopNodeNode OverviewNode ApproveNode DeleteNode ListNode DescribeNode RejectCLI Commands OverviewCommand MigrationAPI GuideBacalhau API overviewBest PracticesAgent EndpointOrchestrator EndpointMigration APINode ManagementAuthentication & AuthorizationDatabase IntegrationDebuggingDebugging Failed JobsDebugging LocallyOpen Telemetry in BacalhauRunning locally in 'devstack'Setting up Dev EnvironmentHelp & FAQBacalhau FAQsRelease NotesGlossaryIntegrationsApache Airflow Provider for BacalhauLilypadBacalhau Python SDKObservability for WebAssembly WorkloadsCommunitySocial MediaStyle GuideWays to ContributePowered by GitBookS3 Source SpecificationThe S3 Input Source provides a seamless way to utilize data stored in S3 or any S3-compatible storage service as input for Bacalhau jobs. Users can specify files or entire prefixes stored in S3 buckets to be fetched and mounted directly into the task's execution environment. This capability ensures that your tasks have immediate access to the necessary data.Source Specification ParametersHere are the parameters that you can define for an S3 input source:Bucket(string: <required>): The name of the S3 bucket where the data is stored.Key(string: <optional>): The object key or prefix within the bucket. Supports trailing wildcard for fetching multiple objects with matching prefixes.Filter(string: <optional>): A regex pattern to filter the objects to be fetched. If aKeyis also provided as a prefix, the filter pattern will be applied to object keys after the prefix.Region(string: <optional>): The AWS region where the S3 bucket is hosted.Endpoint(string: <optional>): The endpoint URL of the S3 or S3-compatible service.VersionID(string: <optional>): The specific version of the object if versioning is enabled on the bucket. Only applicable when fetching a single object, and not a prefix or a pattern of objects.ChecksumSHA256(string: <optional>): The SHA-256 checksum of the object to ensure data integrity. Only applicable when fetching a single object, and not a prefix or a pattern of objects.Fetching MechanismSingle Object: If the key points to a single object, that object is fetched and made available to the task. e.g.s3://myBucket/dir/file-001.txtPrefix Matching: If the key ends with a slash (/), it's interpreted as a prefix, and all objects with keys that start with that prefix are fetched, mimicking the behavior of fetching all objects in a "directory". e.g.s3://myBucket/dir/Wildcard: Supports a trailing wildcard (*). All objects with keys matching the prefix are fetched, facilitating batch processing or analysis of multiple files. e.g.s3://myBucket/dir/log-2023-09-*ExamplesDeclarative ExamplesWhen using the Bacalhau YAML configuration to define the S3 input source, you can employ the following declarative approach.Below is an example of how to define an S3 input source in YAML format.CopyInputSources:-Source:Type:"s3"Params:Bucket:"my-bucket"Key:"data/"Endpoint:"https://s3.us-west-2.amazonaws.com"ChecksumSHA256:"e3b0c44b542b..."-Target:"/data"Imperative ExamplesWhen using the Bacalhau CLI to define the S3 input source, you can employ the following imperative approach. Below are example commands demonstrating how to define the S3 input source with various configurations:Mount an S3 object to a specific path:Copybacalhaudockerrun-isrc=s3://bucket/key,dst=/my/input/pathubuntu...Mount an S3 object with a specific endpoint and region:Copybacalhau docker run -i src=s3://bucket/key,dst=/my/input/path,opt=endpoint=http://s3.example.com,opt=region=us-east-1 ubuntu ...Mount an S3 object using long flag names:Copybacalhaudockerrun--inputsource=s3://bucket/key,destination=/my/input/pathubuntu...With these commands, you can seamlessly fetch and mount data from S3 into your task's execution environment directly through the CLI.Credential RequirementsTo support this storage provider, no extra dependencies are necessary. However, valid AWS credentials are essential to sign the requests. The storage provider employs the default credentials chain to retrieve credentials, primarily sourcing them from:Environment variables: AWS credentials can be specified usingAWS_ACCESS_KEY_IDandAWS_SECRET_ACCESS_KEYenvironment variables.Credentials file: The credentials file typically located at~/.aws/credentialscan also be used to fetch the necessary AWS credentials.IAM Roles for Amazon EC2 Instances: If you're running your tasks within an Amazon EC2 instance, IAM roles can be utilized to provide the necessary permissions and credentials.For a more detailed overview on AWS credential management and other ways to provide these credentials, please refer to the AWS official documentation onstandardized credentials.Required IAM PoliciesCompute nodes must run with the following policies to support S3 input source:Copy{"Version":"2012-10-17","Statement":[{"Effect":"Allow","Action":"s3:ListBucket","Resource":"arn:aws:s3:::BUCKET_NAME"},{"Effect":"Allow","Action":["s3:GetObject","s3:GetObjectVersion"],"Resource":"arn:aws:s3:::BUCKET_NAME/*"}]}ListBucket Permission:Thes3:ListBucketpermission is necessary to list the objects within the specified S3 bucket, allowing prefixes and wildcard expressions as the S3 Key for fetching.GetObject and GetObjectVersion Permissions:Thes3:GetObjectands3:GetObjectVersionpermissions enable the fetching of object data and its versions, respectively.Resource:TheResourcefield in the policy specifies the Amazon Resource Name (ARN) of the S3 bucket. The/*suffix is necessary to allow fetching of all objects within the bucket or can be replaced with a prefix to limit the scope of the policy. You can also specify multiple resources in the policy to allow fetching from multiple buckets, or*to allow fetching from all buckets in the account.For more information on IAM policies specific to Amazon S3 buckets and users, please refer to theAWS documentation on Using IAM Policies with Amazon S3.S3-Compatible ServicesThis feature isn't limited to AWS S3 - it supports all S3-compatible storage services. It means you can pull data from the likes of Google Cloud Storage and open-source solutions like MinIO, giving you the flexibility to utilize a diverse range of data sources.Using Google Cloud StorageTo seamlessly integrate Google Cloud Storage with Bacalhau, follow these steps:Obtain HMAC Keys:To access Google Cloud Storage, you'll need HMAC (Hash-based Message Authentication Code) keys. Refer to theGoogle Cloud documentationfor detailed instructions on creating a service account and generating HMAC keys.Provide HMAC Keys to Bacalhau:You can provide the HMAC keys to Bacalhau using the same options as AWS credentials, as documented in theCredential Requirementssection.Configure the S3 Input Source:In your S3 input source configuration, set the endpoint for Google Cloud Storage tohttps://storage.googleapis.com, as shown in the example below:CopyInputSources:-Source:Type:"s3"Params:Bucket:"my-bucket"Key:"data/"Endpoint:"https://storage.googleapis.com"-Target:"/data"PreviousLocal Source SpecificationNextURL Source SpecificationOn this pageSource Specification ParametersFetching MechanismExamplesDeclarative ExamplesImperative ExamplesCredential RequirementsRequired IAM PoliciesS3-Compatible ServicesUsing Google Cloud StorageWas this helpful?Edit on GitHubExport as PDFGet SupportExpansoSupportUse CasesDistributed ETLEdge MLDistributed Data WarehousingFlett ManagementAbout UsWho we areWhat we valueNews & BlogBlogNewsExpanso (2024). All Rights Reserved.