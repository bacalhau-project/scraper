Source: bacalhau_docs
URL: https://docs.bacalhau.org/examples/model-inference/running-inference-on-a-model-stored-on-s3

Running Inference on a Model stored on S3
Introduction
In this example, we will demonstrate how to run inference on a model stored on Amazon S3. We will use a PyTorch model trained on the MNIST dataset.
Running Locally
Prerequisites
Consider using the latest versions or use the docker method listed below in the article.
Python
PyTorch
Downloading the Datasets
Use the following commands to download the model and test image:
Creating the Inference Script
This script is designed to load a pretrained PyTorch model for MNIST digit classification from a tar.gz
file, extract it, and use the model to perform inference on a given input image. Ensure you have all required dependencies installed:
Running the Inference Script
To use this script, you need to provide the paths to the tar.gz
file containing the pre-trained model, the output directory where the model will be extracted, and the input image file for which you want to perform inference. The script will output the predicted digit (class) for the given input image.
Running Inference on Bacalhau
Prerequisite
To get started, you need to install the Bacalhau client, see more information [here](/getting-started/installation)
Structure of the Command
export JOB_ID=$( ... )
: Export results of a command execution as environment variable-w /inputs
Set the current working directory at/inputs
in the container-i src=s3://sagemaker-sample-files/datasets/image/MNIST/model/pytorch-training-2020-11-21-22-02-56-203/model.tar.gz,dst=/model/,opt=region=us-east-1
: Mount the s3 bucket at the destination path provided -/model/
and specifying the region where the bucket is locatedopt=region=us-east-1
-i git://github.com/js-ts/mnist-test.git
: Flag to mount the source code repo from GitHub. It would mount the repo at/inputs/js-ts/mnist-test
in this case it also contains the test imagepytorch/pytorch
: The name of the Docker image-- python3 /inputs/js-ts/mnist-test/inference.py --tar_gz_file_path /model/model.tar.gz --output_directory /model-pth --image_path /inputs/js-ts/mnist-test/image.png
: The command to run inference on the model. It consists of:/model/model.tar.gz
is the path to the model file/model-pth
is the output directory for the model/inputs/js-ts/mnist-test/image.png
is the path to the input image
When the job is submitted Bacalhau prints out the related job id. We store that in an environment variable JOB_ID
so that we can reuse it later on.
Viewing the Output
Use the bacalhau job logs
command to view the job output, since the script prints the result of execution to the stdout:
You can also use bacalhau job get
to download job results:
Support
If you have questions or need support or guidance, please reach out to the [Bacalhau team via Slack](https://bacalhauproject.slack.com/ssb/redirect) (#general channel).
Last updated