URL: https://docs.bacalhau.org/examples/data-engineering/ethereum-blockchain-analysis-with-ethereum-etl-and-bacalhau

Bacalhau Docsv.1.4.0v.1.3.0v.1.3.1v.1.3.2v.1.4.0GitHubSlackContactMoreGitHubSlackContactAsk or SearchCtrl+ KWelcomeGetting StartedHow Bacalhau WorksInstallationCreate NetworkHardware SetupContainer OnboardingDocker WorkloadsWebAssembly (Wasm) WorkloadsSetting UpRunning NodesNode OnboardingGPU InstallationJob selection policyAccess ManagementNode persistenceConnect StorageConfiguration ManagementConfiguring Transport Level SecurityLimits and TimeoutsTest Network LocallyBacalhau WebUIWorkload OnboardingContainerDocker Workload OnboardingWebAssembly (Wasm) WorkloadsBacalhau Docker ImageHow To Work With Custom Containers in BacalhauPythonBuilding and Running Custom Python ContainerRunning Pandas on BacalhauRunning a Python ScriptRunning Jupyter Notebooks on BacalhauScripting Bacalhau with PythonR (language)Building and Running your Custom R Containers on BacalhauRunning a Simple R Script on BacalhauRun CUDA programs on BacalhauRunning a Prolog ScriptReading Data from Multiple S3 Buckets using BacalhauRunning Rust programs as WebAssembly (WASM)Generate Synthetic Data using Sparkov Data Generation techniqueData IngestionCopy Data from URL to Public StoragePinning DataRunning a Job over S3 dataNetworking InstructionsAccessing the Internet from JobsUtilizing NATS.io within BacalhauGPU Workloads SetupAutomatic Update CheckingMarketplace DeploymentsGoogle Cloud MarketplaceGuidesWrite a config.yamlWrite a SpecConfigExamplesData EngineeringUsing Bacalhau with DuckDBEthereum Blockchain Analysis with Ethereum-ETL and BacalhauConvert CSV To Parquet Or AvroSimple Image ProcessingOceanography - Data ConversionVideo ProcessingModel InferenceEasyOCR (Optical Character Recognition) on BacalhauRunning Inference on Dolly 2.0 Model with Hugging FaceSpeech Recognition using WhisperStable Diffusion on a GPUStable Diffusion on a CPUObject Detection with YOLOv5 on BacalhauGenerate Realistic Images using StyleGAN3 and BacalhauStable Diffusion Checkpoint InferenceRunning Inference on a Model stored on S3Model TrainingTraining Pytorch Model with BacalhauTraining Tensorflow ModelStable Diffusion Dreambooth (Finetuning)Molecular DynamicsRunning BIDS Apps on BacalhauCoresets On BacalhauGenomics Data GenerationGromacs for AnalysisMolecular Simulation with OpenMM and BacalhauReferencesJobs GuideJob SpecificationJob TypesTask SpecificationEnginesDocker Engine SpecificationWebAssembly (WASM) Engine SpecificationPublishersIPFS Publisher SpecificationLocal Publisher SpecificationS3 Publisher SpecificationSourcesIPFS Source SpecificationLocal Source SpecificationS3 Source SpecificationURL Source SpecificationNetwork SpecificationInput Source SpecificationResources SpecificationResultPath SpecificationConstraint SpecificationLabels SpecificationMeta SpecificationJob TemplatesQueuing & TimeoutsJob QueuingTimeouts SpecificationJob ResultsStateCLI GuideSingle CLI commandsAgentAgent OverviewAgent AliveAgent NodeAgent VersionConfigConfig OverviewConfig Auto-ResourcesConfig DefaultConfig ListConfig SetJobJob OverviewJob DescribeJob ExecJob ExecutionsJob HistoryJob ListJob LogsJob RunJob StopNodeNode OverviewNode ApproveNode DeleteNode ListNode DescribeNode RejectCLI Commands OverviewCommand MigrationAPI GuideBacalhau API overviewBest PracticesAgent EndpointOrchestrator EndpointMigration APINode ManagementAuthentication & AuthorizationDatabase IntegrationDebuggingDebugging Failed JobsDebugging LocallyOpen Telemetry in BacalhauRunning locally in 'devstack'Setting up Dev EnvironmentHelp & FAQBacalhau FAQsRelease NotesGlossaryIntegrationsApache Airflow Provider for BacalhauLilypadBacalhau Python SDKObservability for WebAssembly WorkloadsCommunitySocial MediaStyle GuideWays to ContributePowered by GitBookEthereum Blockchain Analysis with Ethereum-ETL and BacalhauIntroductionMature blockchains are difficult to analyze because of their size. Ethereum-ETL is a tool that makes it easy to extract information from an Ethereum node, but it's not easy to get working in a batch manner. It takes approximately 1 week for an Ethereum node to download the entire chain (even more in my experience) and importing and exporting data from the Ethereum node is slow.For this example, we ran an Ethereum node for a week and allowed it to synchronize. We then ran ethereum-etl to extract the information and pinned it on Filecoin. This means that we can both now access the data without having to run another Ethereum node.But there's still a lot of data and these types of analyses typically need repeating or refining. So it makes absolute sense to use a decentralized network like Bacalhau to process the data in a scalable way.In this tutorial example, we will run Ethereum-ETL tool on Bacalhau to extract data from an Ethereum node.Prerequisite​To get started, you need to install the Bacalhau client, see more informationhereAnalysing Ethereum Data Locally​First let's download one of the IPFS files and inspect it locally:Copywget-q-Ofile.tar.gzhttps://w3s.link/ipfs/bafybeifgqjvmzbtz427bne7af5tbndmvniabaex77us6l637gqtb2iwlwqtar-xvffile.tar.gzYou can see the full list of IPFS CIDs in the appendix at the bottom of the page.If you don't already have the Pandas library, let's install it:CopypipinstallpandasCopy# Use pandas to read in transaction data and clean up the columnsimportpandasaspdimportglobfile=glob.glob('output_*/transactions/start_block=*/end_block=*/transactions*.csv')[0]print("Loading file%s"%file)df=pd.read_csv(file)df['value']=df['value'].astype('float')df['from_address']=df['from_address'].astype('string')df['to_address']=df['to_address'].astype('string')df['hash']=df['hash'].astype('string')df['block_hash']=df['block_hash'].astype('string')df['block_datetime']=pd.to_datetime(df['block_timestamp'], unit='s')df.info()# Total volume per daydf[['block_datetime','value']].groupby(pd.Grouper(key='block_datetime', freq='1D')).sum().plot()The following code inspects the daily trading volume of Ethereum for a single chunk (100,000 blocks) of data.This is all good, but we can do better. We can use the Bacalhau client to download the data from IPFS and then run the analysis on the data in the cloud. This means that we can analyze the entire Ethereum blockchain without having to download it locally.Analysing Ethereum Data With Bacalhau​To run jobs on the Bacalhau network you need to package your code. In this example, I will package the code as a Docker image.But before we do that, we need to develop the code that will perform the analysis. The code below is a simple script to parse the incoming data and produce a CSV file with the daily trading volume of Ethereum.Copy# main.pyimportglob,os,sys,shutil,tempfileimportpandasaspddefmain(input_dir,output_dir):search_path = os.path.join(input_dir, "output*", "transactions", "start_block*", "end_block*", "transactions_*.csv")csv_files=glob.glob(search_path)iflen(csv_files)==0:print("No CSV files found in%s"%search_path)sys.exit(1)fortransactions_fileincsv_files:print("Loading%s"%transactions_file)df=pd.read_csv(transactions_file)df['value']=df['value'].astype('float')df['block_datetime']=pd.to_datetime(df['block_timestamp'], unit='s')print("Processing%dblocks"%(df.shape[0]))results=df[['block_datetime','value']].groupby(pd.Grouper(key='block_datetime', freq='1D')).sum()print("Finished processing%ddays worth of records"%(results.shape[0]))save_path=os.path.join(output_dir, os.path.basename(transactions_file))os.makedirs(os.path.dirname(save_path), exist_ok=True)print("Saving to%s"%(save_path))results.to_csv(save_path)defextractData(input_dir,output_dir):search_path=os.path.join(input_dir,"*.tar.gz")gz_files=glob.glob(search_path)iflen(gz_files)==0:print("No tar.gz files found in%s"%search_path)sys.exit(1)forfingz_files:shutil.unpack_archive(filename=f, extract_dir=output_dir)if__name__=="__main__":iflen(sys.argv)!=3:print('Must pass arguments. Format: [command] input_dir output_dir')sys.exit()withtempfile.TemporaryDirectory()astmp_dir:extractData(sys.argv[1], tmp_dir)main(tmp_dir, sys.argv[2])Next, let's make sure the file works as expected:Copypythonmain.py.outputs/And finally, package the code inside a Docker image to make the process reproducible. Here I'm passing the Bacalhau default/inputsand/outputsdirectories. The/inputsdirectory is where the data will be read from and the/outputsdirectory is where the results will be saved to.CopyFROMpython:3.11-slim-bullseyeWORKDIR/srcRUNpip install pandas==1.5.1ADDmain.py .CMD["python","main.py","/inputs","/outputs"]We've already pushed the container, but for posterity, the following command pushes this container to GHCR.Copydockerbuildxbuild--platformlinux/amd64--push-tghcr.io/bacalhau-project/examples/blockchain-etl:0.0.1.Running a Bacalhau Job​To run our analysis on the Ethereum blockchain, we will use thebacalhau docker runcommand.CopyexportJOB_ID=$(bacalhaudockerrun\--id-only\--inputipfs://bafybeifgqjvmzbtz427bne7af5tbndmvniabaex77us6l637gqtb2iwlwq:/inputs/data.tar.gz\ghcr.io/bacalhau-project/examples/blockchain-etl:0.0.6)The job has been submitted and Bacalhau has printed out the related job id. We store that in an environment variable so that we can reuse it later on.Thebacalhau docker runcommand allows passing input data volume with--inputor-i ipfs://CID:pathargument just like Docker, except the left-hand side of the argument is acontent identifier (CID). This results in Bacalhau mounting adata volumeinside the container. By default, Bacalhau mounts the input volume at the path/inputsinside the container.Bacalhau also mounts a data volume to store output data. Thebacalhau docker runcommand creates an output data volume mounted at/outputs. This is a convenient location to store the results of your job.Declarative job description​The same job can be presented in thedeclarativeformat. In this case, the description will look like this:Copyname: Ethereum Blockchain Analysis with Ethereum-ETLtype: batchcount: 1tasks:- name: My main taskEngine:type: dockerparams:Image: ghcr.io/bacalhau-project/examples/blockchain-etl:0.0.6Publisher:Type: ipfsResultPaths:- Name: outputsPath: /outputsInputSources:- Target: "/inputs/data.tar.gz"Source:Type: "ipfs"Params:CID: "bafybeifgqjvmzbtz427bne7af5tbndmvniabaex77us6l637gqtb2iwlwq"The job description should be saved in.yamlformat, e.g.blockchain.yaml, and then run with the command:CopyCopybacalhau job run blockchain.yamlChecking the State of your Jobs​Job status: You can check the status of the job usingbacalhau job list.Copybacalhaujoblist--id-filter${JOB_ID}When it saysPublishedorCompleted, that means the job is done, and we can get the results.Job information: You can find out more information about your job by usingbacalhau job describe.Copybacalhaujobdescribe${JOB_ID}Job download: You can download your job results directly by usingbacalhau job get. Alternatively, you can choose to create a directory to store your results. In the command below, we created a directory (results) and downloaded our job output to be stored in that directory.Copyrm-rfresults&&mkdir-presults# Temporary directory to store the resultsbacalhaujobget${JOB_ID}--output-dirresults# Download the resultsViewing your Job Output​To view the file, run the following command:Copyls-lahresults/outputsDisplay the image​To view the images, we will useglobto return all file paths that match a specific pattern.Copyimportglobimportpandasaspd# Get CSV files list from a foldercsv_files=glob.glob("results/outputs/*.csv")df=pd.read_csv(csv_files[0], index_col='block_datetime')df.plot()Massive Scale Ethereum Analysis​Ok, so that works. Let's scale this up! We can run the same analysis on the entire Ethereum blockchain (up to the point where I have uploaded the Ethereum data). To do this, we need to run the analysis on each of the chunks of data that we have stored on IPFS. We can do this by running the same job on each of the chunks.See the appendix for thehashes.txtfile.Copyprintf"">job_ids.txtforhin$(cathashes.txt);do\bacalhaudockerrun\--id-only\--wait=false\--input=ipfs://$h:/inputs/data.tar.gz\ghcr.io/bacalhau-project/examples/blockchain-etl:0.0.6>>job_ids.txtdoneNow take a look at the job id's. You can use these to check the status of the jobs and download the results:Copycatjob_ids.txtd840df7b-9318-4e5b-ab06-adb72dd9539409d01f9c-9409-42b9-829d-92e22fcdd0620072758f-3575-44d7-b193-da4a22f6bc862043dee4-fc82-4768-92cb-4d23dd2514b136ef8e9e-9eae-4218-81e6-15883d0a5b8d932aa406-cd29-4933-b09f-c8cea4d771641f3e5273-bdd4-4ef0-b7ed-b83591fab64e8bfabe96-54e3-4fee-b344-a0517c6832681cd588a1-5c76-4f91-ba90-af7931bca596b9c29531-e1b4-4520-b03d-7406a22bbdb38665b8be-24a9-4c78-9913-803d3e3c9a6506115147-bc83-49e8-bb71-7b447c8ad1bc84afed3e-831c-462b-a3e3-9a23bc7d6fb8ed6e55e6-98d3-4bde-8ece-1f05838d489e...You might want to double-check that the jobs ran ok by doing abacalhau job list.Copybacalhaujoblist-n50Wait until all of these jobs have been completed. And then download all the results and merge them into a single directory. This might take a while, so this is a good time to treat yourself to a nice Dark Mild. There's also been some issues in the past communicating with IPFS, so if you get an error, try again.Copyforidin$(catjob_ids.txt);do\rm-rfresults_$id&&mkdirresults_$idbacalhaujobget--output-dirresults_$id $id&donewaitDisplay the image​To view the images, we will useglobto return all file paths that match a specific pattern.Copyimportos,globimportpandasaspd# Get CSV files list from a folderpath=os.path.join("results_*","outputs","*.csv")csv_files=glob.glob(path)# Read each CSV file into a list of DataFramesdf_list=(pd.read_csv(file, index_col='block_datetime')forfileincsv_files)# Concatenate all DataFramesdf_unsorted=pd.concat(df_list, ignore_index=False)# Some files will cross days, so group by day and sum the valuesdf=df_unsorted.groupby(level=0).sum()# Plotdf.plot(figsize=(16,9))That's it! There are several years of Ethereum transaction volume data.Copyrm-rfresults_*output_*outputsresultstemp# Remove temporary resultsAppendix: List Ethereum Data CIDs​The following list is a list of IPFS CID's for the Ethereum data that we used in this tutorial. You can use these CID's to download the rest of the chain if you so desire. The CIDs are ordered by block number and they increase 50,000 blocks at a time. Here's a list of ordered CIDs:Copy# hashes.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​If you have questions or need support or guidance, please reach out to theBacalhau team via Slack(#generalchannel).PreviousUsing Bacalhau with DuckDBNextConvert CSV To Parquet Or AvroLast updated1 month agoOn this pageIntroductionPrerequisite​Analysing Ethereum Data Locally​Analysing Ethereum Data With Bacalhau​Running a Bacalhau Job​Declarative job description​Checking the State of your Jobs​Viewing your Job Output​Massive Scale Ethereum Analysis​Display the image​Appendix: List Ethereum Data CIDs​Support​Was this helpful?Edit on GitHubExport as PDFGet SupportExpansoSupportUse CasesDistributed ETLEdge MLDistributed Data WarehousingFlett ManagementAbout UsWho we areWhat we valueNews & BlogBlogNewsExpanso (2024). All Rights Reserved.