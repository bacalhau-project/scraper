URL: https://en.wikipedia.org/wiki/High_availability

Jump to contentMain menuMain menumove to sidebarhideNavigationMain pageContentsCurrent eventsRandom articleAbout WikipediaContact usDonateContributeHelpLearn to editCommunity portalRecent changesUpload fileSearchSearchAppearanceCreate accountLog inPersonal toolsCreate accountLog inPages for logged out editorslearn moreContributionsTalkContentsmove to sidebarhide(Top)1Resilience2Principles3Scheduled and unscheduled downtime4Percentage calculationToggle Percentage calculation subsection4.1Five-by-five mnemonic4.2"Powers of 10" trick4.3"Nines"5Measurement and interpretation6Closely related concepts7Military control systems8System design9Reasons for unavailability10Costs of unavailability11See also12Notes13References14External linksToggle the table of contentsHigh availability14 languagesCatalàČeštinaDeutschEspañolFrançais한국어日本語Norsk bokmålPolskiRomânăРусскийSuomiTürkçe中文Edit linksArticleTalkEnglishReadEditView historyToolsToolsmove to sidebarhideActionsReadEditView historyGeneralWhat links hereRelated changesUpload fileSpecial pagesPermanent linkPage informationCite this pageGet shortened URLDownload QR codeWikidata itemPrint/exportDownload as PDFPrintable versionAppearancemove to sidebarhideFrom Wikipedia, the free encyclopediaSystems with high up-time, a.k.a. "always on""Always-on" redirects here. For the software restriction, seeAlways-on DRM.High availability(HA) is a characteristic of a system that aims to ensure an agreed level of operational performance, usuallyuptime, for a higher than normal period.[1]Modernization has resulted in an increased reliance on these systems.  For example, hospitals and data centers require high availability of their systems to perform routine daily activities.Availabilityrefers to the ability of the user community to obtain a service or good, access the system, whether to submit new work, update or alter existing work, or collect the results of previous work. If a user cannot access the system, it is – from the user's point of view –unavailable.[2]Generally, the termdowntimeis used to refer to periods when a system is unavailable.Resilience[edit]High availability is a property ofnetworkresilience, the ability to "provide and maintain an acceptable level of service in the face offaultsand challenges to normal operation."[3]Threats and challenges for services can range from simple misconfiguration over large scale natural disasters to targeted attacks.[4]As such, network resilience touches a very wide range of topics. In order to increase the resilience of a given communication network, the probable challenges and risks have to be identified and appropriate resilience metrics have to be defined for the service to be protected.[5]The importance of network resilience is continuously increasing, as communication networks are becoming a fundamental component in the operation of critical infrastructures.[6]Consequently, recent efforts focus on interpreting and improving network and computing resilience with applications to critical infrastructures.[7]As an example, one can consider as a resilience objective the provisioning of services over the network, instead of the services of the network itself. This may require coordinated response from both the network and from the services running on top of the network.[8]These services include:supportingdistributed processingsupportingnetwork storagemaintaining service of communication services such asvideo conferencinginstant messagingonline collaborationaccess to applications and data as neededResilience andsurvivabilityare interchangeably used according to the specific context of a given study.[9]Principles[edit]Further information:Design for availabilityThere are three principles ofsystems designinreliability engineeringwhich can help achieve high availability.Elimination ofsingle points of failure. This means adding or building redundancy into the system so that failure of a component does not mean failure of the entire system.Reliable crossover. Inredundant systems, the crossover point itself tends to become a single point of failure.  Reliable systems must provide for reliable crossover.Detection of failures as they occur. If the two principles above are observed, then a user may never see a failure – but the maintenance activity must.Scheduled and unscheduled downtime[edit]This sectiondoes notciteanysources.Please helpimprove this sectionbyadding citations to reliable sources. Unsourced material may be challenged andremoved.(June 2008)(Learn how and when to remove this message)A distinction can be made between scheduled and unscheduled downtime. Typically,scheduled downtimeis a result ofmaintenancethat is disruptive to system operation and usually cannot be avoided with a currently installed system design. Scheduled downtime events might include patches tosystem softwarethat require arebootor system configuration changes that only take effect upon a reboot. In general, scheduled downtime is usually the result of some logical, management-initiated event. Unscheduled downtime events typically arise from some physical event, such as a hardware or software failure or environmental anomaly. Examples of unscheduled downtime events include power outages, failedCPUorRAMcomponents (or possibly other failed hardware components), an over-temperature related shutdown, logically or physically severed network connections, security breaches, or variousapplication,middleware, andoperating systemfailures.If users can be warned away from scheduled downtimes, then the distinction is useful.  But if the requirement is for true high availability, then downtime is downtime whether or not it is scheduled.Many computing sites exclude scheduled downtime from availability calculations, assuming that it has little or no impact upon the computing user community. By doing this, they can claim to have phenomenally high availability, which might give the illusion ofcontinuous availability. Systems that exhibit truly continuous availability are comparatively rare and higher priced, and most have carefully implemented specialty designs that eliminate anysingle point of failureand allow online hardware, network, operating system,middleware, and application upgrades, patches, and replacements. For certain systems, scheduled downtime does not matter, for example system downtime at an office building after everybody has gone home for the night.Percentage calculation[edit]Availability is usually expressed as a percentage of uptime in a given year. The following table shows the downtime that will be allowed for a particular percentage of availability, presuming that the system is required to operate continuously.Service level agreementsoften refer to monthly downtime or availability in order to calculate service credits to match monthly billing cycles. The following table shows the translation from a given availability percentage to the corresponding amount of time a system would be unavailable.Availability %Downtime per year[note 1]Downtime per quarterDowntime per monthDowntime per weekDowntime per day (24 hours)90% ("one nine")36.53 days9.13 days73.05 hours16.80 hours2.40 hours95% ("one nine five")18.26 days4.56 days36.53 hours8.40 hours1.20 hours97% ("one nine seven")10.96 days2.74 days21.92 hours5.04 hours43.20 minutes98% ("one nine eight")7.31 days43.86 hours14.61 hours3.36 hours28.80 minutes99% ("two nines")3.65 days21.9 hours7.31 hours1.68 hours14.40 minutes99.5% ("two nines five")1.83 days10.98 hours3.65 hours50.40 minutes7.20 minutes99.8% ("two nines eight")17.53 hours4.38 hours87.66 minutes20.16 minutes2.88 minutes99.9% ("three nines")8.77 hours2.19 hours43.83 minutes10.08 minutes1.44 minutes99.95% ("three nines five")4.38 hours65.7 minutes21.92 minutes5.04 minutes43.20 seconds99.99% ("four nines")52.60 minutes13.15 minutes4.38 minutes1.01 minutes8.64 seconds99.995% ("four nines five")26.30 minutes6.57 minutes2.19 minutes30.24 seconds4.32 seconds99.999% ("five nines")5.26 minutes1.31 minutes26.30 seconds6.05 seconds864.00milliseconds99.9999% ("six nines")31.56 seconds7.89 seconds2.63 seconds604.80 milliseconds86.40 milliseconds99.99999% ("seven nines")3.16 seconds0.79 seconds262.98 milliseconds60.48 milliseconds8.64 milliseconds99.999999% ("eight nines")315.58 milliseconds78.89 milliseconds26.30 milliseconds6.05 milliseconds864.00microseconds99.9999999% ("nine nines")31.56 milliseconds7.89 milliseconds2.63 milliseconds604.80 microseconds86.40 microseconds99.99999999% ("ten nines")3.16 milliseconds788.40 microseconds262.80 microseconds60.48 microseconds8.64 microseconds99.999999999% ("eleven nines")315.58 microseconds78.84 microseconds26.28 microseconds6.05 microseconds864.00nanoseconds99.9999999999% ("twelve nines")31.56 microseconds7.88 microseconds2.63 microseconds604.81 nanoseconds86.40 nanosecondsThe termsuptimeandavailabilityare often used interchangeably but do not always refer to the same thing. For example, a system can be "up" with its services not "available" in the case of anetwork outage. Or a system undergoing software maintenance can be "available" to be worked on by asystem administrator, but its services do not appear "up" to theend useror customer. The subject of the terms is thus important here: whether the focus of a discussion is the server hardware, server OS, functional service, software service/process, or similar, it is only if there is a single, consistent subject of the discussion that the words uptime and availability can be used synonymously.Five-by-five mnemonic[edit]A simple mnemonic rule states that5 ninesallows approximately 5 minutes of downtime per year. Variants can be derived by multiplying or dividing by 10: 4 nines is 50 minutes and 3 nines is 500 minutes. In the opposite direction, 6 nines is 0.5 minutes (30 sec) and 7 nines is 3 seconds."Powers of 10" trick[edit]Another memory trick to calculate the allowed downtime duration for an "n{\displaystyle n}-nines" availability percentage is to use the formula8.64×104−n{\displaystyle 8.64\times 10^{4-n}}seconds per day.For example, 90% ("one nine") yields the exponent4−1=3{\displaystyle 4-1=3}, and therefore the allowed downtime is8.64×103{\displaystyle 8.64\times 10^{3}}seconds per day.Also, 99.999% ("five nines") gives the exponent4−5=−1{\displaystyle 4-5=-1}, and therefore the allowed downtime is8.64×10−1{\displaystyle 8.64\times 10^{-1}}seconds per day."Nines"[edit]Main article:Nine (purity)Percentages of a particular order of magnitude are sometimes referred to by thenumber of ninesor "class of nines" in the digits.  For example, electricity that is delivered without interruptions (blackouts,brownoutsorsurges) 99.999% of the time would have 5 nines reliability, or class five.[10]In particular, the term is used in connection withmainframes[11][12]or enterprise computing, often as part of aservice-level agreement.Similarly, percentages ending in a 5 have conventional names, traditionally the number of nines, then "five", so 99.95% is "three nines five", abbreviated 3N5.[13][14]This is casually referred to as "three and a half nines",[15]but this is incorrect: a 5 is only a factor of 2, while a 9 is a factor of 10, so a 5 is 0.3 nines (per below formula:log10⁡2≈0.3{\displaystyle \log _{10}2\approx 0.3}):[note 2]99.95% availability is 3.3 nines, not 3.5 nines.[16]More simply, going from 99.9% availability to 99.95% availability is a factor of 2 (0.1% to 0.05% unavailability), but going from 99.95% to 99.99% availability is a factor of 5 (0.05% to 0.01% unavailability), over twice as much.[note 3]A formulation of theclass of 9sc{\displaystyle c}based on a system'sunavailabilityx{\displaystyle x}would bec:=⌊−log10⁡x⌋{\displaystyle c:=\lfloor -\log _{10}x\rfloor }(cf.Floor and ceiling functions).Asimilar measurementis sometimes used to describe the purity of substances.In general, the number of nines is not often used by a network engineer when modeling and measuring availability because it is hard to apply in formula. More often, the unavailability expressed as aprobability(like 0.00001), or adowntimeper year is quoted. Availability specified as a number of nines is often seen inmarketingdocuments.[citation needed]The use of the "nines" has been called into question, since it does not appropriately reflect that the impact of unavailability varies with its time of occurrence.[17]For large amounts of 9s, the "unavailability" index (measure of downtime rather than uptime) is easier to handle. For example, this is why an "unavailability" rather than availability metric is used in hard disk or data linkbit error rates.Sometimes the humorous term "nine fives" (55.5555555%) is used to contrast with "five nines" (99.999%),[18][19][20]though this is not an actual goal, but rather a sarcastic reference to something totally failing to meet any reasonable target.Measurement and interpretation[edit]Availability measurement is subject to some degree of interpretation. A system that has been up for 365 days in a non-leap year might have been eclipsed by a network failure that lasted for 9 hours during a peak usage period; the user community will see the system as unavailable, whereas the system administrator will claim 100% uptime. However, given the true definition of availability, the system will be approximately 99.9% available, or three nines (8751 hours of available time out of 8760 hours per non-leap year). Also, systems experiencing performance problems are often deemed partially or entirely unavailable by users, even when the systems are continuing to function. Similarly, unavailability of select application functions might go unnoticed by administrators yet be devastating to users – a true availability measure is holistic.Availability must be measured to be determined, ideally with comprehensive monitoring tools ("instrumentation") that are themselves highly available. If there is a lack of instrumentation, systems supporting high volume transaction processing throughout the day and night, such as credit card processing systems or telephone switches, are often inherently better monitored, at least by the users themselves, than systems which experience periodic lulls in demand.An alternative metric ismean time between failures(MTBF).Closely related concepts[edit]Recovery time (or estimated time of repair (ETR), also known asrecovery time objective(RTO) is closely related to availability, that is the total time required for a planned outage or the time required to fully recover from an unplanned outage. Another metric ismean time to recovery(MTTR).  Recovery time could be infinite with certain system designs and failures, i.e. full recovery is impossible. One such example is a fire or flood that destroys a data center and its systems when there is no secondarydisaster recoverydata center.Another related concept isdata availability, that is the degree to whichdatabasesand other information storage systems faithfully record and report system transactions. Information management often focuses separately on data availability, orRecovery Point Objective, in order to determine acceptable (or actual)data losswith various failure events. Some users can tolerate application service interruptions but cannot tolerate data loss.Aservice level agreement("SLA") formalizes an organization's availability objectives and requirements.Military control systems[edit]High availability is one of the primary requirements of thecontrol systemsinunmanned vehiclesandautonomous maritime vessels. If the controlling system becomes unavailable, theGround Combat Vehicle(GCV) orASW Continuous Trail Unmanned Vessel(ACTUV) would be lost.System design[edit]Adding more components to an overall system design can undermine efforts to achieve high availability becausecomplex systemsinherently have more potential failure points and are more difficult to implement correctly. While some analysts would put forth the theory that the most highly available systems adhere to a simple architecture (a single, high quality, multi-purpose physical system with comprehensive internal hardware redundancy), this architecture suffers from the requirement that the entire system must be brought down for patching and operating system upgrades. More advanced system designs allow for systems to be patched and upgraded without compromising service availability (seeload balancingandfailover).High availability requires less human intervention to restore operation in complex systems; the reason for this being that the most common cause for outages is human error.[21]Redundancyis used to create systems with high levels of availability (e.g. aircraft flight computers). In this case it is required to have high levels of failure detectability and avoidance of common cause failures. Two kinds of redundancy are passive redundancy and active redundancy.Passive redundancy is used to achieve high availability by including enough excess capacity in the design to accommodate a performance decline. The simplest example is a boat with two separate engines driving two separate propellers. The boat continues toward its destination despite failure of a single engine or propeller. A more complex example is multiple redundant power generation facilities within a large system involvingelectric power transmission. Malfunction of single components is not considered to be a failure unless the resulting performance decline exceeds the specification limits for the entire system.Active redundancy is used in complex systems to achieve high availability with no performance decline. Multiple items of the same kind are incorporated into a design that includes a method to detect failure and automatically reconfigure the system to bypass failed items using a voting scheme. This is used with complex computing systems that are linked. Internetroutingis derived from early work by Birman and Joseph in this area.[22]Active redundancy may introduce more complex failure modes into a system, such as continuous system reconfiguration due to faulty voting logic.Zero downtime system design means that modeling and simulation indicatesmean time between failuressignificantly exceeds the period of time betweenplanned maintenance,upgradeevents, or system lifetime. Zero downtime involves massive redundancy, which is needed for some types of aircraft and for most kinds ofcommunications satellites.Global Positioning Systemis an example of a zero downtime system.Faultinstrumentationcan be used in systems with limited redundancy to achieve high availability. Maintenance actions occur during brief periods of down-time only after a fault indicator activates. Failure is only significant if this occurs during amission criticalperiod.Modeling and simulationis used to evaluate the theoretical reliability for large systems. The outcome of this kind of model is used to evaluate different design options. A model of the entire system is created, and the model is stressed by removing components. Redundancy simulation involves the N-x criteria. N represents the total number of components in the system. x is the number of components used to stress the system. N-1 means the model is stressed by evaluating performance with all possible combinations where one component is faulted. N-2 means the model is stressed by evaluating performance with all possible combinations where two component are faulted simultaneously.Reasons for unavailability[edit]A survey among academic availability experts in 2010 ranked reasons for unavailability of enterprise IT systems. All reasons refer tonot following best practicein each of the following areas (in order of importance):[23]Monitoring of the relevant componentsRequirementsand procurementOperationsAvoidance ofnetwork failuresAvoidance of internal application failuresAvoidance of external services that failPhysical environmentNetwork redundancyTechnical solution of backupProcess solution of backupPhysical locationInfrastructure redundancyStorage architecture redundancyA book on the factors themselves was published in 2003.[24]Costs of unavailability[edit]In a 1998 report fromIBM Global Services, unavailable systems were estimated to have cost American businesses $4.54 billion in 1996, due to lost productivity and revenues.[25]See also[edit]AvailabilityFault toleranceHigh-availability clusterOverall equipment effectivenessReliability, availability and serviceabilityResponsivenessScalabilityUbiquitous computingNotes[edit]^Using365.25days per year; respectively, a quarter is a ¼ of that value (i.e.,91.3125days), and a month is a twelfth of it (i.e.,30.4375days). For consistency, all times are rounded to two decimal digits.^Seemathematical coincidences concerning base 2for details on this approximation.^"Twice as much" on a logarithmic scale, meaning twofactorsof 2:×2×2<×5{\displaystyle \times 2\times 2<\times 5}References[edit]^Robert, Sheldon (April 2024)."high availability (HA)".Techtarget.^Floyd Piedad, Michael Hawkins (2001).High Availability: Design, Techniques, and Processes. Prentice Hall.ISBN9780130962881.^"Definitions - ResiliNetsWiki".resilinets.org.^"Webarchiv ETHZ / Webarchive ETH".webarchiv.ethz.ch.^Smith, Paul; Hutchison, David; Sterbenz, James P.G.; Schöller, Marcus; Fessi, Ali; Karaliopoulos, Merkouris; Lac, Chidung; Plattner, Bernhard (July 3, 2011)."Network resilience: a systematic approach".IEEE Communications Magazine.49(7): 88–97.doi:10.1109/MCOM.2011.5936160.S2CID10246912– via IEEE Xplore.^accesstel (June 9, 2022)."operational resilience | telcos | accesstel | risk | crisis".accesstel. RetrievedMay 8,2023.^"The CERCES project - Center for Resilient Critical Infrastructures at KTH Royal Institute of Technology". Archived fromthe originalon October 19, 2018. RetrievedAugust 26,2023.^Zhao, Peiyue; Dán, György (December 3, 2018)."A Benders Decomposition Approach for Resilient Placement of Virtual Process Control Functions in Mobile Edge Clouds".IEEE Transactions on Network and Service Management.15(4): 1460–1472.doi:10.1109/TNSM.2018.2873178.S2CID56594760– via IEEE Xplore.^Castet J., Saleh J. Survivability and Resiliency of Spacecraft and Space-Based Networks: a Framework for Characterization and Analysis",American Institute of Aeronautics and Astronautics, AIAA Technical Report 2008-7707. Conference on Network Protocols (ICNP 2006), Santa Barbara, California, USA, November 2006^Lecture NotesM. Nesterenko, Kent State University^Introduction to the new mainframe: Large scale commercial computing Chapter 5 AvailabilityArchivedMarch 4, 2016, at theWayback MachineIBM (2006)^IBM zEnterprise EC12 Business Value Videoatyoutube.com^Precious metals, Volume 4. Pergamon Press. 1981. p.page 262.ISBN9780080253695.^PVD for Microelectronics: Sputter Desposition to Semiconductor Manufacturing. 1998. p.387.^Murphy, Niall Richard; Beyer, Betsy; Petoff, Jennifer; Jones, Chris (2016).Site Reliability Engineering: How Google Runs Production Systems. p.38.^Josh Deprez (April 23, 2016)."Nines of Nines". Archived fromthe originalon September 4, 2016. RetrievedMay 31,2016.^Evan L. Marcus,The myth of the nines^Newman, David; Snyder, Joel; Thayer, Rodney (June 24, 2012)."Crying Wolf: False alarms hide attacks".Network World. Vol. 19, no. 25. p. 60. RetrievedMarch 15,2019.leading to crashes and uptime numbers closer to nine fives than to five nines.^Metcalfe, Bob(April 2, 2001)."After 35 years of technology crusades, Bob Metcalfe rides off into the sunset".ITworld. RetrievedMarch 15,2019.and five nines (not nine fives) of reliability[permanent dead link]^Pilgrim, Jim (October 20, 2010)."Goodbye Five 9s". Clearfield, Inc. RetrievedMarch 15,2019.but it seems to me we are moving closer to 9-5s (55.5555555%) in network reliability rather than 5-9s^"What is network downtime?".Networking. RetrievedDecember 27,2023.^RFC992^Ulrik Franke, Pontus Johnson, Johan König, Liv Marcks von Würtemberg: Availability of enterprise IT systems – an expert-based Bayesian model,Proc. Fourth International Workshop on Software Quality and Maintainability(WSQM 2010), Madrid,[1]ArchivedAugust 4, 2012, atarchive.today^Marcus, Evan; Stern, Hal (2003).Blueprints for high availability(Second ed.). Indianapolis, IN: John Wiley & Sons.ISBN0-471-43026-9.^IBM Global Services,Improving systems availability, IBM Global Services, 1998,[2]ArchivedApril 1, 2011, at theWayback MachineExternal links[edit]Lecture Notes on Enterprise ComputingArchivedNovember 16, 2013, at theWayback MachineUniversity of TübingenLecture notes on Embedded Systems Engineeringby Prof. Phil KoopmanUptime Calculator (SLA)Retrieved from "https://en.wikipedia.org/w/index.php?title=High_availability&oldid=1242616638"Categories:System administrationQuality controlApplied probabilityReliability engineeringMeasurementHidden categories:Webarchive template wayback linksAll articles with dead external linksArticles with dead external links from July 2024Articles with permanently dead external linksWebarchive template archiveis linksArticles with short descriptionShort description is different from WikidataUse American English from March 2019All Wikipedia articles written in American EnglishUse mdy dates from March 2019Articles needing additional references from June 2008All articles needing additional referencesAll articles with unsourced statementsArticles with unsourced statements from August 2008This page was last edited on 27 August 2024, at 19:32(UTC).Text is available under theCreative Commons Attribution-ShareAlike License 4.0;
additional terms may apply. By using this site, you agree to theTerms of UseandPrivacy Policy. Wikipedia® is a registered trademark of theWikimedia Foundation, Inc., a non-profit organization.Privacy policyAbout WikipediaDisclaimersContact WikipediaCode of ConductDevelopersStatisticsCookie statementMobile view