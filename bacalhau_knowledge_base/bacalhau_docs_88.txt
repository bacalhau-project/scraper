URL: https://docs.bacalhau.org/references/jobs/job/job-types

Bacalhau Docsv.1.4.0v.1.3.0v.1.3.1v.1.3.2v.1.4.0GitHubSlackContactMoreGitHubSlackContactAsk or SearchCtrl+ KWelcomeGetting StartedHow Bacalhau WorksInstallationCreate NetworkHardware SetupContainer OnboardingDocker WorkloadsWebAssembly (Wasm) WorkloadsSetting UpRunning NodesNode OnboardingGPU InstallationJob selection policyAccess ManagementNode persistenceConnect StorageConfiguration ManagementConfiguring Transport Level SecurityLimits and TimeoutsTest Network LocallyBacalhau WebUIWorkload OnboardingContainerDocker Workload OnboardingWebAssembly (Wasm) WorkloadsBacalhau Docker ImageHow To Work With Custom Containers in BacalhauPythonBuilding and Running Custom Python ContainerRunning Pandas on BacalhauRunning a Python ScriptRunning Jupyter Notebooks on BacalhauScripting Bacalhau with PythonR (language)Building and Running your Custom R Containers on BacalhauRunning a Simple R Script on BacalhauRun CUDA programs on BacalhauRunning a Prolog ScriptReading Data from Multiple S3 Buckets using BacalhauRunning Rust programs as WebAssembly (WASM)Generate Synthetic Data using Sparkov Data Generation techniqueData IngestionCopy Data from URL to Public StoragePinning DataRunning a Job over S3 dataNetworking InstructionsAccessing the Internet from JobsUtilizing NATS.io within BacalhauGPU Workloads SetupAutomatic Update CheckingMarketplace DeploymentsGoogle Cloud MarketplaceGuidesWrite a config.yamlWrite a SpecConfigExamplesData EngineeringUsing Bacalhau with DuckDBEthereum Blockchain Analysis with Ethereum-ETL and BacalhauConvert CSV To Parquet Or AvroSimple Image ProcessingOceanography - Data ConversionVideo ProcessingModel InferenceEasyOCR (Optical Character Recognition) on BacalhauRunning Inference on Dolly 2.0 Model with Hugging FaceSpeech Recognition using WhisperStable Diffusion on a GPUStable Diffusion on a CPUObject Detection with YOLOv5 on BacalhauGenerate Realistic Images using StyleGAN3 and BacalhauStable Diffusion Checkpoint InferenceRunning Inference on a Model stored on S3Model TrainingTraining Pytorch Model with BacalhauTraining Tensorflow ModelStable Diffusion Dreambooth (Finetuning)Molecular DynamicsRunning BIDS Apps on BacalhauCoresets On BacalhauGenomics Data GenerationGromacs for AnalysisMolecular Simulation with OpenMM and BacalhauReferencesJobs GuideJob SpecificationJob TypesTask SpecificationEnginesDocker Engine SpecificationWebAssembly (WASM) Engine SpecificationPublishersIPFS Publisher SpecificationLocal Publisher SpecificationS3 Publisher SpecificationSourcesIPFS Source SpecificationLocal Source SpecificationS3 Source SpecificationURL Source SpecificationNetwork SpecificationInput Source SpecificationResources SpecificationResultPath SpecificationConstraint SpecificationLabels SpecificationMeta SpecificationJob TemplatesQueuing & TimeoutsJob QueuingTimeouts SpecificationJob ResultsStateCLI GuideSingle CLI commandsAgentAgent OverviewAgent AliveAgent NodeAgent VersionConfigConfig OverviewConfig Auto-ResourcesConfig DefaultConfig ListConfig SetJobJob OverviewJob DescribeJob ExecJob ExecutionsJob HistoryJob ListJob LogsJob RunJob StopNodeNode OverviewNode ApproveNode DeleteNode ListNode DescribeNode RejectCLI Commands OverviewCommand MigrationAPI GuideBacalhau API overviewBest PracticesAgent EndpointOrchestrator EndpointMigration APINode ManagementAuthentication & AuthorizationDatabase IntegrationDebuggingDebugging Failed JobsDebugging LocallyOpen Telemetry in BacalhauRunning locally in 'devstack'Setting up Dev EnvironmentHelp & FAQBacalhau FAQsRelease NotesGlossaryIntegrationsApache Airflow Provider for BacalhauLilypadBacalhau Python SDKObservability for WebAssembly WorkloadsCommunitySocial MediaStyle GuideWays to ContributePowered by GitBookJob TypesThe different job types available in BacalhauBacalhau has recently introduced different job types in v1.1, providing more control and flexibility over the orchestration and scheduling of those jobs - depending on their type.Despite the differences in job types, all jobs benefit from core functionalities provided by Bacalhau, including:Node selection- the appropriate nodes are selected based on several criteria, including resource availability, priority and feedback from the nodes.Job monitoring- jobs are monitored to ensure they complete, and that they stay in a healthy state.Retries- within limits, Bacalhau will retry certain jobs a set number of times should it fail to complete successfully when requested.Batch JobsBatch jobs are executed on demand, running on a specified number of Bacalhau nodes. These jobs either run until completion or until they reach a timeout. They are designed to carry out a single, discrete task before finishing. This is the onlyqueueablejob type.Ideal for intermittent yet intensive data dives, for instance performing computation over large datasets before publishing the response. This approach eliminates the continuous processing overhead, focusing on specific, in-depth investigations and computation.Batch Job ExampleThis example shows a sample Batch jobdeclarativedescription with all available parameters.The example demonstrates a job that:Has a priority of 100Will be executed on 2 nodesWill be executed only on nodes with Linux OSUses the docker engineExecutes a python script with multiple argumentsPreloads and mounts IPFS data as a local directoryPublishes the results to the IPFSHas network access type HTTP and 2 allowed domainsCopy# This example shows a sample job file.# Parameters, marked as Optional can be skipped - the default values will be used# Name of the job. Optional. Default value - job IDName:Batch Job Example# Type of the jobType:batch# The namespace in which the job is running. Default value - “default”Namespace:default# Priority - determines the scheduling priority. By default is 0Priority:100# Count - number of replicas to be scheduled.# This is only applicable for jobs of type batch and service.Count:2# Meta - arbitrary metadata associated with the job.# OptionalMeta:Job purpose:Provide detailed example of the batch jobMeta purpose:Describe the job# Labels - Arbitrary labels associated with the job for filtering purposes.# OptionalLabels:Some option:Some textSome other option:Some other text# Constraint - a condition that must be met for a compute node to be eligible to run a given job.# Should be specified in a following format: key - operator - value# Optional.Constraints:-Key:"Operating-System"Operator:"="Values:["linux"]# Task associated with the job, which defines a unit of work within the job.# Currently, only one task per job is supported.Tasks:# Name - unique identifier for a task. Default value - “main”-Name:Important Calculations# Engine - the execution engine for the task.# Defines engine type (docker or wasm) and relevant parameters.# In this example, docker engine will be used.Engine:Type:docker# Params: A set of key-value pairs that provide the specific configurations for the chosen typeParams:# Image: docker image to be used in the task.Image:alek5eyk/batchjobexample:1.1# Entrypoint defines a command that will be executed when container starts.# For this example we don't need any so default value 'null' can be usedEntrypoint:null# Parameters define CLI commands, executed after entrypointParameters:-python-supercalc.py-"5"-/outputs/result.txt# WorkingDirectory sets a working directory for entrypoint and paramters' commands.# Default value - empty string ""WorkingDirectory:""# EnvironmentVariables sets environment variables for the engineEnvironmentVariables:-DEFAULT_USER_NAME = root-API_KEY = none# Meta - arbitrary metadata associated with the task.# OptionalMeta:Task goal:show how to create declarative descriptions# Publisher specifies where the results of the task should be published - S3, IPFS, Local or none# Optional# To use IPFS publisher you need to specify only type# To use S3 publisher you need to specify bucket, key, region and endpoint# See S3 Publisher specification for more detailsPublisher:Type:ipfs# InputSources lists remote artifacts that should be downloaded before task execution# and mounted within the task# OptionalInputSources:-Target:/dataSource:Type:ipfsParams:CID:"QmSYE8dVx6RTdDFFhBu51JjFG1fwwPdUJoXZ4ZNXvfoK2V"# ResultPaths indicate volumes within the task that should be included in the published result# Only applicable for batch and ops jobs.# OptionalResultPaths:-Name:outputsPath:/outputs# Resources is a structured way to detail the required computational resources for the task.# OptionalResources:# CPU can be specified in cores (e.g. 1) or in milliCPU units (e.g. 250m or 0.25)CPU:250m# Memory highlights amount of RAM for a job. Can be specified in Kb, Mb, Gb, TbMemory:1Gb# Disk states disk storage space, needed for the task.Disk:100mb# Denotes the number of GPU units required.GPU:"0"# Network specifies networking requirements.# Optional# Job may have full access to the network,# may have no access at all,# or may have limited HTTP(S) access to a specific list of domainsNetwork:Domains:-example.com-ghcr.ioType:HTTP# Timeouts define configurations concerning any timeouts associated with the task.# OptionalTimeouts:# QueueTimeout defines how long will job wait for suitable nodes in the network# if none are currently available.QueueTimeout:101# TotalTimeout defines job execution timeout. When it is reached the job will be terminatedTotalTimeout:301Ops JobsSimilar to batch jobs, ops jobs have a broader reach. They are executed on all nodes that align with the job specification, but otherwise behave like batch jobs.Ops jobs are perfect for urgent investigations, granting direct access to logs on host machines, where previously you may have had to wait for the logs to arrive at a central location before being able to query them. They can also be used for delivering configuration files for other systems should you wish to deploy an update to many machines at once.Ops Job ExampleThis example shows a sample Ops jobdeclarativedescription with all available parameters.The example demonstrates a job that:Has a priority of 100Will be executed on all suitable nodesWill be executed only on nodes with label = WebServiceUses the docker engineExecutes a query with manually specified parametersHas access to a local directoryPublishes the results to the IPFS, if anyHas network access type HTTP and 2 allowed domainsCopy# This example shows a sample ops job file.# Parameters, marked as Optional can be skipped - the default values will be used# Example from the https://blog.bacalhau.org/p/real-time-log-analysis-with-bacalhau is used# Name of the job. Optional. Default value - job IDName:Live logs processing# Type of the jobType:ops# The namespace in which the job is running. Default value - “default”Namespace:logging# Priority - determines the scheduling priority. By default is 0Priority:100# Meta - arbitrary metadata associated with the job.# OptionalMeta:Job purpose:Provide detailed example of the ops jobMeta purpose:Describe the job# Labels - Arbitrary labels associated with the job for filtering purposes.# OptionalLabels:Job type:ops jobOps job feature:To be executed on all suitable nodes# Constraint - a condition that must be met for a compute node to be eligible to run a given job.# Should be specified in a following format: key - operator - value# Optional.Constraints:-Key:serviceOperator:==Values:-WebService# Task associated with the job, which defines a unit of work within the job.# Currently, only one task per job is supported.Tasks:# Name - unique identifier for a task. Default value - “main”-Name:LiveLogProcessing# Engine - the execution engine for the task.# Defines engine type (docker or wasm) and relevant parameters.# In this example, docker engine will be used.Engine:Type:docker# Params: A set of key-value pairs that provide the specific configurations for the chosen typeParams:# Image: docker image to be used in the task.Image:expanso/nginx-access-log-processor:1.0.0# Entrypoint defines a command that will be executed when container starts.# For this example we don't need any so default value 'null' can be usedEntrypoint:null# Parameters define CLI commands, executed after entrypointParameters:---query- {{.query}}---start-time- {{or (index . "start-time") ""}}---end-time- {{or (index . "end-time") ""}}# WorkingDirectory sets a working directory for entrypoint and paramters' commands.# Default value - empty string ""WorkingDirectory:""# EnvironmentVariables sets environment variables for the engineEnvironmentVariables:-DEFAULT_USER_NAME = root-API_KEY = none# Meta - arbitrary metadata associated with the task.# OptionalMeta:Task goal:show how to create declarative descriptions# Publisher specifies where the results of the task should be published - S3, IPFS, Local or none# Optional# To use IPFS publisher you need to specify only type# To use S3 publisher you need to specify bucket, key, region and endpoint# See S3 Publisher specification for more detailsPublisher:Type:ipfs# InputSources lists remote artifacts that should be downloaded before task execution# and mounted within the task.# Ensure that localDirectory source is enabled on the nodes# OptionalInputSources:-Target:/logsSource:Type:localDirectoryParams:SourcePath:/data/log-orchestration/logs# ResultPaths indicate volumes within the task that should be included in the published result# Only applicable for batch and ops jobs.# OptionalResultPaths:-Name:outputsPath:/outputs# Resources is a structured way to detail the required computational resources for the task.# OptionalResources:# CPU can be specified in cores (e.g. 1) or in milliCPU units (e.g. 250m or 0.25)CPU:250m# Memory highlights amount of RAM for a job. Can be specified in Kb, Mb, Gb, TbMemory:1Gb# Disk states disk storage space, needed for the task.Disk:100mb# Denotes the number of GPU units required.GPU:"0"# Network specifies networking requirements.# Optional# Job may have full access to the network,# may have no access at all,# or may have limited HTTP(S) access to a specific list of domainsNetwork:Domains:-example.com-ghcr.ioType:HTTP# Timeouts define configurations concerning any timeouts associated with the task.# OptionalTimeouts:# QueueTimeout defines how long will job wait for suitable nodes in the network# if none are currently available.QueueTimeout:101# TotalTimeout defines job execution timeout. When it is reached the job will be terminatedTotalTimeout:301Daemon JobsDaemon jobs run continuously on all nodes that meet the criteria given in the job specification. Should any new compute nodes join the cluster after the job was started, and should they meet the criteria, the job will be scheduled to run on that node too.A good application of daemon jobs is to handle continuously generated data on every compute node. This might be from edge devices like sensors, or cameras, or from logs where they are generated. The data can then be aggregated and compressed them before sending it onwards. For logs, the aggregated data can be relayed at regular intervals to platforms like Kafka or Kinesis, or directly to other logging services with edge devices potentially delivering results via MQTT.Daemon Job ExampleThis example shows a sample Daemon jobdeclarativedescription with all available parameters.The example demonstrates a job that:Has a priority of 100Will be executed continuously on all suitable nodesWill be executed only on nodes with label = WebServiceUses the docker engineExecutes a query with manually specified parametersHas access to 2 local directories with logsPublishes the results to the IPFS, if anyHas network access type Full in order to send data to the S3 storageCopy# This example shows a sample daemon job file.# Parameters, marked as Optional can be skipped - the default values will be used# Example from the https://blog.bacalhau.org/p/tutorial-save-25-m-yearly-by-managing is used# Name of the job. Optional. Default value - job IDName:Logstash# Type of the jobType:daemon# The namespace in which the job is running. Default value - “default”Namespace:logging# Priority - determines the scheduling priority. By default is 0Priority:100# Meta - arbitrary metadata associated with the job.# OptionalMeta:Job purpose:Provide detailed example of the daemon jobMeta purpose:Describe the job# Labels - Arbitrary labels associated with the job for filtering purposes.# OptionalLabels:Job type:daemon jobDaemon job feature:To be executed continuously on all suitable nodes# Constraint - a condition that must be met for a compute node to be eligible to run a given job.# Should be specified in a following format: key - operator - value# Optional.Constraints:-Key:serviceOperator:==Values:-WebService# Task associated with the job, which defines a unit of work within the job.# Currently, only one task per job is supported.Tasks:# Name - unique identifier for a task. Default value - “main”-Name:main# Engine - the execution engine for the task.# Defines engine type (docker or wasm) and relevant parameters.# In this example, docker engine will be used.Engine:Type:docker# Params: A set of key-value pairs that provide the specific configurations for the chosen typeParams:# Image: docker image to be used in the task.Image:expanso/nginx-access-log-agent:1.0.0# Entrypoint defines a command that will be executed when container starts.# For this example we don't need any so default value 'null' can be usedEntrypoint:null# Parameters define CLI commands, executed after entrypointParameters:---query- {{.query}}---start-time- {{or (index . "start-time") ""}}---end-time- {{or (index . "end-time") ""}}# WorkingDirectory sets a working directory for entrypoint and paramters' commands.# Default value - empty string ""WorkingDirectory:""# EnvironmentVariables sets environment variables for the engineEnvironmentVariables:-OPENSEARCH_ENDPOINT={{.OpenSearchEndpoint}}-S3_BUCKET={{.AccessLogBucket}}-AWS_REGION={{.AWSRegion}}-AGGREGATE_DURATION=10-S3_TIME_FILE=60# Meta - arbitrary metadata associated with the task.# OptionalMeta:Task goal:show how to create declarative descriptions# Publisher specifies where the results of the task should be published - S3, IPFS, Local or none# Optional# To use IPFS publisher you need to specify only type# To use S3 publisher you need to specify bucket, key, region and endpoint# See S3 Publisher specification for more detailsPublisher:Type:ipfs# InputSources lists remote artifacts that should be downloaded before task execution# and mounted within the task.# Ensure that localDirectory source is enabled on the nodes# OptionalInputSources:-Target:/app/logsSource:Type:localDirectoryParams:SourcePath:/data/log-orchestration/logs-Target:/app/stateSource:Type:localDirectoryParams:SourcePath:/data/log-orchestration/stateReadWrite:true# ResultPaths indicate volumes within the task that should be included in the published result# Only applicable for batch and ops jobs.# OptionalResultPaths:-Name:outputsPath:/outputs# Resources is a structured way to detail the required computational resources for the task.# OptionalResources:# CPU can be specified in cores (e.g. 1) or in milliCPU units (e.g. 250m or 0.25)CPU:250m# Memory highlights amount of RAM for a job. Can be specified in Kb, Mb, Gb, TbMemory:1Gb# Disk states disk storage space, needed for the task.Disk:100mb# Denotes the number of GPU units required.GPU:"0"# Network specifies networking requirements.# Optional# Job may have full access to the network,# may have no access at all,# or may have limited HTTP(S) access to a specific list of domainsNetwork:Type:Full# Timeouts define configurations concerning any timeouts associated with the task.# OptionalTimeouts:# QueueTimeout defines how long will job wait for suitable nodes in the network# if none are currently available.QueueTimeout:101# TotalTimeout defines job execution timeout. When it is reached the job will be terminatedTotalTimeout:301Service JobsService jobs run continuously on a specified number of nodes that meet the criteria given in the job specification. Bacalhau's orchestrator selects the optimal nodes to run the job, and continuously monitors its health, performance. If required, it will reschedule on other nodes.This job type is good for long-running consumers such as streaming or queuing services, or real-time event listeners.Service Job ExampleThis example shows a sample Service jobdeclarativedescription with all available parameters.The example demonstrates a job that:Has a priority of 100Will be executed continuously on all suitable nodesWill be executed only on nodes with architecture = arm64 and located in the us-west-2 regionUses the docker engineExecutes a query with multiple parametersHas access to 2 local directories with logsPublishes the results to the IPFS, if anyHas network access type Full in order to send data to the S3 storageCopy# This example shows a sample daemon job file.# Parameters, marked as Optional can be skipped - the default values will be used# Example from the https://blog.bacalhau.org/p/introducing-new-job-types-new-horizons is used# Name of the job. Optional. Default value - job IDName:Kinesis Consumer# Type of the jobType:service# The namespace in which the job is running. Default value - “default”Namespace:service# Priority - determines the scheduling priority. By default is 0Priority:100# Meta - arbitrary metadata associated with the job.# OptionalMeta:Job purpose:Provide detailed example of the service jobMeta purpose:Describe the job# Labels - Arbitrary labels associated with the job for filtering purposes.# OptionalLabels:Job type:service jobDaemon job feature:To be executed continuously on a certain amount of suitable nodes# Constraint - a condition that must be met for a compute node to be eligible to run a given job.# Should be specified in a following format: key - operator - value# Optional.Constraints:-Key:ArchitectureOperator:'='Values:-arm64-Key:regionOperator:'='Values:-us-west-2# Task associated with the job, which defines a unit of work within the job.# Currently, only one task per job is supported.Tasks:# Name - unique identifier for a task. Default value - “main”-Name:main# Engine - the execution engine for the task.# Defines engine type (docker or wasm) and relevant parameters.# In this example, docker engine will be used.Engine:Type:docker# Params: A set of key-value pairs that provide the specific configurations for the chosen typeParams:# Image: docker image to be used in the task.Image:my-kinesis-consumer:latest# Entrypoint defines a command that will be executed when container starts.# For this example we don't need any so default value 'null' can be usedEntrypoint:null# Parameters define CLI commands, executed after entrypointParameters:--stream-arn-arn:aws:kinesis:us-west-2:123456789012:stream/my-kinesis-stream--shard-iterator-TRIM_HORIZON# WorkingDirectory sets a working directory for entrypoint and paramters' commands.# Default value - empty string ""WorkingDirectory:""# EnvironmentVariables sets environment variables for the engineEnvironmentVariables:-DEFAULT_USER_NAME = root-API_KEY = none# Meta - arbitrary metadata associated with the task.# OptionalMeta:Task goal:show how to create declarative descriptions# Publisher specifies where the results of the task should be published - S3, IPFS, Local or none# Optional# To use IPFS publisher you need to specify only type# To use S3 publisher you need to specify bucket, key, region and endpoint# See S3 Publisher specification for more detailsPublisher:Type:ipfs# InputSources lists remote artifacts that should be downloaded before task execution# and mounted within the task.# Ensure that localDirectory source is enabled on the nodes# OptionalInputSources:-Target:/app/logsSource:Type:localDirectoryParams:SourcePath:/data/log-orchestration/logs-Target:/app/stateSource:Type:localDirectoryParams:SourcePath:/data/log-orchestration/stateReadWrite:true# ResultPaths indicate volumes within the task that should be included in the published result# Only applicable for batch and ops jobs.# OptionalResultPaths:-Name:outputsPath:/outputs# Resources is a structured way to detail the required computational resources for the task.# OptionalResources:# CPU can be specified in cores (e.g. 1) or in milliCPU units (e.g. 250m or 0.25)CPU:250m# Memory highlights amount of RAM for a job. Can be specified in Kb, Mb, Gb, TbMemory:4Gb# Disk states disk storage space, needed for the task.Disk:100mb# Denotes the number of GPU units required.GPU:"0"# Network specifies networking requirements.# Optional# Job may have full access to the network,# may have no access at all,# or may have limited HTTP(S) access to a specific list of domainsNetwork:Type:Full# Timeouts define configurations concerning any timeouts associated with the task.# OptionalTimeouts:# QueueTimeout defines how long will job wait for suitable nodes in the network# if none are currently available.QueueTimeout:101# TotalTimeout defines job execution timeout. When it is reached the job will be terminatedTotalTimeout:301PreviousJob SpecificationNextTask SpecificationLast updated22 days agoOn this pageBatch JobsOps JobsDaemon JobsService JobsWas this helpful?Edit on GitHubExport as PDFGet SupportExpansoSupportUse CasesDistributed ETLEdge MLDistributed Data WarehousingFlett ManagementAbout UsWho we areWhat we valueNews & BlogBlogNewsExpanso (2024). All Rights Reserved.