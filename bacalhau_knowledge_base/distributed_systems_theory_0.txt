URL: https://www.the-paper-trail.org/post/2014-08-09-distributed-systems-theory-for-the-distributed-systems-engineer/

Paper TrailBlogDistributed systems theory for the distributed systems engineerAugust 09, 2014Updated June 2018 with content on atomic broadcast, gossip, chain replication and moreGwen Shapira, who at the time was an engineer at Cloudera and now is spreading the Kafka gospel, asked a question on Twitter that got me thinking.I need to improve my proficiency in distributed systems theory. Where do I start? Any recommended books?— Gwen (Chen) Shapira (@gwenshap)August 7, 2014My response of old might have been “well, here’s the FLP paper, and here’s the Paxos paper, and here’s the Byzantine generals paper…”, and I’d have prescribed a laundry list of primary source material which would have taken at least six months to get through if you rushed. But I’ve come to thinking that recommending a ton of theoretical papers is often precisely the wrong way to go about learning distributed systems theory (unless you are in a PhD program). Papers are usually deep, usually complex, and require both serious study, and usuallysignificant experienceto glean their important contributions and to place them in context. What good is requiring that level of expertise of engineers?And yet, unfortunately, there’s a paucity of good ‘bridge’ material that summarises, distills and contextualises the important results and ideas in distributed systems theory; particularly material that does so without condescending. Considering that gap lead me to another interesting question:What distributed systems theory should a distributed systems engineer know?A little theory is, in this case, not such a dangerous thing. So I tried to come up with a list of what I consider the basic concepts that are applicable to my every-day job as a distributed systems engineer. Let me know what you think I missed!First stepsThese four readings do a pretty good job of explaining what about building distributed systems is challenging. Collectively they outline a set of abstract but technical difficulties that the distributed systems engineer has to overcome, and set the stage for the more detailed investigation in later sectionsDistributed Systems for Fun and Profitis a short book which tries to cover some of the basic issues in distributed systems including the role of time and different strategies for replication.Notes on distributed systems for young bloods- not theory, but a good practical counterbalance to keep the rest of your reading grounded.A Note on Distributed Systems- a classic paper on why you can’t just pretend all remote interactions are like local objects.The fallacies of distributed computing- 8 fallacies of distributed computing that set the stage for the kinds of things system designers forget.You should know aboutsafety and liveness properties:safetyproperties say that nothing bad will ever happen. For example, the property
of never returning an inconsistent value is a safety property, as is never electing
two leaders at the same time.livenessproperties say that something good will eventually happen. For example,
saying that a system will eventually return a result to every API call is a liveness
property, as is guaranteeing that a write to disk always eventually completes.Failure and TimeMany difficulties that the distributed systems engineer faces can be blamed on two underlying causes:Processes mayfailThere isno good way to tell that they have done soThere is a very deep relationship between what, if anything, processes share about their knowledge oftime, what failure scenarios are possible to detect, and what algorithms and primitives may be correctly implemented. Most of the time, we assume that two different nodes have absolutely no shared knowledge of what time it is, or how quickly time passes.You should know:The (partial) hierarchy of failure modes:crash stop -> omission->Byzantine. You should understand that what is possible at the top of the hierarchy must be possible at lower levels, and what is impossible at lower levels must be impossible at higher levels.How you decide whether an event happened before another event in the absence of any shared clock. This meansLamport clocksand their generalisation toVector clocks, but also see theDynamo paper.How big an impact the possibility of even a single failure can actually have on our ability to implement correct distributed systems (see my notes on the FLP result below).Different models of time:synchronous, partially synchronous and asynchronousThat detecting failures is a fundamental problem, one that trades offaccuracyandcompleteness- yet another safety vs liveness conflict. The paper that really set out failure detection as a theoretical problem isChandra and Toueg’s ‘Unreliable Failure Detectors for Reliable Distributed Systems’. But there are several shorter summaries around - I quite likethis random one from Stanford.The basic tension of fault toleranceA system that tolerates some faults without degrading must be able to act as though those faults had not occurred. This means usually that parts of the system must do work redundantly, but doing more work than is absolutely necessary typically carries a cost both in performance and resource consumption. This is the basic tension of adding fault tolerance to a system.You should know:The quorum technique for ensuring single-copy serialisability. SeeSkeen’s original paper, but perhaps better isWikipedia’s entry.About2-phase-commit,3-phase-commitandPaxos, and why they have different fault-tolerance properties.How eventual consistency, and other techniques, seek to avoid this tension at the cost of weaker guarantees about system behaviour. TheDynamo paperis a great place to start, but also Pat Helland’s classicLife Beyond Transactionsis a must-read.Basic primitivesThere are few agreed-upon basic building blocks in distributed systems, but more are beginning to emerge. You should know what the following problems are, and where to find a solution for them:Leader election (e.g. theBully algorithm)Consistent snapshotting (e.g.this classic paperfrom Chandy and Lamport)Consensus (see the blog posts on 2PC and Paxos above)Distributed state machine replication (Wikipediais ok,Lampson’s paperis canonical but dry).Broadcast - delivering messages to more than one node at onceAtomic broadcast- can you deliver a message to all nodes in a group, or none?Gossip (the classic paper)Causal multicast(but also consider the enjoyableback-and-forthbetween Birman and Cheriton).Chain replication (a neat way of ensuring consistency and ordering of writes by organizing nodes into a virtual linked list).Theoriginal paperA set of improvementsfor read-mostly workloadsAn experiential reportby@slfritchieFundamental ResultsSome facts just need to be internalised. There are more than this, naturally, but here’s a flavour:You can’t implement consistent storage and respond to all requests if you might drop
messages between processes. This is theCAP theorem.Consensus is impossible to implement in such a way that it both a) is always correct and
b) always terminates if even one machine might fail in an asynchronous system with
crash-* stop failures (the FLP result). The first slides - before the proof gets going -
of myPapers We Love SF talkdo a
reasonable job of explaining the result, I hope. _Suggestion: there’s no real need to
understand the proof_.Consensus is impossible to solve in fewer than 2 rounds of messages in general.Atomic broadcast is exactly as hard as consensus - in a precise sense, if you solve
atomic broadcast, you solve consensus, and vice
versa.Chandra and Touegprove this, but you just need to know that it’s true.Real systemsThe most important exercise to repeat is to read descriptions of new, real systems, and to critique their design decisions. Do this over and over again. Some suggestions:Google:GFSSpannerF1ChubbyBigTableMillWheelOmegaDapperPaxos Made LiveThe Tail At ScaleNot Google:DryadCassandraCephRAMCloudHyperDexPNUTSAzure Data Lake StorePostscriptIf you tame all the concepts and techniques on this list, I’dlike to talk to youabout engineering positions working with the menagerie of distributed systems we curate atClouderaSlack.©  - 2023 ·Paper Trail·
  ThemeSimplenessPowered byHugo·