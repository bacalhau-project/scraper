URL: https://docs.bacalhau.org/setting-up/data-ingestion/from-url

Bacalhau Docsv.1.4.0v.1.3.0v.1.3.1v.1.3.2v.1.4.0GitHubSlackContactMoreGitHubSlackContactAsk or SearchCtrl+ KWelcomeGetting StartedHow Bacalhau WorksInstallationCreate NetworkHardware SetupContainer OnboardingDocker WorkloadsWebAssembly (Wasm) WorkloadsSetting UpRunning NodesNode OnboardingGPU InstallationJob selection policyAccess ManagementNode persistenceConnect StorageConfiguration ManagementConfiguring Transport Level SecurityLimits and TimeoutsTest Network LocallyBacalhau WebUIWorkload OnboardingContainerDocker Workload OnboardingWebAssembly (Wasm) WorkloadsBacalhau Docker ImageHow To Work With Custom Containers in BacalhauPythonBuilding and Running Custom Python ContainerRunning Pandas on BacalhauRunning a Python ScriptRunning Jupyter Notebooks on BacalhauScripting Bacalhau with PythonR (language)Building and Running your Custom R Containers on BacalhauRunning a Simple R Script on BacalhauRun CUDA programs on BacalhauRunning a Prolog ScriptReading Data from Multiple S3 Buckets using BacalhauRunning Rust programs as WebAssembly (WASM)Generate Synthetic Data using Sparkov Data Generation techniqueData IngestionCopy Data from URL to Public StoragePinning DataRunning a Job over S3 dataNetworking InstructionsAccessing the Internet from JobsUtilizing NATS.io within BacalhauGPU Workloads SetupAutomatic Update CheckingMarketplace DeploymentsGoogle Cloud MarketplaceGuidesWrite a config.yamlWrite a SpecConfigExamplesData EngineeringUsing Bacalhau with DuckDBEthereum Blockchain Analysis with Ethereum-ETL and BacalhauConvert CSV To Parquet Or AvroSimple Image ProcessingOceanography - Data ConversionVideo ProcessingModel InferenceEasyOCR (Optical Character Recognition) on BacalhauRunning Inference on Dolly 2.0 Model with Hugging FaceSpeech Recognition using WhisperStable Diffusion on a GPUStable Diffusion on a CPUObject Detection with YOLOv5 on BacalhauGenerate Realistic Images using StyleGAN3 and BacalhauStable Diffusion Checkpoint InferenceRunning Inference on a Model stored on S3Model TrainingTraining Pytorch Model with BacalhauTraining Tensorflow ModelStable Diffusion Dreambooth (Finetuning)Molecular DynamicsRunning BIDS Apps on BacalhauCoresets On BacalhauGenomics Data GenerationGromacs for AnalysisMolecular Simulation with OpenMM and BacalhauReferencesJobs GuideJob SpecificationJob TypesTask SpecificationEnginesDocker Engine SpecificationWebAssembly (WASM) Engine SpecificationPublishersIPFS Publisher SpecificationLocal Publisher SpecificationS3 Publisher SpecificationSourcesIPFS Source SpecificationLocal Source SpecificationS3 Source SpecificationURL Source SpecificationNetwork SpecificationInput Source SpecificationResources SpecificationResultPath SpecificationConstraint SpecificationLabels SpecificationMeta SpecificationJob TemplatesQueuing & TimeoutsJob QueuingTimeouts SpecificationJob ResultsStateCLI GuideSingle CLI commandsAgentAgent OverviewAgent AliveAgent NodeAgent VersionConfigConfig OverviewConfig Auto-ResourcesConfig DefaultConfig ListConfig SetJobJob OverviewJob DescribeJob ExecJob ExecutionsJob HistoryJob ListJob LogsJob RunJob StopNodeNode OverviewNode ApproveNode DeleteNode ListNode DescribeNode RejectCLI Commands OverviewCommand MigrationAPI GuideBacalhau API overviewBest PracticesAgent EndpointOrchestrator EndpointMigration APINode ManagementAuthentication & AuthorizationDatabase IntegrationDebuggingDebugging Failed JobsDebugging LocallyOpen Telemetry in BacalhauRunning locally in 'devstack'Setting up Dev EnvironmentHelp & FAQBacalhau FAQsRelease NotesGlossaryIntegrationsApache Airflow Provider for BacalhauLilypadBacalhau Python SDKObservability for WebAssembly WorkloadsCommunitySocial MediaStyle GuideWays to ContributePowered by GitBookCopy Data from URL to Public StorageTo upload a file from a URL we will use thebacalhau docker runcommand.Copybacalhaudockerrun\--id-only\--wait\--inputhttps://raw.githubusercontent.com/filecoin-project/bacalhau/main/README.md\ghcr.io/bacalhau-project/examples/upload:v1The job has been submitted and Bacalhau has printed out the related job id.Structure of the command​Let's look closely at the command above:bacalhau docker run: call to bacalhau using docker executor--input https://raw.githubusercontent.com/filecoin-project/bacalhau/main/README.md: URL path of the input data volumes downloaded from a URL source.ghcr.io/bacalhau-project/examples/upload:v1: the name and tag of the docker image we are usingThebacalhau docker runcommand takes advantage of the--inputparameter. This will download a file from a public URL and place it in the/inputsdirectory of the container (by default). Then we will use a helper container to move that data to the /outputs directory.You can find out more about thehelper container in the examples repositorywhich is designed to simplify the data uploading process.For more details, see theCLI commands guideChecking the State of Your Jobs​Job status: You can check the status of the job usingbacalhau job list, processing the json ouput with thejq:Copybacalhaujoblist$JOB_ID--output=json|jq'.[0].Status.JobState.Nodes[] | .Shards."0" | select(.RunOutput)'When the job status isPublishedorCompleted, that means the job is done, and we can get the results using the job ID.Job information: You can find out more information about your job by usingbacalhau job describe.Copybacalhaujobdescribe$JOB_IDJob download: You can download your job results directly by usingbacalhau job get. Alternatively, you can choose to create a directory to store your results. In the command below, we removed a directory in case it was present before, created it and downloaded our job output to be stored in that directory.Copyrm-rfresults&&mkdir./resultsbacalhaujobget--output-dir./results$JOB_IDViewing your Job Output​Each job result contains anoutputssubfolder andexitCode,stderrandstdoutfiles with relevant content. To view the execution logs execute following:Copyhead-n15./results/stdoutAnd to view the job execution result (README.mdfile in the example case), which was saved as a job output, execute:Copytail./results/outputs/README.mdGet the CID From the Completed Job​To get the output CID from a completed job, run the following command:Copybacalhau job list $JOB_ID --output=json | jq -r '.[0].Status.JobState.Nodes[] | .Shards."0".PublishedResults | select(.CID) | .CID'The job will upload the CID to the public storage via IPFS. We will store the CID in an environment variable so that we can reuse it later on.Use the CID in a New Bacalhau Job​Now that we have the CID, we can use it in a new job. This time we will use the--inputparameter to tell Bacalhau to use the CID we just uploaded.In this case, the only goal of our job is just to list the contents of the/inputsdirectory. You can see that the "input" data is located under/inputs/outputs/README.md.Copybacalhaudockerrun\--id-only\--wait\--inputipfs://$CID \ubuntu--\bash-c"set -x; ls -l /inputs; ls -l /inputs/outputs; cat /inputs/outputs/README.md"The job has been submitted and Bacalhau has printed out the related job id. We store that in an environment variable so that we can reuse it later on.Need Support?​For questions and feedback, please reach out in ourSlackPreviousData IngestionNextPinning DataLast updated2 months agoOn this pageChecking the State of Your Jobs​Viewing your Job Output​Get the CID From the Completed Job​Use the CID in a New Bacalhau Job​Need Support?​Was this helpful?Edit on GitHubExport as PDFGet SupportExpansoSupportUse CasesDistributed ETLEdge MLDistributed Data WarehousingFlett ManagementAbout UsWho we areWhat we valueNews & BlogBlogNewsExpanso (2024). All Rights Reserved.