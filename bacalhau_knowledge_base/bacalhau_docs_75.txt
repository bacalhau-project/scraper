URL: https://docs.bacalhau.org/examples/model-inference/running-inference-on-a-model-stored-on-s3

Bacalhau Docsv.1.4.0v.1.3.0v.1.3.1v.1.3.2v.1.4.0GitHubSlackContactMoreGitHubSlackContactAsk or SearchCtrl+ KWelcomeGetting StartedHow Bacalhau WorksInstallationCreate NetworkHardware SetupContainer OnboardingDocker WorkloadsWebAssembly (Wasm) WorkloadsSetting UpRunning NodesNode OnboardingGPU InstallationJob selection policyAccess ManagementNode persistenceConnect StorageConfiguration ManagementConfiguring Transport Level SecurityLimits and TimeoutsTest Network LocallyBacalhau WebUIWorkload OnboardingContainerDocker Workload OnboardingWebAssembly (Wasm) WorkloadsBacalhau Docker ImageHow To Work With Custom Containers in BacalhauPythonBuilding and Running Custom Python ContainerRunning Pandas on BacalhauRunning a Python ScriptRunning Jupyter Notebooks on BacalhauScripting Bacalhau with PythonR (language)Building and Running your Custom R Containers on BacalhauRunning a Simple R Script on BacalhauRun CUDA programs on BacalhauRunning a Prolog ScriptReading Data from Multiple S3 Buckets using BacalhauRunning Rust programs as WebAssembly (WASM)Generate Synthetic Data using Sparkov Data Generation techniqueData IngestionCopy Data from URL to Public StoragePinning DataRunning a Job over S3 dataNetworking InstructionsAccessing the Internet from JobsUtilizing NATS.io within BacalhauGPU Workloads SetupAutomatic Update CheckingMarketplace DeploymentsGoogle Cloud MarketplaceGuidesWrite a config.yamlWrite a SpecConfigExamplesData EngineeringUsing Bacalhau with DuckDBEthereum Blockchain Analysis with Ethereum-ETL and BacalhauConvert CSV To Parquet Or AvroSimple Image ProcessingOceanography - Data ConversionVideo ProcessingModel InferenceEasyOCR (Optical Character Recognition) on BacalhauRunning Inference on Dolly 2.0 Model with Hugging FaceSpeech Recognition using WhisperStable Diffusion on a GPUStable Diffusion on a CPUObject Detection with YOLOv5 on BacalhauGenerate Realistic Images using StyleGAN3 and BacalhauStable Diffusion Checkpoint InferenceRunning Inference on a Model stored on S3Model TrainingTraining Pytorch Model with BacalhauTraining Tensorflow ModelStable Diffusion Dreambooth (Finetuning)Molecular DynamicsRunning BIDS Apps on BacalhauCoresets On BacalhauGenomics Data GenerationGromacs for AnalysisMolecular Simulation with OpenMM and BacalhauReferencesJobs GuideJob SpecificationJob TypesTask SpecificationEnginesDocker Engine SpecificationWebAssembly (WASM) Engine SpecificationPublishersIPFS Publisher SpecificationLocal Publisher SpecificationS3 Publisher SpecificationSourcesIPFS Source SpecificationLocal Source SpecificationS3 Source SpecificationURL Source SpecificationNetwork SpecificationInput Source SpecificationResources SpecificationResultPath SpecificationConstraint SpecificationLabels SpecificationMeta SpecificationJob TemplatesQueuing & TimeoutsJob QueuingTimeouts SpecificationJob ResultsStateCLI GuideSingle CLI commandsAgentAgent OverviewAgent AliveAgent NodeAgent VersionConfigConfig OverviewConfig Auto-ResourcesConfig DefaultConfig ListConfig SetJobJob OverviewJob DescribeJob ExecJob ExecutionsJob HistoryJob ListJob LogsJob RunJob StopNodeNode OverviewNode ApproveNode DeleteNode ListNode DescribeNode RejectCLI Commands OverviewCommand MigrationAPI GuideBacalhau API overviewBest PracticesAgent EndpointOrchestrator EndpointMigration APINode ManagementAuthentication & AuthorizationDatabase IntegrationDebuggingDebugging Failed JobsDebugging LocallyOpen Telemetry in BacalhauRunning locally in 'devstack'Setting up Dev EnvironmentHelp & FAQBacalhau FAQsRelease NotesGlossaryIntegrationsApache Airflow Provider for BacalhauLilypadBacalhau Python SDKObservability for WebAssembly WorkloadsCommunitySocial MediaStyle GuideWays to ContributePowered by GitBookRunning Inference on a Model stored on S3Introduction​In this example, we will demonstrate how to run inference on a model stored on Amazon S3. We will use a PyTorch model trained on the MNIST dataset.Running Locally​Prerequisites​Consider using the latest versions or use the docker method listed below in the article.PythonPyTorchDownloading the Datasets​Use the following commands to download the model and test image:Copywget https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/model/pytorch-training-2020-11-21-22-02-56-203/model.tar.gzwgethttps://raw.githubusercontent.com/js-ts/mnist-test/main/digit.pngCreating the Inference Script​This script is designed to load a pretrained PyTorch model for MNIST digit classification from atar.gzfile, extract it, and use the model to perform inference on a given input image. Ensure you have all required dependencies installed:CopypipinstallPillowtorchtorchvisionCopy# content of the inference.py fileimporttorchimporttorchvision.transformsastransformsfromPILimportImagefromtorch.autogradimportVariableimportargparseimporttarfileclassCustomModel(torch.nn.Module):def__init__(self):super(CustomModel, self).__init__()self.conv1=torch.nn.Conv2d(1,10,5)self.conv2=torch.nn.Conv2d(10,20,5)self.fc1=torch.nn.Linear(320,50)self.fc2=torch.nn.Linear(50,10)defforward(self,x):x=torch.relu(self.conv1(x))x=torch.max_pool2d(x,2)x=torch.relu(self.conv2(x))x=torch.max_pool2d(x,2)x=torch.flatten(x,1)x=torch.relu(self.fc1(x))x=self.fc2(x)output=torch.log_softmax(x, dim=1)returnoutputdefextract_tar_gz(file_path,output_dir):withtarfile.open(file_path,'r:gz')astar:tar.extractall(path=output_dir)# Parse command-line argumentsparser=argparse.ArgumentParser()parser.add_argument('--tar_gz_file_path', type=str, required=True, help='Path to the tar.gz file')parser.add_argument('--output_directory', type=str, required=True, help='Output directory to extract the tar.gz file')parser.add_argument('--image_path', type=str, required=True, help='Path to the input image file')args=parser.parse_args()# Extract the tar.gz filetar_gz_file_path=args.tar_gz_file_pathoutput_directory=args.output_directoryextract_tar_gz(tar_gz_file_path, output_directory)# Load the modelmodel_path=f"{output_directory}/model.pth"model=CustomModel()model.load_state_dict(torch.load(model_path, map_location=torch.device("cpu")))model.eval()# Transformations for the MNIST datasettransform=transforms.Compose([transforms.Resize((28,28)),transforms.Grayscale(num_output_channels=1),transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,)),])# Function to run inference on an imagedefrun_inference(image,model):image_tensor=transform(image).unsqueeze(0)# Apply transformations and add batch dimensioninput=Variable(image_tensor)# Perform inferenceoutput=model(input)_,predicted=torch.max(output.data,1)returnpredicted.item()# Example usageimage_path=args.image_pathimage=Image.open(image_path)predicted_class=run_inference(image, model)print(f"Predicted class:{predicted_class}")Running the Inference Script​To use this script, you need to provide the paths to thetar.gzfile containing the pre-trained model, the output directory where the model will be extracted, and the input image file for which you want to perform inference. The script will output the predicted digit (class) for the given input image.Copypython3inference.py--tar_gz_file_path./model.tar.gz--output_directory./model--image_path./digit.pngRunning Inference on Bacalhau​Prerequisite​To get started, you need to install the Bacalhau client, see more informationhereStructure of the Command​export JOB_ID=$( ... ): Export results of a command execution as environment variable-w /inputsSet the current working directory at/inputsin the container-i src=s3://sagemaker-sample-files/datasets/image/MNIST/model/pytorch-training-2020-11-21-22-02-56-203/model.tar.gz,dst=/model/,opt=region=us-east-1: Mount the s3 bucket at the destination path provided -/model/and specifying the region where the bucket is locatedopt=region=us-east-1-i git://github.com/js-ts/mnist-test.git: Flag to mount the source code repo from GitHub. It would mount the repo at/inputs/js-ts/mnist-testin this case it also contains the test imagepytorch/pytorch: The name of the Docker image-- python3 /inputs/js-ts/mnist-test/inference.py --tar_gz_file_path /model/model.tar.gz --output_directory /model-pth --image_path /inputs/js-ts/mnist-test/image.png: The command to run inference on the model. It consists of:/model/model.tar.gzis the path to the model file/model-pthis the output directory for the model/inputs/js-ts/mnist-test/image.pngis the path to the input imageCopyexportJOB_ID=$(bacalhaudockerrun\--wait \--id-only \--timeout3600\--wait-timeout-secs3600\-w/inputs\-i src=s3://sagemaker-sample-files/datasets/image/MNIST/model/pytorch-training-2020-11-21-22-02-56-203/model.tar.gz,dst=/model/,opt=region=us-east-1 \-igit://github.com/js-ts/mnist-test.git\pytorch/pytorch \-- python3 /inputs/js-ts/mnist-test/inference.py --tar_gz_file_path /model/model.tar.gz --output_directory /model-pth --image_path /inputs/js-ts/mnist-test/image.png)When the job is submitted Bacalhau prints out the related job id. We store that in an environment variableJOB_IDso that we can reuse it later on.Viewing the Output​Use thebacalhau job logscommand to view the job output, since the script prints the result of execution to the stdout:Copybacalhaujoblogs${JOB_ID}Predictedclass:0You can also usebacalhau job getto download job results:Copybacalhaujobget${JOB_ID}SupportIf you have questions or need support or guidance, please reach out to theBacalhau team via Slack(#generalchannel).PreviousStable Diffusion Checkpoint InferenceNextModel TrainingLast updated1 month agoOn this pageIntroduction​Running Locally​Prerequisites​Downloading the Datasets​Creating the Inference Script​Running the Inference Script​Running Inference on Bacalhau​Prerequisite​Structure of the Command​Viewing the Output​SupportWas this helpful?Edit on GitHubExport as PDFGet SupportExpansoSupportUse CasesDistributed ETLEdge MLDistributed Data WarehousingFlett ManagementAbout UsWho we areWhat we valueNews & BlogBlogNewsExpanso (2024). All Rights Reserved.