URL: https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/MemoryStorage.html

Wiki|git|Apache Hadoop| Last Published: 2024-03-04
               | Version: 3.4.0GeneralOverviewSingle Node SetupCluster SetupCommands ReferenceFileSystem ShellCompatibility SpecificationDownstream Developer's GuideAdmin Compatibility GuideInterface ClassificationFileSystem SpecificationCommonCLI Mini ClusterFair Call QueueNative LibrariesProxy UserRack AwarenessSecure ModeService Level AuthorizationHTTP AuthenticationCredential Provider APIHadoop KMSTracingUnix Shell GuideRegistryAsync ProfilerHDFSArchitectureUser GuideCommands ReferenceNameNode HA With QJMNameNode HA With NFSObserver NameNodeFederationViewFsViewFsOverloadSchemeSnapshotsEdits ViewerImage ViewerPermissions and HDFSQuotas and HDFSlibhdfs (C API)WebHDFS (REST API)HttpFSShort Circuit Local ReadsCentralized Cache ManagementNFS GatewayRolling UpgradeExtended AttributesTransparent EncryptionMultihomingStorage PoliciesMemory Storage SupportSynthetic Load GeneratorErasure CodingDisk BalancerUpgrade DomainDataNode AdminRouter FederationProvided StorageMapReduceTutorialCommands ReferenceCompatibility with 1.xEncrypted ShufflePluggable Shuffle/SortDistributed Cache DeploySupport for YARN Shared CacheMapReduce REST APIsMR Application MasterMR History ServerYARNArchitectureCommands ReferenceCapacity SchedulerFair SchedulerResourceManager RestartResourceManager HAResource ModelNode LabelsNode AttributesWeb Application ProxyTimeline ServerTimeline Service V.2Writing YARN ApplicationsYARN Application SecurityNodeManagerRunning Applications in Docker ContainersRunning Applications in runC ContainersUsing CGroupsSecure ContainersReservation SystemGraceful DecommissionOpportunistic ContainersYARN FederationShared CacheUsing GPUUsing FPGAPlacement ConstraintsYARN UI2YARN REST APIsIntroductionResource ManagerNode ManagerTimeline ServerTimeline Service V.2YARN ServiceOverviewQuickStartConceptsYarn Service APIService DiscoverySystem ServicesHadoop Compatible File SystemsAliyun OSSAmazon S3Azure Blob StorageAzure Data Lake StorageTencent COSHuaweicloud OBSAuthOverviewExamplesConfigurationBuildingToolsHadoop StreamingHadoop ArchivesHadoop Archive LogsDistCpHDFS Federation BalanceGridMixRumenResource Estimator ServiceScheduler Load SimulatorHadoop BenchmarkingDynamometerReferenceChangelog and Release NotesJava API docsUnix Shell APIMetricsConfigurationcore-default.xmlhdfs-default.xmlhdfs-rbf-default.xmlmapred-default.xmlyarn-default.xmlkms-default.xmlhttpfs-default.xmlDeprecated PropertiesMemory Storage Support in HDFSIntroductionAdministrator ConfigurationLimit RAM used for replicas in MemorySetup RAM Disks on Data NodesChoosing tmpfs (vs ramfs)Mount RAM DisksTag tmpfs volume with the RAM_DISK Storage TypeEnsure Storage Policies are enabledApplication UsageUse the LAZY_PERSIST Storage PolicyInvoke hdfs storagepolicies command for directoriesCall setStoragePolicy method for directoriesPass LAZY_PERSIST CreateFlag for new filesIntroductionHDFS supports writing to off-heap memory managed by the Data Nodes. The Data Nodes will flush in-memory data to disk asynchronously thus removing expensive disk IO and checksum computations from the performance-sensitive IO path, hence we call such writesLazy Persistwrites. HDFS provides best-effort persistence guarantees for Lazy Persist Writes. Rare data loss is possible in the event of a node restart before replicas are persisted to disk. Applications can choose to use Lazy Persist Writes to trade off some durability guarantees in favor of reduced latency.This feature is available starting with Apache Hadoop 2.6.0 and was developed under JiraHDFS-6581.The target use cases are applications that would benefit from writing relatively low amounts of data (from a few GB up to tens of GBs depending on available memory) with low latency. Memory storage is for applications that run within the cluster and collocated with HDFS Data Nodes. We have observed that the latency overhead from network replication negates the benefits of writing to memory.Applications that use Lazy Persist Writes will continue to work by falling back to DISK storage if memory is insufficient or unconfigured.Administrator ConfigurationThis section enumerates the administrative steps required before applications can start using the feature in a cluster.Limit RAM used for replicas in MemoryFirst decide the amount of memory to be dedicated for replicas stored in memory. Setdfs.datanode.max.locked.memoryaccordingly inhdfs-site.xml. This is the same setting used by theCentralized Cache Managementfeature. The Data Node will ensure that the combined memory used by Lazy Persist Writes and Centralized Cache Management does not exceed the amount configured indfs.datanode.max.locked.memory.E.g. To reserve 32 GB for in-memory replicas<property>
      <name>dfs.datanode.max.locked.memory</name>
      <value>34359738368</value>
    </property>This memory is not allocated by the Data Node on startup.On Unix-like systems, the “locked-in-memory size” ulimit (ulimit -l) of the Data Node user also needs to be increased to match this parameter (see the related section onOS Limits). When setting this value, please remember that you will need space in memory for other things as well, such as the Data Node and application JVM heaps and the operating system page cache. You will also need memory for YARN containers if there is a YARN Node Manager process running on the same node as the Data Node.Setup RAM Disks on Data NodesInitialize a RAM disk on each Data Node. The choice of RAM Disk allows better data persistence across Data Node process restarts. The following setup will work on most Linux distributions. Using RAM disks on other platforms is not currently supported.Choosingtmpfs(vsramfs)Linux supports using two kinds of RAM disks -tmpfsandramfs. The size oftmpfsis limited by the Linux kernel whileramfsgrows to fill all available system memory. There is a downside totmpfssince its contents can be swapped to disk under memory pressure. However many performance-sensitive deployments run with swapping disabled so we do not expect this to be an issue in practice.HDFS currently supports usingtmpfspartitions. Support for addingramfsis in progress (SeeHDFS-8584).Mount RAM DisksMount the RAM Disk partition with the Unixmountcommand. E.g. to mount a 32 GBtmpfspartition under/mnt/dn-tmpfs/sudo mount -t tmpfs -o size=32g tmpfs /mnt/dn-tmpfs/It is recommended you create an entry in the/etc/fstabso the RAM Disk is recreated automatically on node restarts. Another option is to use a sub-directory under/dev/shmwhich is atmpfsmount available by default on most Linux distributions. Ensure that the size of the mount is greater than or equal to yourdfs.datanode.max.locked.memorysetting else override it in/etc/fstab. Using more than onetmpfspartition per Data Node for Lazy Persist Writes is not recommended.Tagtmpfsvolume with the RAM_DISK Storage TypeTag thetmpfsdirectory with the RAM_DISK storage type via thedfs.datanode.data.dirconfiguration setting inhdfs-site.xml. E.g. On a Data Node with three hard disk volumes/grid/0,/grid/1and/grid/2and atmpfsmount/mnt/dn-tmpfs,dfs.datanode.data.dirmust be set as follows:<property>
      <name>dfs.datanode.data.dir</name>
      <value>/grid/0,/grid/1,/grid/2,[RAM_DISK]/mnt/dn-tmpfs</value>
    </property>This step is crucial. Without the RAM_DISK tag, HDFS will treat thetmpfsvolume as non-volatile storage and data will not be saved to persistent storage. You will lose data on node restart.Ensure Storage Policies are enabledEnsure that the global setting to turn on Storage Policies is enabledas documented here. This setting is on by default.Application UsageUse the LAZY_PERSIST Storage PolicyApplications indicate that HDFS can use Lazy Persist Writes for a file with theLAZY_PERSISTstorage policy. Administrative privileges arenotrequired to set the policy and it can be set in one of three ways.Invokehdfs storagepoliciescommand for directoriesSetting the policy on a directory causes it to take effect for all new files in the directory. Thehdfs storagepoliciescommand can be used to set the policy as described in theStorage Policies documentation.hdfs storagepolicies -setStoragePolicy -path <path> -policy LAZY_PERSISTCallsetStoragePolicymethod for directoriesStarting with Apache Hadoop 2.8.0, an application can programmatically set the Storage Policy withFileSystem.setStoragePolicy. E.g.fs.setStoragePolicy(path, "LAZY_PERSIST");PassLAZY_PERSISTCreateFlagfor new filesAn application can passCreateFlag#LAZY_PERSISTwhen creating a new file withFileSystem#createAPI. E.g.FSDataOutputStream fos =
        fs.create(
            path,
            FsPermission.getFileDefault(),
            EnumSet.of(CreateFlag.CREATE, CreateFlag.LAZY_PERSIST),
            bufferLength,
            replicationFactor,
            blockSize,
            null);©            2008-2024
              Apache Software Foundation
            
                          -Privacy Policy.
        Apache Maven, Maven, Apache, the Apache feather logo, and the Apache Maven project logos are trademarks of The Apache Software Foundation.