URL: https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDataNodeAdminGuide.html

Wiki|git|Apache Hadoop| Last Published: 2024-03-04
               | Version: 3.4.0GeneralOverviewSingle Node SetupCluster SetupCommands ReferenceFileSystem ShellCompatibility SpecificationDownstream Developer's GuideAdmin Compatibility GuideInterface ClassificationFileSystem SpecificationCommonCLI Mini ClusterFair Call QueueNative LibrariesProxy UserRack AwarenessSecure ModeService Level AuthorizationHTTP AuthenticationCredential Provider APIHadoop KMSTracingUnix Shell GuideRegistryAsync ProfilerHDFSArchitectureUser GuideCommands ReferenceNameNode HA With QJMNameNode HA With NFSObserver NameNodeFederationViewFsViewFsOverloadSchemeSnapshotsEdits ViewerImage ViewerPermissions and HDFSQuotas and HDFSlibhdfs (C API)WebHDFS (REST API)HttpFSShort Circuit Local ReadsCentralized Cache ManagementNFS GatewayRolling UpgradeExtended AttributesTransparent EncryptionMultihomingStorage PoliciesMemory Storage SupportSynthetic Load GeneratorErasure CodingDisk BalancerUpgrade DomainDataNode AdminRouter FederationProvided StorageMapReduceTutorialCommands ReferenceCompatibility with 1.xEncrypted ShufflePluggable Shuffle/SortDistributed Cache DeploySupport for YARN Shared CacheMapReduce REST APIsMR Application MasterMR History ServerYARNArchitectureCommands ReferenceCapacity SchedulerFair SchedulerResourceManager RestartResourceManager HAResource ModelNode LabelsNode AttributesWeb Application ProxyTimeline ServerTimeline Service V.2Writing YARN ApplicationsYARN Application SecurityNodeManagerRunning Applications in Docker ContainersRunning Applications in runC ContainersUsing CGroupsSecure ContainersReservation SystemGraceful DecommissionOpportunistic ContainersYARN FederationShared CacheUsing GPUUsing FPGAPlacement ConstraintsYARN UI2YARN REST APIsIntroductionResource ManagerNode ManagerTimeline ServerTimeline Service V.2YARN ServiceOverviewQuickStartConceptsYarn Service APIService DiscoverySystem ServicesHadoop Compatible File SystemsAliyun OSSAmazon S3Azure Blob StorageAzure Data Lake StorageTencent COSHuaweicloud OBSAuthOverviewExamplesConfigurationBuildingToolsHadoop StreamingHadoop ArchivesHadoop Archive LogsDistCpHDFS Federation BalanceGridMixRumenResource Estimator ServiceScheduler Load SimulatorHadoop BenchmarkingDynamometerReferenceChangelog and Release NotesJava API docsUnix Shell APIMetricsConfigurationcore-default.xmlhdfs-default.xmlhdfs-rbf-default.xmlmapred-default.xmlyarn-default.xmlkms-default.xmlhttpfs-default.xmlDeprecated PropertiesHDFS DataNode Admin GuideOverviewHost-level settingsHostname-only configurationJSON-based configurationCluster-level settingsBacking-off Decommission Monitor (experimental)MetricsOverviewThe Hadoop Distributed File System (HDFS) namenode maintains states of all datanodes. There are two types of states. The fist type describes the liveness of a datanode indicating if the node is live, dead or stale. The second type describes the admin state indicating if the node is in service, decommissioned or under maintenance.When an administrator decommission a datanode, the datanode will first be transitioned intoDECOMMISSION_INPROGRESSstate. After all blocks belonging to that datanode have been fully replicated elsewhere based on each block’s replication factor. the datanode will be transitioned toDECOMMISSIONEDstate. After that, the administrator can shutdown the node to perform long-term repair and maintenance that could take days or weeks. After the machine has been repaired, the machine can be recommissioned back to the cluster.Sometimes administrators only need to take datanodes down for minutes/hours to perform short-term repair/maintenance. In such scenario, the HDFS block replication overhead incurred by decommission might not be necessary and a light-weight process is desirable. And that is what maintenance state is used for. When an administrator put a datanode in maintenance state, the datanode will first be transitioned toENTERING_MAINTENANCEstate. As long as all blocks belonging to that datanode is minimally replicated elsewhere, the datanode will immediately be transitioned toIN_MAINTENANCEstate. After the maintenance has completed, the administrator can take the datanode out of the maintenance state. In addition, maintenance state supports timeout that allows administrators to config the maximum duration in which a datanode is allowed to stay in maintenance state. After the timeout, the datanode will be transitioned out of maintenance state automatically by HDFS without human intervention.In summary, datanode admin operations include the followings:DecommissionRecommissionPutting nodes in maintenance stateTaking nodes out of maintenance stateAnd datanode admin states include the followings:NORMALThe node is in service.DECOMMISSIONEDThe node has been decommissioned.DECOMMISSION_INPROGRESSThe node is being transitioned to DECOMMISSIONED state.IN_MAINTENANCEThe node is in maintenance state.ENTERING_MAINTENANCEThe node is being transitioned to maintenance state.Host-level settingsTo perform any of datanode admin operations, there are two steps.Update host-level configuration files to indicate the desired admin states of targeted datanodes. There are two supported formats for configuration files.Hostname-only configuration. Each line includes the hostname/ip address for a datanode. That is the default format.JSON-based configuration. The configuration is in JSON format. Each element maps to one datanode and each datanode can have multiple properties. This format is required to put datanodes to maintenance states.Run the following command to have namenode reload the host-level configuration files.hdfs dfsadmin [-refreshNodes]Hostname-only configurationThis is the default configuration used by the namenode. It only supports node decommission and recommission; it doesn’t support admin operations related to maintenance state. Usedfs.hostsanddfs.hosts.excludeas explained inhdfs-default.xml.In the following example,host1andhost2need to be in service.host3andhost4need to be in decommissioned state.dfs.hosts filehost1
host2
host3
host4dfs.hosts.exclude filehost3
host4JSON-based configurationJSON-based format is the new configuration format that supports generic properties on datanodes. Set the following configurations to enable JSON-based format as explained inhdfs-default.xml.SettingValuedfs.namenode.hosts.provider.classnameorg.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManagerdfs.hoststhe path of the json hosts fileHere is the list of currently supported properties by HDFS.PropertyDescriptionhostNameRequired. The host name of the datanode.upgradeDomainOptional. The upgrade domain id of the datanode.adminStateOptional. The expected admin state. The default value isNORMAL;DECOMMISSIONEDfor decommission;IN_MAINTENANCEfor maintenance state.portOptional. the port number of the datanodemaintenanceExpireTimeInMSOptional. The epoch time in milliseconds until which the datanode will remain in maintenance state. The default value is forever.In the following example,host1andhost2need to be in service.host3needs to be in decommissioned state.host4needs to be in maintenance state.dfs.hosts file[
  {
    "hostName": "host1"
  },
  {
    "hostName": "host2",
    "upgradeDomain": "ud0"
  },
  {
    "hostName": "host3",
    "adminState": "DECOMMISSIONED"
  },
  {
    "hostName": "host4",
    "upgradeDomain": "ud2",
    "adminState": "IN_MAINTENANCE"
  }
]Cluster-level settingsThere are several cluster-level settings related to datanode administration. For common use cases, you should rely on the default values. Please refer tohdfs-default.xmlfor descriptions and default values.dfs.namenode.maintenance.replication.min
dfs.namenode.decommission.interval
dfs.namenode.decommission.blocks.per.interval
dfs.namenode.decommission.max.concurrent.tracked.nodesBacking-off Decommission Monitor (experimental)The original decommissioning algorithm has issues when DataNodes having lots of blocks are decommissioned such asWrite lock in the NameNode could be held for a long time for queueing re-replication.Re-replication work progresses node by node if there are multiple decommissioning DataNodes.HDFS-14854introduced new decommission monitor in order to mitigate those issues. This feature is currently marked as experimental and disabled by default. You can enable this by setting the value ofdfs.namenode.decommission.monitor.classtoorg.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminBackoffMonitorin hdfs-site.xml.The relevant configuration properties are listed in the table below. Please refer tohdfs-default.xmlfor descriptions and default values.Propertydfs.namenode.decommission.monitor.classdfs.namenode.decommission.backoff.monitor.pending.limitdfs.namenode.decommission.backoff.monitor.pending.blocks.per.lockMetricsAdmin states are part of the namenode’s webUI and JMX. As explained inHDFSCommands.html, you can also verify admin states using the following commands.Usedfsadminto check admin states at the cluster level.hdfs dfsadmin -reportUsefsckto check admin states of datanodes storing data at a specific path. For backward compatibility, a special flag is required to return maintenance states.hdfs fsck <path> // only show decommission state
hdfs fsck <path> -maintenance // include maintenance state©            2008-2024
              Apache Software Foundation
            
                          -Privacy Policy.
        Apache Maven, Maven, Apache, the Apache feather logo, and the Apache Maven project logos are trademarks of The Apache Software Foundation.