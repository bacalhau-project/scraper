URL: https://docs.bacalhau.org/examples/model-inference/stable-diffusion-checkpoint-inference

Bacalhau Docsv.1.4.0v.1.3.0v.1.3.1v.1.3.2v.1.4.0GitHubSlackContactMoreGitHubSlackContactAsk or SearchCtrl+ KWelcomeGetting StartedHow Bacalhau WorksInstallationCreate NetworkHardware SetupContainer OnboardingDocker WorkloadsWebAssembly (Wasm) WorkloadsSetting UpRunning NodesNode OnboardingGPU InstallationJob selection policyAccess ManagementNode persistenceConnect StorageConfiguration ManagementConfiguring Transport Level SecurityLimits and TimeoutsTest Network LocallyBacalhau WebUIWorkload OnboardingContainerDocker Workload OnboardingWebAssembly (Wasm) WorkloadsBacalhau Docker ImageHow To Work With Custom Containers in BacalhauPythonBuilding and Running Custom Python ContainerRunning Pandas on BacalhauRunning a Python ScriptRunning Jupyter Notebooks on BacalhauScripting Bacalhau with PythonR (language)Building and Running your Custom R Containers on BacalhauRunning a Simple R Script on BacalhauRun CUDA programs on BacalhauRunning a Prolog ScriptReading Data from Multiple S3 Buckets using BacalhauRunning Rust programs as WebAssembly (WASM)Generate Synthetic Data using Sparkov Data Generation techniqueData IngestionCopy Data from URL to Public StoragePinning DataRunning a Job over S3 dataNetworking InstructionsAccessing the Internet from JobsUtilizing NATS.io within BacalhauGPU Workloads SetupAutomatic Update CheckingMarketplace DeploymentsGoogle Cloud MarketplaceGuidesWrite a config.yamlWrite a SpecConfigExamplesData EngineeringUsing Bacalhau with DuckDBEthereum Blockchain Analysis with Ethereum-ETL and BacalhauConvert CSV To Parquet Or AvroSimple Image ProcessingOceanography - Data ConversionVideo ProcessingModel InferenceEasyOCR (Optical Character Recognition) on BacalhauRunning Inference on Dolly 2.0 Model with Hugging FaceSpeech Recognition using WhisperStable Diffusion on a GPUStable Diffusion on a CPUObject Detection with YOLOv5 on BacalhauGenerate Realistic Images using StyleGAN3 and BacalhauStable Diffusion Checkpoint InferenceRunning Inference on a Model stored on S3Model TrainingTraining Pytorch Model with BacalhauTraining Tensorflow ModelStable Diffusion Dreambooth (Finetuning)Molecular DynamicsRunning BIDS Apps on BacalhauCoresets On BacalhauGenomics Data GenerationGromacs for AnalysisMolecular Simulation with OpenMM and BacalhauReferencesJobs GuideJob SpecificationJob TypesTask SpecificationEnginesDocker Engine SpecificationWebAssembly (WASM) Engine SpecificationPublishersIPFS Publisher SpecificationLocal Publisher SpecificationS3 Publisher SpecificationSourcesIPFS Source SpecificationLocal Source SpecificationS3 Source SpecificationURL Source SpecificationNetwork SpecificationInput Source SpecificationResources SpecificationResultPath SpecificationConstraint SpecificationLabels SpecificationMeta SpecificationJob TemplatesQueuing & TimeoutsJob QueuingTimeouts SpecificationJob ResultsStateCLI GuideSingle CLI commandsAgentAgent OverviewAgent AliveAgent NodeAgent VersionConfigConfig OverviewConfig Auto-ResourcesConfig DefaultConfig ListConfig SetJobJob OverviewJob DescribeJob ExecJob ExecutionsJob HistoryJob ListJob LogsJob RunJob StopNodeNode OverviewNode ApproveNode DeleteNode ListNode DescribeNode RejectCLI Commands OverviewCommand MigrationAPI GuideBacalhau API overviewBest PracticesAgent EndpointOrchestrator EndpointMigration APINode ManagementAuthentication & AuthorizationDatabase IntegrationDebuggingDebugging Failed JobsDebugging LocallyOpen Telemetry in BacalhauRunning locally in 'devstack'Setting up Dev EnvironmentHelp & FAQBacalhau FAQsRelease NotesGlossaryIntegrationsApache Airflow Provider for BacalhauLilypadBacalhau Python SDKObservability for WebAssembly WorkloadsCommunitySocial MediaStyle GuideWays to ContributePowered by GitBookStable Diffusion Checkpoint InferenceIntroduction​Stable Diffusionis a state of the art text-to-image model that generates images from text and was developed as an open-source alternative toDALL·E 2. It is based on aDiffusion Probabilistic Modeland uses aTransformerto generate images from text.This example demonstrates how to use stable diffusion using a finetuned model and run inference on it. The first section describes the development of the code and the container - it is optional as users don't need to build their own containers to use their own custom model. The second section demonstrates how to convert your model weights to ckpt. The third section demonstrates how to run the job using Bacalhau.The following guide is using the fine-tuned model, which was finetuned on Bacalhau. To learn how to finetune your own stable diffusion model refer tothis guide.TL;DR​Convert your existing model weights to theckptformat and upload to the IPFS storage.Create a job usingbacalhau docker run, relevant docker image, model weights and any prompt.Download results usingbacalhau job getand the job id.Prerequisite​To get started, you need to install:Bacalhau client, see more informationhereNVIDIA GPUCUDA driversNVIDIA dockerRunning Locally​Containerize your Script using Docker​This part of the guide is optional - you can skip it and proceed to theRunning a Bacalhau jobif you are not going to use your own custom image.To build your own docker container, create aDockerfile, which contains instructions to containerize the code for inference.CopyFROMpytorch/pytorch:1.13.0-cuda11.6-cudnn8-runtimeWORKDIR/RUNapt update &&  apt install -y gitRUNgit clone https://github.com/runwayml/stable-diffusion.gitWORKDIR/stable-diffusionRUNconda env create -f environment.yamlSHELL["conda","run","-n","ldm","/bin/bash","-c"]RUNpip install opencv-pythonRUNapt updateRUNapt-get install ffmpeg libsm6 libxext6 libxrender-dev  -yThis container is using thepytorch/pytorch:1.13.0-cuda11.6-cudnn8-runtimeimage and the working directory is set. Next the Dockerfile installs required dependencies. Then we add our custom code and pull the dependent repositories.See more information on how to containerize your script/apphereBuild the container​We will rundocker buildcommand to build the container.Copydocker build -t <hub-user>/<repo-name>:<tag> .Before running the command replace:hub-userwith your docker hub username, If you don’t have a docker hub accountfollow these instructions to create the Docker account, and use the username of the account you createdrepo-namewith the name of the container, you can name it anything you wanttagthis is not required but you can use thelatesttagSo in our case, the command will look like this:Copydockerbuild-tjsacex/stable-diffusion-ckptPush the container​Next, upload the image to the registry. This can be done by using the Docker hub username, repo name or tag.Copydockerpush<hub-user>/<repo-name>:<tag>Thus, in this case, the command would look this way:Copydockerpushjsacex/stable-diffusion-ckptAfter the repo image has been pushed to Docker Hub, you can now use the container for running on Bacalhau. But before that you need to check whether your model is ackptfile or not. If your model is ackptfile you can skip to the running on Bacalhau, and if not - the next section describes how to convert your model into theckptformat.Converting model weights to CKPT​To download the convert script:Copywget-qhttps://github.com/TheLastBen/diffusers/raw/main/scripts/convert_diffusers_to_original_stable_diffusion.pyTo convert the model weights intockptformat, the--halfflag cuts the size of the output model from 4GB to 2GB:Copypython3convert_diffusers_to_original_stable_diffusion.py\--model_path<path-to-the-model-weights>\--checkpoint_path<path-to-save-the-checkpoint>/model.ckpt\--halfRunning a Bacalhau Job​To do inference on your own checkpoint on Bacalhau you need to first upload it to your public storage, which can be mounted anywhere on your machine. In this case, we will be usingNFT.Storage(Recommended Option). To upload your dataset usingNFTupdrag and drop your directory and it will upload it to IPFS.After the checkpoint file has been uploaded copy its CID.Some of the jobs presented in the Examples section may require more resources than are currently available on the demo network. Considerstarting your own networkor running less resource-intensive jobs on the demo networkStructure of the command​Let's look closely at the command above:export JOB_ID=$( ... ): Export results of a command execution as environment variableThe--gpu 1flag is set to specify hardware requirements, a GPU is needed to run such a job-i ipfs://QmUCJuFZ2v7KvjBGHRP2K1TMPFce3reTkKVGF2BJY5bXdZ:/model.ckpt: Path to mount the checkpoint-- conda run --no-capture-output -n ldm: since we are using conda we need to specify the name of the environment which we are going to use, in this case it isldmscripts/txt2img.py: running the python script--prompt "a photo of a person drinking coffee": the prompt you need to specify the session name in the prompt.--plms: the sampler you want to use. In this case we will use theplmssampler--ckpt ../model.ckpt: here we specify the path to our checkpoint--n_samples 1: no of samples we want to produce--skip_grid: skip creating a grid of images--outdir ../outputs: path to store the outputs--seed $RANDOM: The output generated on the same prompt will always be the same for different outputs on the same prompt set the seed parameter to randomWhen a job is submitted, Bacalhau prints out the relatedjob_id. We store that in an environment variable so that we can reuse it later on.CopyexportJOB_ID=$(bacalhaudockerrun\--gpu1\--timeout3600\--wait-timeout-secs3600\--wait \--id-only \-iipfs://QmUCJuFZ2v7KvjBGHRP2K1TMPFce3reTkKVGF2BJY5bXdZ:/model.ckpt\jsacex/stable-diffusion-ckpt \-- conda run --no-capture-output -n ldm python scripts/txt2img.py --prompt "a photo of a person drinking coffee" --plms --ckpt ../model.ckpt --skip_grid --n_samples 1 --skip_grid --outdir ../outputs)Checking the State of your Jobs​Job status: You can check the status of the job usingbacalhau job list:Copybacalhaujoblist--id-filter${JOB_ID}When it saysCompleted, that means the job is done, and we can get the results.Job information: You can find out more information about your job by usingbacalhau job describe:Copybacalhaujobdescribe${JOB_ID}Job download: You can download your job results directly by usingbacalhau job get. Alternatively, you can choose to create a directory to store your results. In the command below, we created a directory and downloaded our job output to be stored in that directory.Copyrm-rfresults&&mkdirresultsbacalhaujobget${JOB_ID}--output-dirresultsViewing your Job Output​After the download has finished we can see the results in theresults/outputsfolder. We received following image for our prompt:PreviousGenerate Realistic Images using StyleGAN3 and BacalhauNextRunning Inference on a Model stored on S3Last updated1 month agoOn this pageIntroduction​TL;DR​Prerequisite​Running Locally​Containerize your Script using Docker​Build the container​Push the container​Converting model weights to CKPT​Running a Bacalhau Job​Structure of the command​Checking the State of your Jobs​Viewing your Job Output​Was this helpful?Edit on GitHubExport as PDFGet SupportExpansoSupportUse CasesDistributed ETLEdge MLDistributed Data WarehousingFlett ManagementAbout UsWho we areWhat we valueNews & BlogBlogNewsExpanso (2024). All Rights Reserved.The photo generated with Stable Diffusion on the basis of Checkpoint Inference with Bacalhau.