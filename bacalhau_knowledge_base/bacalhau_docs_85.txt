URL: https://docs.bacalhau.org/examples/molecular-dynamics/molecular-simulation-with-openmm-and-bacalhau

Bacalhau Docsv.1.4.0v.1.3.0v.1.3.1v.1.3.2v.1.4.0GitHubSlackContactMoreGitHubSlackContactAsk or SearchCtrl+ KWelcomeGetting StartedHow Bacalhau WorksInstallationCreate NetworkHardware SetupContainer OnboardingDocker WorkloadsWebAssembly (Wasm) WorkloadsSetting UpRunning NodesNode OnboardingGPU InstallationJob selection policyAccess ManagementNode persistenceConnect StorageConfiguration ManagementConfiguring Transport Level SecurityLimits and TimeoutsTest Network LocallyBacalhau WebUIWorkload OnboardingContainerDocker Workload OnboardingWebAssembly (Wasm) WorkloadsBacalhau Docker ImageHow To Work With Custom Containers in BacalhauPythonBuilding and Running Custom Python ContainerRunning Pandas on BacalhauRunning a Python ScriptRunning Jupyter Notebooks on BacalhauScripting Bacalhau with PythonR (language)Building and Running your Custom R Containers on BacalhauRunning a Simple R Script on BacalhauRun CUDA programs on BacalhauRunning a Prolog ScriptReading Data from Multiple S3 Buckets using BacalhauRunning Rust programs as WebAssembly (WASM)Generate Synthetic Data using Sparkov Data Generation techniqueData IngestionCopy Data from URL to Public StoragePinning DataRunning a Job over S3 dataNetworking InstructionsAccessing the Internet from JobsUtilizing NATS.io within BacalhauGPU Workloads SetupAutomatic Update CheckingMarketplace DeploymentsGoogle Cloud MarketplaceGuidesWrite a config.yamlWrite a SpecConfigExamplesData EngineeringUsing Bacalhau with DuckDBEthereum Blockchain Analysis with Ethereum-ETL and BacalhauConvert CSV To Parquet Or AvroSimple Image ProcessingOceanography - Data ConversionVideo ProcessingModel InferenceEasyOCR (Optical Character Recognition) on BacalhauRunning Inference on Dolly 2.0 Model with Hugging FaceSpeech Recognition using WhisperStable Diffusion on a GPUStable Diffusion on a CPUObject Detection with YOLOv5 on BacalhauGenerate Realistic Images using StyleGAN3 and BacalhauStable Diffusion Checkpoint InferenceRunning Inference on a Model stored on S3Model TrainingTraining Pytorch Model with BacalhauTraining Tensorflow ModelStable Diffusion Dreambooth (Finetuning)Molecular DynamicsRunning BIDS Apps on BacalhauCoresets On BacalhauGenomics Data GenerationGromacs for AnalysisMolecular Simulation with OpenMM and BacalhauReferencesJobs GuideJob SpecificationJob TypesTask SpecificationEnginesDocker Engine SpecificationWebAssembly (WASM) Engine SpecificationPublishersIPFS Publisher SpecificationLocal Publisher SpecificationS3 Publisher SpecificationSourcesIPFS Source SpecificationLocal Source SpecificationS3 Source SpecificationURL Source SpecificationNetwork SpecificationInput Source SpecificationResources SpecificationResultPath SpecificationConstraint SpecificationLabels SpecificationMeta SpecificationJob TemplatesQueuing & TimeoutsJob QueuingTimeouts SpecificationJob ResultsStateCLI GuideSingle CLI commandsAgentAgent OverviewAgent AliveAgent NodeAgent VersionConfigConfig OverviewConfig Auto-ResourcesConfig DefaultConfig ListConfig SetJobJob OverviewJob DescribeJob ExecJob ExecutionsJob HistoryJob ListJob LogsJob RunJob StopNodeNode OverviewNode ApproveNode DeleteNode ListNode DescribeNode RejectCLI Commands OverviewCommand MigrationAPI GuideBacalhau API overviewBest PracticesAgent EndpointOrchestrator EndpointMigration APINode ManagementAuthentication & AuthorizationDatabase IntegrationDebuggingDebugging Failed JobsDebugging LocallyOpen Telemetry in BacalhauRunning locally in 'devstack'Setting up Dev EnvironmentHelp & FAQBacalhau FAQsRelease NotesGlossaryIntegrationsApache Airflow Provider for BacalhauLilypadBacalhau Python SDKObservability for WebAssembly WorkloadsCommunitySocial MediaStyle GuideWays to ContributePowered by GitBookMolecular Simulation with OpenMM and BacalhauIntroductionIn this tutorial example, we will showcase how to containerize an OpenMM workload so that it can be executed on the Bacalhau network and take advantage of the distributed storage & compute resources.OpenMMis a toolkit for molecular simulation. It is a physic-based library that is useful for refining the structure and exploring functional interactions with other molecules. It provides a combination of extreme flexibility (through custom forces and integrators), openness, and high performance (especially on recent GPUs) that make it truly unique among simulation codes.In this example tutorial, our focus will be on running OpenMM molecular simulation with Bacalhau.Prerequisite​To get started, you need to install the Bacalhau client, see more informationhereRunning Locally​Downloading Datasets​We use a processed 2DRI dataset that represents the ribose binding protein in bacterial transport and chemotaxis. The source organism is theEscherichia colibacteria.Protein data can be stored in a.pdbfile, this is a human-readable format. It provides for the description and annotation of protein and nucleic acid structures including atomic coordinates, secondary structure assignments, as well as atomic connectivity. See more information about PDB formathere. For the original, unprocessed 2DRI dataset, you can download it from the RCSB Protein Data Bankhere.The relevant code of the processed 2DRI dataset can be foundhere. Let's print the first 10 lines of the2dri-processed.pdbfile. The output contains a number of ATOM records. These describe the coordinates of the atoms that are part of the protein.Copyhead./dataset/2dri-processed.pdbExpectedOutputREMARK1CREATEDWITHOPENMM7.6,2022-07-12CRYST181.30981.30981.30990.0090.0090.00P11ATOM1NLYSA164.7319.46159.4301.000.00NATOM2CALYSA163.58810.28658.9271.000.00CATOM3HALYSA162.7079.48659.0381.000.00HATOM4CLYSA163.79010.67157.4681.000.00CATOM5OLYSA164.88711.08957.0781.000.00OATOM6CBLYSA163.45811.56759.7491.000.00CATOM7HB2LYSA163.33312.36658.8791.000.00HATOM8HB3LYSA164.43511.86760.3721.000.00HWriting the Script​To run the script above all we need is a Python environment with theOpenMM libraryinstalled. This script makes sure that there are no empty cells and to filter out potential error sources from the file.Copy# Import the packagesimportosfromopenmmimport*fromopenmm.appimport*fromopenmm.unitimport*# Specify the input filesinput_path='inputs/2dri-processed.pdb'ifnotos.path.exists(input_path):raiseFileNotFoundError(f"Input file not found:{input_path}")# Function to check and filter PDB file linesdeffilter_valid_pdb_lines(input_path,output_path):withopen(input_path,'r')asinfile,open(output_path,'w')asoutfile:lines=infile.readlines()fori,lineinenumerate(lines):ifline.startswith("ATOM")orline.startswith("HETATM"):iflen(line)>=54:try:float(line[30:38].strip())float(line[38:46].strip())float(line[46:54].strip())outfile.write(line)exceptValueError:print(f"Skipping line {i + 1} because it has invalid coordinates: {line.strip()}")else:print(f"Skipping line {i + 1} because it is too short: {line.strip()}")else:outfile.write(line)# Filter PDB filefiltered_pdb_path='inputs/filtered_2dri-processed.pdb'filter_valid_pdb_lines(input_path, filtered_pdb_path)# Load the filtered PDB filetry:pdb=PDBFile(filtered_pdb_path)exceptValueErrorase:print(f"ValueError while reading filtered PDB file:{e}")raiseforcefield=ForceField('amber14-all.xml','amber14/tip3pfb.xml')# Outputoutput_path='outputs/final_state.pdbx'ifnotos.path.exists(os.path.dirname(output_path)):os.makedirs(os.path.dirname(output_path))# System ConfigurationnonbondedMethod=PMEnonbondedCutoff=1.0*nanometersewaldErrorTolerance=0.0005constraints=HBondsrigidWater=TrueconstraintTolerance=0.000001hydrogenMass=1.5*amu# Integration Optionsdt=0.002*picosecondstemperature=310*kelvinfriction=1.0/picosecondpressure=1.0*atmospheresbarostatInterval=25# Simulation Optionssteps=10equilibrationSteps=0# platform = Platform.getPlatformByName('CUDA')platform=Platform.getPlatformByName('CPU')# platformProperties = {'Precision': 'single'}platformProperties={}dcdReporter=DCDReporter('trajectory.dcd',1000)dataReporter=StateDataReporter('log.txt',1000, totalSteps=steps,step=True, time=True, speed=True, progress=True, elapsedTime=True, remainingTime=True,potentialEnergy=True, kineticEnergy=True, totalEnergy=True, temperature=True,volume=True, density=True, separator='\t')checkpointReporter=CheckpointReporter('checkpoint.chk',1000)# Prepare the Simulationprint('Building system...')topology=pdb.topologypositions=pdb.positionssystem=forcefield.createSystem(topology, nonbondedMethod=nonbondedMethod, nonbondedCutoff=nonbondedCutoff,constraints=constraints, rigidWater=rigidWater, ewaldErrorTolerance=ewaldErrorTolerance,hydrogenMass=hydrogenMass)system.addForce(MonteCarloBarostat(pressure, temperature, barostatInterval))integrator=LangevinMiddleIntegrator(temperature, friction, dt)integrator.setConstraintTolerance(constraintTolerance)simulation=Simulation(topology, system, integrator, platform, platformProperties)simulation.context.setPositions(positions)# Minimize and Equilibrateprint('Performing energy minimization...')simulation.minimizeEnergy()print('Equilibrating...')simulation.context.setVelocitiesToTemperature(temperature)simulation.step(equilibrationSteps)# Simulateprint('Simulating...')simulation.reporters.append(dcdReporter)simulation.reporters.append(dataReporter)simulation.reporters.append(checkpointReporter)simulation.currentStep=0simulation.step(steps)# Write a file with the final simulation statestate=simulation.context.getState(getPositions=True, enforcePeriodicBox=system.usesPeriodicBoundaryConditions())withopen(output_path, mode="w+")asfile:PDBxFile.writeFile(simulation.topology, state.getPositions(), file)print('Simulation complete, file written to disk at:{}'.format(output_path))Running the Script​Copypythonrun_openmm_simulation.pyThis is only done to check whether your Python script is running. If there are no errors occurring, proceed further.Uploading the Data to IPFS​The simplest way to upload the data to IPFS is to use a third-party service to "pin" data to the IPFS network, to ensure that the data exists and is available. To do this, you need an account with a pinning service likePinataornft.storage. Once registered, you can use their UI or API or SDKs to upload files.When you pin your data, you'll get a CID. Copy the CID as it will be used to access your dataContainerize Script using Docker​To build your own docker container, create aDockerfile, which contains instructions to build your image.See more information on how to containerize your script/apphereCopyFROMconda/miniconda3RUNconda install -y -c conda-forge openmmWORKDIR/projectCOPY./run_openmm_simulation.py /projectLABELorg.opencontainers.image.source https://github.com/bacalhau-project/examplesCMD["python","run_openmm_simulation.py"]Build the container​We will rundocker buildcommand to build the container:Copydockerbuild-t<hub-user>/<repo-name>:<tag>.Before running the command, replace:hub-userwith your docker hub username, If you don’t have a docker hub accountfollow these instructions to create docker account, and use the username of the account you createdrepo-namewith the name of the container, you can name it anything you wanttagthis is not required but you can use the latest tagIn our case, this will be:Copydockerbuildxbuild--platformlinux/amd64--push-tghcr.io/bacalhau-project/examples/openmm:0.3.Push the container​Next, upload the image to the registry. This can be done by using the Docker hub username, repo name, or tag.Copydockerpush<hub-user>/<repo-name>:<tag>Run a Bacalhau Job​Now that we have the data in IPFS and the docker image pushed, we can run a job on the Bacalhau network.CopyexportJOB_ID=$(bacalhaudockerrun\--inputipfs://bafybeig63whfqyuvwqqrp5456fl4anceju24ttyycexef3k5eurg5uvrq4\--wait\--id-only\ghcr.io/bacalhau-project/examples/openmm:0.3\--pythonrun_openmm_simulation.py)Structure of the command​Lets look closely at the command above:bacalhau docker run: call to Bacalhaubafybeig63whfqyuvwqqrp5456fl4anceju24ttyycexef3k5eurg5uvrq4: here we mount the CID of the dataset we uploaded to IPFS to use on the jobghcr.io/bacalhau-project/examples/openmm:0.3: the name and the tag of the image we are usingpython run_openmm_simulation.py: the script that will be executed inside the containerWhen a job is submitted, Bacalhau prints out the relatedjob_id. We store that in an environment variable so that we can reuse it later on.Checking the State of your Jobs​Job status: You can check the status of the job usingbacalhau job list.Copybacalhaujoblist--id-filter=${JOB_ID}--no-styleWhen it saysPublishedorCompleted, that means the job is done, and we can get the results.Job information: You can find out more information about your job by usingbacalhau job describe.Copybacalhaujobdescribe${JOB_ID}Job download: You can download your job results directly by usingbacalhau job get. Alternatively, you can choose to create a directory to store your results. In the command below, we created a directory (results) and downloaded our job output to be stored in that directory.Copyrm-rfresults&&mkdir-presultsbacalhaujobget${JOB_ID}--output-dirresults# Download the resultsViewing your Job Output​To view the file, run the following command:Copycatresults/outputs/final_state.pdbxSupport​If you have questions or need support or guidance, please reach out to theBacalhau team via Slack(#generalchannel).PreviousGromacs for AnalysisNextJobs GuideLast updated1 month agoOn this pageIntroductionPrerequisite​Running Locally​Writing the Script​Running the Script​Uploading the Data to IPFS​Containerize Script using Docker​Build the container​Push the container​Run a Bacalhau Job​Structure of the command​Checking the State of your Jobs​Viewing your Job Output​Support​Was this helpful?Edit on GitHubExport as PDFGet SupportExpansoSupportUse CasesDistributed ETLEdge MLDistributed Data WarehousingFlett ManagementAbout UsWho we areWhat we valueNews & BlogBlogNewsExpanso (2024). All Rights Reserved.