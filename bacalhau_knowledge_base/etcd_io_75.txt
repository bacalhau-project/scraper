URL: https://etcd.io/docs/v3.6/op-guide/maintenance/

etcdDocsBlogCommunityInstallPlayVersionsv3.6v3.5v3.4v3.3v3.2v3.1v2.3Versionsv3.6-DRAFTQuickstartDemoTutorialsHow to Set Up a Demo etcd ClusterReading from etcdWriting to etcdHow to get keys by prefixHow to delete keysHow to make multiple writes in a transactionHow to watch keysHow to create leaseHow to create locksHow to conduct leader election in etcd clusterHow to check Cluster statusHow to save the databaseHow to migrate etcd from v2 to v3How to Add and Remove MembersInstallFAQLibraries and toolsMetricsReporting bugsTuningDiscovery service protocolLogging conventionsGolang modulesLearningData modeletcd client designetcd learner designetcd v3 authentication designetcd APIetcd persistent storage filesetcd API guaranteesetcd versus other key-value storesGlossaryDeveloper guideDiscovery service protocolSet up a local clusterInteracting with etcdWhy gRPC gatewaygRPC naming and discoverySystem limitsetcd featuresAPI referenceAPI reference: concurrencyOperations guideAuthentication GuidesRole-based access controlAuthenticationConfiguration optionsTransport security modelClustering GuideRun etcd clusters as a Kubernetes StatefulSetRun etcd clusters inside containersFailure modesDisaster recoveryetcd gatewaygRPC proxyHardware recommendationsMaintenanceMonitoring etcdPerformanceDesign of runtime reconfigurationRuntime reconfigurationSupported platformsVersioningData CorruptionBenchmarksStorage Memory Usage BenchmarkWatch Memory Usage BenchmarkBenchmarking etcd v3Benchmarking etcd v2.2.0-rc-memoryBenchmarking etcd v2.2.0-rcBenchmarking etcd v2.2.0Benchmarking etcd v2.1.0UpgradingUpgrading etcd clusters and applicationsUpgrade etcd from 3.4 to 3.5Upgrade etcd from 3.3 to 3.4Upgrade etcd from 3.2 to 3.3Upgrade etcd from 3.1 to 3.2Upgrade etcd from 3.0 to 3.1Upgrade etcd from 2.3 to 3.0TriageIssue triage guidelinesPR managementv3.5QuickstartDemoTutorialsHow to Set Up a Demo etcd ClusterReading from etcdWriting to etcdHow to get keys by prefixHow to delete keysHow to make multiple writes in a transactionHow to watch keysHow to create leaseHow to create locksHow to conduct leader election in etcd clusterHow to check Cluster statusHow to save the databaseHow to migrate etcd from v2 to v3How to Add and Remove MembersInstallFAQLibraries and toolsMetricsReporting bugsTuningDiscovery service protocolLogging conventionsGolang modulesLearningData modeletcd client designetcd learner designetcd v3 authentication designetcd APIetcd persistent storage filesetcd API guaranteesetcd versus other key-value storesGlossaryDeveloper guideDiscovery service protocolSet up a local clusterInteracting with etcdWhy gRPC gatewaygRPC naming and discoverySystem limitsetcd featuresAPI referenceAPI reference: concurrencyOperations guideAuthentication GuidesRole-based access controlAuthenticationConfiguration optionsTransport security modelClustering GuideRun etcd clusters as a Kubernetes StatefulSetRun etcd clusters inside containersFailure modesDisaster recoveryetcd gatewaygRPC proxyHardware recommendationsMaintenanceMonitoring etcdPerformanceDesign of runtime reconfigurationRuntime reconfigurationSupported platformsVersioningData CorruptionBenchmarksStorage Memory Usage BenchmarkWatch Memory Usage BenchmarkBenchmarking etcd v3Benchmarking etcd v2.2.0-rc-memoryBenchmarking etcd v2.2.0-rcBenchmarking etcd v2.2.0Benchmarking etcd v2.1.0DowngradingDowngrading etcd clusters and applicationsDowngrade etcd from 3.5 to 3.4UpgradingUpgrading etcd clusters and applicationsUpgrade etcd from 3.4 to 3.5Upgrade etcd from 3.3 to 3.4Upgrade etcd from 3.2 to 3.3Upgrade etcd from 3.1 to 3.2Upgrade etcd from 3.0 to 3.1Upgrade etcd from 2.3 to 3.0TriageIssue triage guidelinesPR managementv3.4QuickstartOverviewDemoInstallFAQLibraries and toolsMetricsReporting bugsTuningDiscovery service protocolLogging conventionsLearningData modeletcd client designetcd learner designetcd v3 authentication designetcd3 APIetcd API guaranteesetcd versus other key-value storesGlossaryDeveloper guideDiscovery service protocolSet up a local clusterInteracting with etcdWhy gRPC gatewaygRPC naming and discoverySystem limitsetcd featuresAPI referenceAPI reference: concurrencyOperations guideConfiguration optionsRole-based access controlTransport security modelClustering GuideRun etcd clusters inside containersFailure modesDisaster recoveryetcd gatewaygRPC proxyHardware recommendationsMaintenancePerformanceDesign of runtime reconfigurationRuntime reconfigurationSupported platformsMigrate applications from using API v2 to API v3VersioningData CorruptionMonitoring etcdBenchmarksStorage Memory Usage BenchmarkWatch Memory Usage BenchmarkBenchmarking etcd v3Benchmarking etcd v2.2.0-rc-memoryBenchmarking etcd v2.2.0-rcBenchmarking etcd v2.2.0Benchmarking etcd v2.1.0UpgradingUpgrading etcd clusters and applicationsUpgrade etcd from 3.4 to 3.5Upgrade etcd from 3.3 to 3.4Upgrade etcd from 3.2 to 3.3Upgrade etcd from 3.1 to 3.2Upgrade etcd from 3.0 to 3.1Upgrade etcd from 2.3 to 3.0PlatformsAmazon Web ServicesContainer Linux with systemdFreeBSDTriageIssue Triage Guidelinesv3.3InstallLibraries and toolsMetricsBenchmarksBenchmarking etcd v2.1.0Benchmarking etcd v2.2.0Benchmarking etcd v2.2.0-rcBenchmarking etcd v2.2.0-rc-memoryBenchmarking etcd v3Storage Memory Usage BenchmarkWatch Memory Usage BenchmarkDemoDeveloper guideDiscovery service protocoletcd API Referenceetcd concurrency API ReferenceExperimental APIs and featuresgRPC naming and discoveryInteracting with etcdSet up a local clusterSystem limitsWhy gRPC gatewayDiscovery service protocoletcd v3 APIFrequently Asked Questions (FAQ)Learningetcd client architectureClient feature matrixData modeletcd v3 authentication designetcd versus other key-value storesetcd3 APIGlossaryKV API guaranteesLearnerLogging conventionsOperations guideMonitoring etcdVersioningClustering GuideConfiguration flagsDesign of runtime reconfigurationDisaster recoveryetcd gatewayFailure modesgRPC proxyHardware recommendationsMaintenanceMigrate applications from using API v2 to API v3PerformanceRole-based access controlRun etcd clusters inside containersRuntime reconfigurationSupported systemsTransport security modelPlatformsAmazon Web ServicesContainer Linux with systemdFreeBSDProduction usersReporting bugsTuningUpgradingUpgrade etcd from 2.3 to 3.0Upgrade etcd from 3.0 to 3.1Upgrade etcd from 3.1 to 3.2Upgrade etcd from 3.2 to 3.3Upgrade etcd from 3.3 to 3.4Upgrade etcd from 3.4 to 3.5Upgrading etcd clusters and applicationsv3.2BenchmarksBenchmarking etcd v2.1.0Benchmarking etcd v2.2.0Benchmarking etcd v2.2.0-rcBenchmarking etcd v2.2.0-rc-memoryBenchmarking etcd v3-demoStorage Memory Usage BenchmarkWatch Memory Usage BenchmarkData modelDemoDeveloper guideDiscovery service protocoletcd API referenceetcd concurrency API ReferenceExperimental APIs and featuresgRPC gatewaygRPC naming and discoveryInteracting with etcdSet up a local clusterSystem limitsetcd dev internalDiscovery service protocolLogging conventionsetcd operations guideAuthentication GuideClustering GuideConfiguration flagsDesign of runtime reconfigurationDisaster recoveryetcd gatewaygRPC proxyHardware recommendationsMaintenanceMigrate applications from using API v2 to API v3Monitoring etcdPerformanceRun etcd clusters inside containersRuntime reconfigurationSecurity modelSupported platformsUnderstand failuresVersioningetcd upgradesUpgrade etcd from 2.3 to 3.0Upgrade etcd from 3.0 to 3.1Upgrade etcd from 3.1 to 3.2Upgrade etcd from 3.2 to 3.3Upgrade etcd from 3.3 to 3.4Upgrading etcd clusters and applicationsetcd v3 authentication designetcd versus other key-value storesetcd3 APIFrequently Asked Questions (FAQ)GlossaryInstallKV API guaranteesLibraries and toolsMetricsPlatformsAmazon Web ServicesFreeBSDRun etcd on Container Linux with systemdProduction usersReporting bugsRFCetcd v3 APITuningv3.1Data modelDemoetcd benchmarksetcd v2.1.0-alpha benchmarksetcd v2.2.0 benchmarksetcd v2.2.0-rc benchmarksetcd v2.2.0-rc-memory benchmarksetcd v3-demo benchmarksStorage Memory Usage BenchmarkWatch Memory Usage Benchmarketcd developer guideDiscovery service protocoletcd API ReferenceExperimental APIs and featuresgRPC GatewaygRPC naming and discoveryInteracting with etcdSetup a local clusterSystem limitsetcd internal devDiscovery service protocolLogging conventionsetcd operations guideClustering GuideConfiguration flagsDesign of runtime reconfigurationDisaster recoveryetcd gatewaygRPC proxyHardware recommendationsMaintenanceMigrate applications from using API v2 to API v3Monitoring etcdPerformanceRun etcd clusters inside containersRuntime reconfigurationSecurity modelSupported platformsUnderstand failuresVersioningetcd3 APIFrequently Asked Questions (FAQ)GlossaryInstallKV API guaranteesLibraries and toolsMetricsPlatformsFreeBSDProduction usersReporting bugsRFCetcd v3 APITuningUpgrading etcd clusters and applicationsUpgrade etcd from 2.3 to 3.0Upgrade etcd from 3.0 to 3.1Upgrade etcd from 3.1 to 3.2Upgrade etcd from 3.2 to 3.3Upgrade etcd from 3.3 to 3.4Why etcdv2.3AdministrationAuthentication GuideBackward CompatibilityBenchmarksBenchmarking etcd v2.2.0etcd 2.1.0-alpha benchmarksetcd 2.2.0-rc benchmarksetcd 2.2.0-rc memory benchmarksetcd 3 demo benchmarksStorage Memory Usage BenchmarkWatch Memory Usage BenchmarkClustering GuideConfiguration FlagsDesign of Runtime ReconfigurationDevelopmentDiscovery Service ProtocolError Codeetcd APIetcd v3 APIFAQGlossaryLibraries and ToolsMembers APIMetricsMiscellaneous APIsPlatformsFreeBSProduction UsersProxyReporting BugsRunning etcd under DockerRuntime ReconfigurationSecurity ModelSnapshot MigrationTuningUpgrade etcd from 2.1 to 2.2Upgrade etcd from 2.1 to 2.2Upgrade etcd from 2.2 to 2.3v2 Auth and SecurityVersioningView page sourceEdit this pageCreate child pageCreate documentation issueCreate project issueOverviewRaft log retentionHistory compaction: v3 API Key-Value DatabaseAuto CompactionDefragmentationSpace quotaSnapshot backupVersionsv3.6-DRAFTOperations guideMaintenanceMaintenancePeriodic etcd cluster maintenance guideOverviewAn etcd cluster needs periodic maintenance to remain reliable. Depending on an etcd application’s needs, this maintenance can usually be automated and performed without downtime or significantly degraded performance.All etcd maintenance manages storage resources consumed by the etcd keyspace. Failure to adequately control the keyspace size is guarded by storage space quotas; if an etcd member runs low on space, a quota will trigger cluster-wide alarms which will put the system into a limited-operation maintenance mode. To avoid running out of space for writes to the keyspace, the etcd keyspace history must be compacted. Storage space itself may be reclaimed by defragmenting etcd members. Finally, periodic snapshot backups of etcd member state makes it possible to recover any unintended logical data loss or corruption caused by operational error.Raft log retentionetcd --snapshot-countconfigures the number of applied Raft entries to hold in-memory before compaction. When--snapshot-countreaches, server first persists snapshot data onto disk, and then truncates old entries. When a slow follower requests logs before a compacted index, leader sends the snapshot forcing the follower to overwrite its state.Higher--snapshot-countholds more Raft entries in memory until snapshot, thus causingrecurrent higher memory usage. Since leader retains latest Raft entries for longer, a slow follower has more time to catch up before leader snapshot.--snapshot-countis a tradeoff between higher memory usage and better availabilities of slow followers.Since v3.2, the default value of--snapshot-counthaschanged from from 10,000 to 100,000.In performance-wise,--snapshot-countgreater than 100,000 may impact the write throughput. Higher number of in-memory objects can slow downGo GC mark phaseruntime.scanobject, and infrequent memory reclamation makes allocation slow. Performance varies depending on the workloads and system environments. However, in general, too frequent compaction affects cluster availabilities and write throughputs. Too infrequent compaction is also harmful placing too much pressure on Go garbage collector. Seehttps://www.slideshare.net/mitakeh/understanding-performance-aspects-of-etcd-and-raftfor more research results.History compaction: v3 API Key-Value DatabaseSince etcd keeps an exact history of its keyspace, this history should be periodically compacted to avoid performance degradation and eventual storage space exhaustion. Compacting the keyspace history drops all information about keys superseded prior to a given keyspace revision. The space used by these keys then becomes available for additional writes to the keyspace.The keyspace can be compacted automatically withetcd’s time windowed history retention policy, or manually withetcdctl. Theetcdctlmethod provides fine-grained control over the compacting process whereas automatic compacting fits applications that only need key history for some length of time.Anetcdctlinitiated compaction works as follows:# compact up to revision 3$ etcdctl compact3Revisions prior to the compaction revision become inaccessible:$ etcdctl get --rev=2somekeyError:  rpc error:code=11desc=etcdserver: mvcc: required revision has been compactedAuto Compactionetcdcan be set to automatically compact the keyspace with the--auto-compaction-*option with a period of hours:# keep one hour of history$ etcd --auto-compaction-retention=1v3.0.0andv3.1.0with--auto-compaction-retention=10run periodic compaction on v3 key-value store for every 10-hour. Compactor only supports periodic compaction. Compactor records latest revisions every 5-minute, until it reaches the first compaction period (e.g. 10-hour). In order to retain key-value history of last compaction period, it uses the last revision that was fetched before compaction period, from the revision records that were collected every 5-minute. When--auto-compaction-retention=10, compactor uses revision 100 for compact revision where revision 100 is the latest revision fetched from 10 hours ago. If compaction succeeds or requested revision has already been compacted, it resets period timer and starts over with new historical revision records (e.g. restart revision collect and compact for the next 10-hour period). If compaction fails, it retries in 5 minutes.v3.2.0compactor runsevery hour. Compactor only supports periodic compaction. Compactor continues to record latest revisions every 5-minute. For every hour, it uses the last revision that was fetched before compaction period, from the revision records that were collected every 5-minute. That is, for every hour, compactor discards historical data created before compaction period. The retention window of compaction period moves to next hour. For instance, when hourly writes are 100 and--auto-compaction-retention=10, v3.1 compacts revision 1000, 2000, and 3000 for every 10-hour, while v3.2.x, v3.3.0, v3.3.1, and v3.3.2 compact revision 1000, 1100, and 1200 for every 1-hour. If compaction succeeds or requested revision has already been compacted, it resets period timer and removes used compacted revision from historical revision records (e.g. start next revision collect and compaction from previously collected revisions). If compaction fails, it retries in 5 minutes.Inv3.3.0,v3.3.1, andv3.3.2,--auto-compaction-mode=revision --auto-compaction-retention=1000automaticallyCompacton"latest revision" - 1000every 5-minute (when latest revision is 30000, compact on revision 29000). For instance,--auto-compaction-mode=periodic --auto-compaction-retention=72hautomaticallyCompactwith 72-hour retention window, for every 7.2-hour. For instance,--auto-compaction-mode=periodic --auto-compaction-retention=30mautomaticallyCompactwith 30-minute retention window, for every 3-minute. Periodic compactor continues to record latest revisions for every 1/10 of given compaction period (e.g. 1-hour when--auto-compaction-mode=periodic --auto-compaction-retention=10h). For every 1/10 of given compaction period, compactor uses the last revision that was fetched before compaction period, to discard historical data. The retention window of compaction period moves for every 1/10 of given compaction period. For instance, when hourly writes are 100 and--auto-compaction-retention=10, v3.1 compacts revision 1000, 2000, and 3000 for every 10-hour, while v3.2.x, v3.3.0, v3.3.1, and v3.3.2 compact revision 1000, 1100, and 1200 for every 1-hour. Furthermore, when writes per minute are 1000, v3.3.0, v3.3.1, and v3.3.2 with--auto-compaction-mode=periodic --auto-compaction-retention=30mcompact revision 30000, 33000, and 36000, for every 3-minute with more finer granularity.When--auto-compaction-retention=10h, etcd first waits 10-hour for the first compaction, and then does compaction every hour (1/10 of 10-hour) afterwards like this:0Hr  (rev = 1)
1hr  (rev = 10)
...
8hr  (rev = 80)
9hr  (rev = 90)
10hr (rev = 100, Compact(1))
11hr (rev = 110, Compact(10))
...Whether compaction succeeds or not, this process repeats for every 1/10 of given compaction period. If compaction succeeds, it just removes compacted revision from historical revision records.Inv3.3.3,--auto-compaction-mode=revision --auto-compaction-retention=1000automaticallyCompacton"latest revision" - 1000every 5-minute (when latest revision is 30000, compact on revision 29000). Previously,--auto-compaction-mode=periodic --auto-compaction-retention=72hautomaticallyCompactwith 72-hour retention window for every 7.2-hour.Now,Compacthappens, for every 1-hour but still with 72-hour retention window.Previously,--auto-compaction-mode=periodic --auto-compaction-retention=30mautomaticallyCompactwith 30-minute retention window for every 3-minute.Now,Compacthappens, for every 30-minute but still with 30-minute retention window.Periodic compactor keeps recording latest revisions for every compaction period when given period is less than 1-hour, or for every 1-hour when given compaction period is greater than 1-hour (e.g. 1-hour when--auto-compaction-mode=periodic --auto-compaction-retention=24h). For every compaction period or 1-hour, compactor uses the last revision that was fetched before compaction period, to discard historical data. The retention window of compaction period moves for every given compaction period or hour. For instance, when hourly writes are 100 and--auto-compaction-mode=periodic --auto-compaction-retention=24h,v3.2.x,v3.3.0,v3.3.1, andv3.3.2compact revision 2400, 2640, and 2880 for every 2.4-hour, whilev3.3.3or latercompacts revision 2400, 2500, 2600 for every 1-hour. Furthermore, when--auto-compaction-mode=periodic --auto-compaction-retention=30mand writes per minute are about 1000,v3.3.0,v3.3.1, andv3.3.2compact revision 30000, 33000, and 36000, for every 3-minute, whilev3.3.3or latercompacts revision 30000, 60000, and 90000, for every 30-minute.DefragmentationAfter compacting the keyspace, the backend database may exhibit internal fragmentation. Any internal fragmentation is space that is free to use by the backend but still consumes storage space. Compacting old revisions internally fragmentsetcdby leaving gaps in backend database. Fragmented space is available for use byetcdbut unavailable to the host filesystem. In other words, deleting application data does not reclaim the space on disk.The process of defragmentation releases this storage space back to the file system. Defragmentation is issued on a per-member basis so that cluster-wide latency spikes may be avoided.To defragment an etcd member, use theetcdctl defragcommand:$ etcdctl defragFinished defragmenting etcd member[127.0.0.1:2379]Note that defragmentation to a live member blocks the system from reading and writing data while rebuilding its states.Note that defragmentation request does not get replicated over cluster. That is, the request is only applied to the local node. Specify all members in--endpointsflag or--clusterflag to automatically find all cluster members.Run defragment operations for all endpoints in the cluster associated with the default endpoint:$ etcdctl defrag --clusterFinished defragmenting etcd member[http://127.0.0.1:2379]Finished defragmenting etcd member[http://127.0.0.1:22379]Finished defragmenting etcd member[http://127.0.0.1:32379]To defragment an etcd data directory directly, while etcd is not running, use the command:$ etcdutl defrag --data-dir <path-to-etcd-data-dir>Space quotaThe space quota inetcdensures the cluster operates in a reliable fashion. Without a space quota,etcdmay suffer from poor performance if the keyspace grows excessively large, or it may simply run out of storage space, leading to unpredictable cluster behavior. If the keyspace’s backend database for any member exceeds the space quota,etcdraises a cluster-wide alarm that puts the cluster into a maintenance mode which only accepts key reads and deletes. Only after freeing enough space in the keyspace and defragmenting the backend database, along with clearing the space quota alarm can the cluster resume normal operation.By default,etcdsets a conservative space quota suitable for most applications, but it may be configured on the command line, in bytes:# set a very small 16 MiB quota$ etcd --quota-backend-bytes=$((16*1024*1024))The space quota can be triggered with a loop:# fill keyspace$while[1];doddif=/dev/urandombs=1024count=1024|ETCDCTL_API=3etcdctl put key||break;done...Error:  rpc error:code=8desc=etcdserver: mvcc: database space exceeded# confirm quota space is exceeded$ETCDCTL_API=3etcdctl --write-out=table endpoint status+----------------+------------------+-----------+---------+-----------+-----------+------------+|ENDPOINT|ID|VERSION|DB SIZE|IS LEADER|RAFT TERM|RAFT INDEX|+----------------+------------------+-----------+---------+-----------+-----------+------------+|127.0.0.1:2379|bf9071f4639c75cc|2.3.0+git|18MB|true|2|3332|+----------------+------------------+-----------+---------+-----------+-----------+------------+# confirm alarm is raised$ETCDCTL_API=3etcdctl alarm listmemberID:13803658152347727308 alarm:NOSPACERemoving excessive keyspace data and defragmenting the backend database will put the cluster back within the quota limits:# get current revision$rev=$(ETCDCTL_API=3etcdctl --endpoints=:2379 endpoint status --write-out="json"|egrep -o'"revision":[0-9]*'|egrep -o'[0-9].*')# compact away all old revisions$ETCDCTL_API=3etcdctl compact$revcompacted revision1516# defragment away excessive space$ETCDCTL_API=3etcdctl defragFinished defragmenting etcd member[127.0.0.1:2379]# disarm alarm$ETCDCTL_API=3etcdctl alarm disarmmemberID:13803658152347727308 alarm:NOSPACE# test puts are allowed again$ETCDCTL_API=3etcdctl put newkey123OKThe metricetcd_mvcc_db_total_size_in_use_in_bytesindicates the actual database usage after a history compaction, whileetcd_debugging_mvcc_db_total_size_in_bytesshows the database size including free space waiting for defragmentation. The latter increases only when the former is close to it, meaning when both of these metrics are close to the quota, a history compaction is required to avoid triggering the space quota.etcd_debugging_mvcc_db_total_size_in_bytesis renamed toetcd_mvcc_db_total_size_in_bytesfrom v3.4.NOTE:it is possible to get anErrGRPCNoSpaceerror for a Put/Txn/LeaseGrant request, and still have the write request succeed in the backend, because etcd checks space quota at the API layer and the internal Apply layer, and the Apply layer will only raise theNOSPACEalarm without blocking the transaction from proceeding.Snapshot backupSnapshotting theetcdcluster on a regular basis serves as a durable backup for an etcd keyspace. By taking periodic snapshots of an etcd member’s backend database, anetcdcluster can be recovered to a point in time with a known good state.A snapshot is taken withetcdctl:$ etcdctl snapshot save backup.db$ etcdutl --write-out=table snapshot status backup.db+----------+----------+------------+------------+|HASH|REVISION|TOTAL KEYS|TOTAL SIZE|+----------+----------+------------+------------+|fe01cf57|10|7|2.1 MB|+----------+----------+------------+------------+FeedbackWas this page helpful?YesNoGlad to hear it! Pleasetell us how we can improve.Sorry to hear that. Pleasetell us how we can improve.Last modified August 17, 2024:fix typo (264cc3e)©
2013–2024etcd AuthorsTerms|Privacy|Trademarks|LicenseAll Rights Reserved