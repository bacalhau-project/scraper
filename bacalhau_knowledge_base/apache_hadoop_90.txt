URL: https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapredCommands.html

Wiki|git|Apache Hadoop| Last Published: 2024-03-04
               | Version: 3.4.0GeneralOverviewSingle Node SetupCluster SetupCommands ReferenceFileSystem ShellCompatibility SpecificationDownstream Developer's GuideAdmin Compatibility GuideInterface ClassificationFileSystem SpecificationCommonCLI Mini ClusterFair Call QueueNative LibrariesProxy UserRack AwarenessSecure ModeService Level AuthorizationHTTP AuthenticationCredential Provider APIHadoop KMSTracingUnix Shell GuideRegistryAsync ProfilerHDFSArchitectureUser GuideCommands ReferenceNameNode HA With QJMNameNode HA With NFSObserver NameNodeFederationViewFsViewFsOverloadSchemeSnapshotsEdits ViewerImage ViewerPermissions and HDFSQuotas and HDFSlibhdfs (C API)WebHDFS (REST API)HttpFSShort Circuit Local ReadsCentralized Cache ManagementNFS GatewayRolling UpgradeExtended AttributesTransparent EncryptionMultihomingStorage PoliciesMemory Storage SupportSynthetic Load GeneratorErasure CodingDisk BalancerUpgrade DomainDataNode AdminRouter FederationProvided StorageMapReduceTutorialCommands ReferenceCompatibility with 1.xEncrypted ShufflePluggable Shuffle/SortDistributed Cache DeploySupport for YARN Shared CacheMapReduce REST APIsMR Application MasterMR History ServerYARNArchitectureCommands ReferenceCapacity SchedulerFair SchedulerResourceManager RestartResourceManager HAResource ModelNode LabelsNode AttributesWeb Application ProxyTimeline ServerTimeline Service V.2Writing YARN ApplicationsYARN Application SecurityNodeManagerRunning Applications in Docker ContainersRunning Applications in runC ContainersUsing CGroupsSecure ContainersReservation SystemGraceful DecommissionOpportunistic ContainersYARN FederationShared CacheUsing GPUUsing FPGAPlacement ConstraintsYARN UI2YARN REST APIsIntroductionResource ManagerNode ManagerTimeline ServerTimeline Service V.2YARN ServiceOverviewQuickStartConceptsYarn Service APIService DiscoverySystem ServicesHadoop Compatible File SystemsAliyun OSSAmazon S3Azure Blob StorageAzure Data Lake StorageTencent COSHuaweicloud OBSAuthOverviewExamplesConfigurationBuildingToolsHadoop StreamingHadoop ArchivesHadoop Archive LogsDistCpHDFS Federation BalanceGridMixRumenResource Estimator ServiceScheduler Load SimulatorHadoop BenchmarkingDynamometerReferenceChangelog and Release NotesJava API docsUnix Shell APIMetricsConfigurationcore-default.xmlhdfs-default.xmlhdfs-rbf-default.xmlmapred-default.xmlyarn-default.xmlkms-default.xmlhttpfs-default.xmlDeprecated PropertiesMapReduce Commands GuideOverviewUser Commandsarchivearchive-logsclasspathdistcpjobpipesqueueversionenvvarsAdministration CommandshistoryserverhsadminframeworkuploaderOverviewAll mapreduce commands are invoked by thebin/mapredscript. Running the mapred script without any arguments prints the description for all commands.Usage:mapred [SHELL_OPTIONS] COMMAND [GENERIC_OPTIONS] [COMMAND_OPTIONS]Hadoop has an option parsing framework that employs parsing generic options as well as running classes.COMMAND_OPTIONSDescriptionSHELL_OPTIONSThe common set of shell options. These are documented on theHadoop Commands Referencepage.GENERIC_OPTIONSThe common set of options supported by multiple commands. See theHadoop Commands Referencefor more information.COMMAND_OPTIONSVarious commands with their options are described in the following sections. The commands have been grouped intoUser CommandsandAdministration Commands.User CommandsCommands useful for users of a hadoop cluster.archiveCreates a hadoop archive. More information can be found atHadoop Archives Guide.archive-logsA tool to combine YARN aggregated logs into Hadoop archives to reduce the number of files in HDFS. More information can be found atHadoop Archive Logs Guide.classpathUsage:yarn classpath [--glob |--jar <path> |-h |--help]COMMAND_OPTIONDescription--globexpand wildcards--jarpathwrite classpath as manifest in jar namedpath-h,--helpprint helpPrints the class path needed to get the Hadoop jar and the required libraries. If called without arguments, then prints the classpath set up by the command scripts, which is likely to contain wildcards in the classpath entries. Additional options print the classpath after wildcard expansion or write the classpath into the manifest of a jar file. The latter is useful in environments where wildcards cannot be used and the expanded classpath exceeds the maximum supported command line length.distcpCopy file or directories recursively. More information can be found atHadoop DistCp Guide.jobCommand to interact with Map Reduce Jobs.Usage:mapred job | [GENERIC_OPTIONS] | [-submit <job-file>] | [-status <job-id>] | [-counter <job-id> <group-name> <counter-name>] | [-kill <job-id>] | [-events <job-id> <from-event-#> <#-of-events>] | [-history [all] <jobHistoryFile|jobId> [-outfile <file>] [-format <human|json>]] | [-list [all]] | [-kill-task <task-id>] | [-fail-task <task-id>] | [-set-priority <job-id> <priority>] | [-list-active-trackers] | [-list-blacklisted-trackers] | [-list-attempt-ids <job-id> <task-type> <task-state>] [-logs <job-id> <task-attempt-id>] [-config <job-id> <file>]COMMAND_OPTIONDescription-submitjob-fileSubmits the job.-statusjob-idPrints the map and reduce completion percentage and all job counters.-counterjob-idgroup-namecounter-namePrints the counter value.-killjob-idKills the job.-eventsjob-idfrom-event-##-of-eventsPrints the events’ details received by jobtracker for the given range.-history [all]jobHistoryFile|jobId[-outfilefile] [-formathuman|json]Prints job details, failed and killed task details. More details about the job such as successful tasks, task attempts made for each task, task counters, etc can be viewed by specifying the [all] option. An optional file output path (instead of stdout) can be specified. The format defaults to human-readable but can also be changed to JSON with the [-format] option.-list [all]Displays jobs which are yet to complete.-list alldisplays all jobs.-kill-tasktask-idKills the task. Killed tasks are NOT counted against failed attempts.-fail-tasktask-idFails the task. Failed tasks are counted against failed attempts.-set-priorityjob-idpriorityChanges the priority of the job. Allowed priority values are VERY_HIGH, HIGH, NORMAL, LOW, VERY_LOW-list-active-trackersList all the active NodeManagers in the cluster.-list-blacklisted-trackersList the black listed task trackers in the cluster. This command is not supported in MRv2 based cluster.-list-attempt-idsjob-idtask-typetask-stateList the attempt-ids based on the task type and the status given. Valid values for task-type are REDUCE, MAP. Valid values for task-state are running, pending, completed, failed, killed.-logsjob-idtask-attempt-idDump the container log for a job if taskAttemptId is not specified, otherwise dump the log for the task with the specified taskAttemptId. The logs will be dumped in system out.-configjob-idfileDownload the job configuration file.pipesRuns a pipes job.Usage:mapred pipes [-conf <path>] [-jobconf <key=value>, <key=value>, ...] [-input <path>] [-output <path>] [-jar <jar file>] [-inputformat <class>] [-map <class>] [-partitioner <class>] [-reduce <class>] [-writer <class>] [-program <executable>] [-reduces <num>]COMMAND_OPTIONDescription-confpathConfiguration for job-jobconfkey=value,key=value, …Add/override configuration for job-inputpathInput directory-outputpathOutput directory-jarjar fileJar filename-inputformatclassInputFormat class-mapclassJava Map class-partitionerclassJava Partitioner-reduceclassJava Reduce class-writerclassJava RecordWriter-programexecutableExecutable URI-reducesnumNumber of reducesqueuecommand to interact and view Job Queue informationUsage:mapred queue [-list] | [-info <job-queue-name> [-showJobs]] | [-showacls]COMMAND_OPTIONDescription-listGets list of Job Queues configured in the system. Along with scheduling information associated with the job queues.-infojob-queue-name[-showJobs]Displays the job queue information and associated scheduling information of particular job queue. If-showJobsoptions is present a list of jobs submitted to the particular job queue is displayed.-showaclsDisplays the queue name and associated queue operations allowed for the current user. The list consists of only those queues to which the user has access.versionPrints the version.Usage:mapred versionenvvarsUsage:mapred envvarsDisplay computed Hadoop environment variables.Administration CommandsCommands useful for administrators of a hadoop cluster.historyserverStart JobHistoryServer.Usage:mapred historyserverhsadminRuns a MapReduce hsadmin client for execute JobHistoryServer administrative commands.Usage:mapred hsadmin [-refreshUserToGroupsMappings] | [-refreshSuperUserGroupsConfiguration] | [-refreshAdminAcls] | [-refreshLoadedJobCache] | [-refreshLogRetentionSettings] | [-refreshJobRetentionSettings] | [-getGroups [username]] | [-help [cmd]]COMMAND_OPTIONDescription-refreshUserToGroupsMappingsRefresh user-to-groups mappings-refreshSuperUserGroupsConfigurationRefresh superuser proxy groups mappings-refreshAdminAclsRefresh acls for administration of Job history server-refreshLoadedJobCacheRefresh loaded job cache of Job history server-refreshJobRetentionSettingsRefresh job history period, job cleaner settings-refreshLogRetentionSettingsRefresh log retention period and log retention check interval-getGroups [username]Get the groups which given user belongs to-help [cmd]Displays help for the given command or all commands if none is specified.frameworkuploaderCollects framework jars and uploads them to HDFS as a tarball.Usage:mapred frameworkuploader -target <target> [-fs <filesystem>] [-input <classpath>] [-blacklist <list>] [-whitelist <list>] [-initialReplication <num>] [-acceptableReplication <num>] [-finalReplication <num>] [-timeout <seconds>] [-nosymlink]COMMAND_OPTIONDescription-inputclasspathThis is the input classpath that is searched for jar files to be included in the tarball.-fsfilesystemThe target file system. Defaults to the default filesystem set by fs.defaultFS.-targettargetThis is the target location of the framework tarball, optionally followed by a # with the localized alias. An example would be /usr/lib/framework.tar#framework. Make sure the target directory is readable by all users but it is not writable by others than administrators to protect cluster security.-blacklistlistThis is a comma separated regex array to filter the jar file names to exclude from the class path. It can be used for example to exclude test jars or Hadoop services that are not necessary to localize.-whitelistlistThis is a comma separated regex array to include certain jar files. This can be used to provide additional security, so that no external source can include malicious code in the classpath when the tool runs.-nosymlinkThis flag can be used to exclude symlinks that point to the same directory. This is not widely used. For example,/a/foo.jarand a symlink/a/bar.jarthat points to/a/foo.jarwould normally addfoo.jarandbar.jarto the tarball as separate files despite them actually being the same file. This flag would make the tool exclude/a/bar.jarso only one copy of the file is added.-initialReplicationnumThis is the replication count that the framework tarball is created with. It is safe to leave this value at the default 3. This is the tested scenario.-finalReplicationnumThe uploader tool sets the replication once all blocks are collected and uploaded. If quick initial startup is required, then it is advised to set this to the commissioned node count divided by two but not more than 512.-acceptableReplicationnumThe tool will wait until the tarball has been replicated this number of times before exiting. This should be a replication count less than or equal to the value infinalReplication. This is typically a 90% of the value infinalReplicationto accomodate failing nodes.-timeoutsecondsA timeout in seconds to wait to reachacceptableReplicationbefore the tool exits. The tool logs an error otherwise and returns.©            2008-2024
              Apache Software Foundation
            
                          -Privacy Policy.
        Apache Maven, Maven, Apache, the Apache feather logo, and the Apache Maven project logos are trademarks of The Apache Software Foundation.