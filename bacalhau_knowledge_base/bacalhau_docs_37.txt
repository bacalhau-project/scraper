URL: https://docs.bacalhau.org/setting-up/workload-onboarding/python/index-4

Bacalhau Docsv.1.4.0v.1.3.0v.1.3.1v.1.3.2v.1.4.0GitHubSlackContactMoreGitHubSlackContactAsk or SearchCtrl+â€†KWelcomeGetting StartedHow Bacalhau WorksInstallationCreate NetworkHardware SetupContainer OnboardingDocker WorkloadsWebAssembly (Wasm) WorkloadsSetting UpRunning NodesNode OnboardingGPU InstallationJob selection policyAccess ManagementNode persistenceConnect StorageConfiguration ManagementConfiguring Transport Level SecurityLimits and TimeoutsTest Network LocallyBacalhau WebUIWorkload OnboardingContainerDocker Workload OnboardingWebAssembly (Wasm) WorkloadsBacalhau Docker ImageHow To Work With Custom Containers in BacalhauPythonBuilding and Running Custom Python ContainerRunning Pandas on BacalhauRunning a Python ScriptRunning Jupyter Notebooks on BacalhauScripting Bacalhau with PythonR (language)Building and Running your Custom R Containers on BacalhauRunning a Simple R Script on BacalhauRun CUDA programs on BacalhauRunning a Prolog ScriptReading Data from Multiple S3 Buckets using BacalhauRunning Rust programs as WebAssembly (WASM)Generate Synthetic Data using Sparkov Data Generation techniqueData IngestionCopy Data from URL to Public StoragePinning DataRunning a Job over S3 dataNetworking InstructionsAccessing the Internet from JobsUtilizing NATS.io within BacalhauGPU Workloads SetupAutomatic Update CheckingMarketplace DeploymentsGoogle Cloud MarketplaceGuidesWrite a config.yamlWrite a SpecConfigExamplesData EngineeringUsing Bacalhau with DuckDBEthereum Blockchain Analysis with Ethereum-ETL and BacalhauConvert CSV To Parquet Or AvroSimple Image ProcessingOceanography - Data ConversionVideo ProcessingModel InferenceEasyOCR (Optical Character Recognition) on BacalhauRunning Inference on Dolly 2.0 Model with Hugging FaceSpeech Recognition using WhisperStable Diffusion on a GPUStable Diffusion on a CPUObject Detection with YOLOv5 on BacalhauGenerate Realistic Images using StyleGAN3 and BacalhauStable Diffusion Checkpoint InferenceRunning Inference on a Model stored on S3Model TrainingTraining Pytorch Model with BacalhauTraining Tensorflow ModelStable Diffusion Dreambooth (Finetuning)Molecular DynamicsRunning BIDS Apps on BacalhauCoresets On BacalhauGenomics Data GenerationGromacs for AnalysisMolecular Simulation with OpenMM and BacalhauReferencesJobs GuideJob SpecificationJob TypesTask SpecificationEnginesDocker Engine SpecificationWebAssembly (WASM) Engine SpecificationPublishersIPFS Publisher SpecificationLocal Publisher SpecificationS3 Publisher SpecificationSourcesIPFS Source SpecificationLocal Source SpecificationS3 Source SpecificationURL Source SpecificationNetwork SpecificationInput Source SpecificationResources SpecificationResultPath SpecificationConstraint SpecificationLabels SpecificationMeta SpecificationJob TemplatesQueuing & TimeoutsJob QueuingTimeouts SpecificationJob ResultsStateCLI GuideSingle CLI commandsAgentAgent OverviewAgent AliveAgent NodeAgent VersionConfigConfig OverviewConfig Auto-ResourcesConfig DefaultConfig ListConfig SetJobJob OverviewJob DescribeJob ExecJob ExecutionsJob HistoryJob ListJob LogsJob RunJob StopNodeNode OverviewNode ApproveNode DeleteNode ListNode DescribeNode RejectCLI Commands OverviewCommand MigrationAPI GuideBacalhau API overviewBest PracticesAgent EndpointOrchestrator EndpointMigration APINode ManagementAuthentication & AuthorizationDatabase IntegrationDebuggingDebugging Failed JobsDebugging LocallyOpen Telemetry in BacalhauRunning locally in 'devstack'Setting up Dev EnvironmentHelp & FAQBacalhau FAQsRelease NotesGlossaryIntegrationsApache Airflow Provider for BacalhauLilypadBacalhau Python SDKObservability for WebAssembly WorkloadsCommunitySocial MediaStyle GuideWays to ContributePowered by GitBookScripting Bacalhau with PythonBacalhau allows you to easily execute batch jobs via the CLI. But sometimes you need to do more than that. You might need to execute a script that requires user input, or you might need to execute a script that requires a lot of parameters. In any case, you probably want to execute your jobs in a repeatable manner.This example demonstrates a simple Python script that is able to orchestrate the execution of lots of jobs in a repeatable manner.PrerequisiteTo get started, you need to install the Bacalhau client, see more informationhereExecuting Bacalhau Jobs with Python ScriptsTo demonstrate this example, I will use the data generated from an Ethereum example. This produced a list of hashes that I will iterate over and execute a job for each one.Copy#write following to the file hashes.txtbafybeihvtzberlxrsz4lvzrzvpbanujmab3hr5okhxtbgv2zvonqos2l3ibafybeifb25fgxrzu45lsc47gldttomycqcsao22xa2gtk2ijbsa5muzegqbafybeig4wwwhs63ly6wbehwd7tydjjtnw425yvi2tlzt3aii3pfcj6hvoqbafybeievpb5q372q3w5fsezflij3wlpx6thdliz5xowimunoqushn3cwkabafybeih6te26iwf5kzzby2wqp67m7a5pmwilwzaciii3zipvhy64utikrebafybeicjd4545xph6rcyoc74wvzxyaz2vftapap64iqsp5ky6nz3f5yndmNow let's create a file calledbacalhau.py. The script below automates the submission, monitoring, and retrieval of results for multiple Bacalhau jobs in parallel. It is designed to be used in a scenario where there are multiple hash files, each representing a job, and the script manages the execution of these jobs using Bacalhau commands.Copy# write following to the file bacalhau.pyimportjson,glob,os,multiprocessing,shutil,subprocess,tempfile,time# checkStatusOfJob checks the status of a Bacalhau jobdefcheckStatusOfJob(job_id:str)->str:assertlen(job_id)>0p=subprocess.run(["bacalhau","list","--output","json","--id-filter", job_id],stdout=subprocess.PIPE,stderr=subprocess.PIPE,text=True,)r=parseJobStatus(p.stdout)ifr=="":print("job status is empty!%s"%job_id)elifr=="Completed":print("job completed:%s"%job_id)else:print("job not completed:%s-%s"%(job_id, r))returnr# submitJob submits a job to the Bacalhau networkdefsubmitJob(cid:str)->str:assertlen(cid)>0p=subprocess.run(["bacalhau","docker","run","--id-only","--wait=false","--input","ipfs://"+cid+":/inputs/data.tar.gz","ghcr.io/bacalhau-project/examples/blockchain-etl:0.0.6",],stdout=subprocess.PIPE,stderr=subprocess.PIPE,text=True,)ifp.returncode!=0:print("failed (%d) job:%s"%(p.returncode, p.stdout))job_id=p.stdout.strip()print("job submitted:%s"%job_id)returnjob_id# getResultsFromJob gets the results from a Bacalhau jobdefgetResultsFromJob(job_id:str)->str:assertlen(job_id)>0temp_dir=tempfile.mkdtemp()print("getting results for job:%s"%job_id)foriinrange(0,5):# try 5 timesp=subprocess.run(["bacalhau","get","--output-dir",temp_dir,job_id,],stdout=subprocess.PIPE,stderr=subprocess.PIPE,text=True,)ifp.returncode==0:breakelse:print("failed (exit%d) to get job:%s"%(p.returncode, p.stdout))returntemp_dir# parseJobStatus parses the status of a Bacalhau jobdefparseJobStatus(result:str)->str:iflen(result)==0:return""r=json.loads(result)iflen(r)>0:returnr[0]["State"]["State"]return""# parseHashes splits lines from a text file into a listdefparseHashes(filename:str)->list:assertos.path.exists(filename)withopen(filename,"r")asf:hashes=f.read().splitlines()returnhashesdefmain(file:str,num_files:int=-1):# Use multiprocessing to work in parallelcount=multiprocessing.cpu_count()withmultiprocessing.Pool(processes=count)aspool:hashes=parseHashes(file)[:num_files]print("submitting%djobs"%len(hashes))job_ids=pool.map(submitJob, hashes)assertlen(job_ids)==len(hashes)print("waiting for jobs to complete...")whileTrue:job_statuses=pool.map(checkStatusOfJob, job_ids)total_finished=sum(map(lambdax: x=="Completed", job_statuses))iftotal_finished>=len(job_ids):breakprint("%d/%djobs completed"%(total_finished,len(job_ids)))time.sleep(2)print("all jobs completed, saving results...")results=pool.map(getResultsFromJob, job_ids)print("finished saving results")# Do something with the resultsshutil.rmtree("results", ignore_errors=True)os.makedirs("results", exist_ok=True)forrinresults:path=os.path.join(r,"outputs","*.csv")csv_file=glob.glob(path)forfincsv_file:print("moving%sto results"%f)shutil.move(f,"results")if__name__=="__main__":main("hashes.txt",10)This code has a few interesting features:Change the value in themaincall (main("hashes.txt", 10)) to change the number of jobs to execute.Because all jobs are complete at different times, there's a loop to check that all jobs have been completed before downloading the results. If you don't do this, you'll likely see an error when trying to download the results. Thewhile Trueloop is used to monitor the status of jobs and wait for them to complete.When downloading the results, the IPFS get often times out, so I wrapped that in a loop. Thefor i in range(0, 5)loop in thegetResultsFromJobfunction involves retrying thebacalhau job getoperation if it fails to complete successfully.Let's run it!Copypython3bacalhau.pyHopefully, theresultsdirectory contains all the combined results from the jobs we just executed. Here's we're expecting to see CSV files:Copylsresultstransactions_00000000_00049999.csvtransactions_00150000_00199999.csvtransactions_00050000_00099999.csvtransactions_00200000_00249999.csvtransactions_00100000_00149999.csvtransactions_00250000_00299999.csvSuccess! We've now executed a bunch of jobs in parallel using Python. This is a great way to execute lots of jobs in a repeatable manner. You can alter the file above for your purposes.Next StepsYou might also be interested in the following examples:Analysing Data with Python PandasSupportIf you have questions or need support or guidance, please reach out to theBacalhau team via Slack(#generalchannel).PreviousRunning Jupyter Notebooks on BacalhauNextR (language)Last updated2 months agoOn this pagePrerequisiteExecuting Bacalhau Jobs with Python ScriptsNext StepsSupportWas this helpful?Edit on GitHubExport as PDFGet SupportExpansoSupportUse CasesDistributed ETLEdge MLDistributed Data WarehousingFlett ManagementAbout UsWho we areWhat we valueNews & BlogBlogNewsExpanso (2024). All Rights Reserved.