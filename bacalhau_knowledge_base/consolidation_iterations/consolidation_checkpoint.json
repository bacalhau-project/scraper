{"consolidated_chunks": ["Task: Compress the following text to approximately 100 tokens for use as an AI system prompt.\nRules:\n1. Prioritize key information, concepts, and technical details.\n2. Remove redundancies and verbose explanations.\n3. Use concise language and technical terminology.\n4. Maintain factual accuracy and important specifics.\n5. Aim for 100 tokens, but do not go below this target.\n\nText to compress:\nSource: expanso_io\nURL: https://expanso.io/newsroom/\n\nHighlighted\n![VentureBeat Logo](https://expanso.io/wp-content/uploads/2023/12/Newsroom-Preview-Pic-Website-2.avif)\n[\nVentureBeat: The end of centralized data? Samsung teams with Expanso on distributed processing ](https://expanso.io/newsroom/venturebeat-samsung-next-teams-with-expanso/)\nRead the Article on VentureBeat\n[Read More](https://expanso.io/newsroom/venturebeat-samsung-next-teams-with-expanso/)\n![](https://expanso.io/wp-content/uploads/2024/05/no-background-newsroom-samsung.avif)\n[\nWhy we invested in Expanso, a distributed compute platform that reduces latency and enhances data governance capabilities ](https://expanso.io/newsroom/why-samsung-next-invested-in-expanso-a-distributed-compute-platform/)\nRead the article on the Samsung Next blog.\n[Read More](https://expanso.io/newsroom/why-samsung-next-invested-in-expanso-a-distributed-compute-platform/)\n![bacalhau v1.2](https://expanso.io/wp-content/uploads/2023/12/Newsroom-Preview-Pic-Website-3.avif)\n[\nAnnouncing Bacalhau 1.2 ](https://expanso.io/newsroom/bacalhau1-2/)\nWe are thrilled to announce the release of Bacalhau 1.2! Following the Bacalhau 1.1 release in September 2023, we\u2019ve explored a variety of innovative and groundbreaking use\n[Read More](https://expanso.io/newsroom/bacalhau1-2/)\nPress releases.\n[\nExpanso Secures Strategic Investment from Samsung Next to Transform Distributed Computing ](https://expanso.io/newsroom/expanso-secures-strategic-investment-by-samsung-next/)\nSEATTLE\u2013(BUSINESS WIRE)- Expanso, the distributed compute company based in Seattle, Washington has secured a strategic investment from Samsung Next that signals big ambitions to transform how data is being managed and processed worldwide. Expanso, founded in early 2023 by tech\n[Read More](https://expanso.io/newsroom/expanso-secures-strategic-investment-by-samsung-next/)\n[\nAnnouncing Bacalhau 1.2 ](https://expanso.io/newsroom/bacalhau1-2/)\nWe are thrilled to announce the release of Bacalhau 1.2! Following the Bacalhau 1.1 release in September 2023, we\u2019ve explored a variety of innovative and groundbreaking use cases, such as: We are also proud to announce that the U.S. Navy chose Bacalhau to manage predictive\n[Read More](https://expanso.io/newsroom/bacalhau1-2/)\n[\nExpanso Lands $7.5M Seed Investment Led by General Catalyst and Hetz Ventures to Revolutionize Distributed Data Processing ](https://expanso.io/newsroom/seed-funding-announcement/)\nExpanso secures $7.5M for it\u2019s open-source software Bacalhau, advancing edge data processing for enterprises, backed by General Catalyst & Hetz Ventures.\n[Read More](https://expanso.io/newsroom/seed-funding-announcement/)\nHeadlines and features.\n![VentureBeat Logo](https://expanso.io/wp-content/uploads/2023/12/Newsroom-Preview-Pic-Website-2.avif)\n[\nVentureBeat: The end of centralized data? Samsung teams with Expanso on distributed processing ](https://expanso.io/newsroom/venture", "Task: Compress the following text to approximately 100 tokens for use as an AI system prompt.\nRules:\n1. Prioritize key information, concepts, and technical details.\n2. Remove redundancies and verbose explanations.\n3. Use concise language and technical terminology.\n4. Maintain factual accuracy and important specifics.\n5. Aim for 100 tokens, but do not go below this target.\n\nText to compress:\nSource: zookeeper_apache\nURL: https://zookeeper.apache.org/doc/r3.7.1/releasenotes.html\n\nRelease Notes - ZooKeeper - Version 3.7.1\nImprovement\nBug\n[ZOOKEEPER-1875](https://issues.apache.org/jira/browse/ZOOKEEPER-1875) - NullPointerException in ClientCnxn$EventThread.processEvent\n[ZOOKEEPER-3128](https://issues.apache.org/jira/browse/ZOOKEEPER-3128) - Get CLI Command displays Authentication error for Authorization error\n[ZOOKEEPER-3652](https://issues.apache.org/jira/browse/ZOOKEEPER-3652) - Improper synchronization in ClientCnxn\n[ZOOKEEPER-3887](https://issues.apache.org/jira/browse/ZOOKEEPER-3887) - In SSL-only server zkServer.sh status command should use secureClientPortAddress instead of clientPortAddress\n[ZOOKEEPER-3988](https://issues.apache.org/jira/browse/ZOOKEEPER-3988) - org.apache.zookeeper.server.NettyServerCnxn.receiveMessage throws NullPointerException\n[ZOOKEEPER-4194](https://issues.apache.org/jira/browse/ZOOKEEPER-4194) - ZooInspector throws NullPointerExceptions to console when node data is null\n[ZOOKEEPER-4204](https://issues.apache.org/jira/browse/ZOOKEEPER-4204) - Flaky test - RequestPathMetricsCollectorTest.testMultiThreadPerf\n[ZOOKEEPER-4247](https://issues.apache.org/jira/browse/ZOOKEEPER-4247) - NPE while processing message from restarted quorum member\n[ZOOKEEPER-4265](https://issues.apache.org/jira/browse/ZOOKEEPER-4265) - Download page broken links\n[ZOOKEEPER-4266](https://issues.apache.org/jira/browse/ZOOKEEPER-4266) - Correct ZooKeeper version in documentation header\n[ZOOKEEPER-4269](https://issues.apache.org/jira/browse/ZOOKEEPER-4269) - acceptedEpoch.tmp rename failure will cause server startup error\n[ZOOKEEPER-4272](https://issues.apache.org/jira/browse/ZOOKEEPER-4272) - Upgrade Netty library to > 4.1.60 due to security vulnerability CVE-2021-21295\n[ZOOKEEPER-4275](https://issues.apache.org/jira/browse/ZOOKEEPER-4275) - Slowness in sasl login or subject.doAs() causes zk client to falsely assume that the server did not respond, closes connection and goes to unnecessary retries\n[ZOOKEEPER-4277](https://issues.apache.org/jira/browse/ZOOKEEPER-4277) - dependency-check:check failing - jetty-server-9.4.38 CVE-2021-28165\n[ZOOKEEPER-4278](https://issues.apache.org/jira/browse/ZOOKEEPER-4278) - dependency-check:check failing - netty-transport-4.1.60.Final CVE-2021-21409\n[ZOOKEEPER-4309](https://issues.apache.org/jira/browse/ZOOKEEPER-4309) - QuorumCnxManager's ListenerHandler thread leak\n[ZOOKEEPER-4331](https://issues.apache.org/jira/browse/ZOOKEEPER-4331) - zookeeper artifact is not compatible with OSGi runtime\n[ZOOKEEPER-4337](https://issues.apache.org/jira/browse/ZOOKEEPER-4337", "Task: Compress the following text to approximately 100 tokens for use as an AI system prompt.\nRules:\n1. Prioritize key information, concepts, and technical details.\n2. Remove redundancies and verbose explanations.\n3. Use concise language and technical terminology.\n4. Maintain factual accuracy and important specifics.\n5. Aim for 100 tokens, but do not go below this target.\n\nText to compress:\nSource: zookeeper_apache\nURL: https://zookeeper.apache.org/doc/r3.9.1/releasenotes.html\n\nRelease Notes - ZooKeeper - Version 3.9.1\nImprovement\n[ZOOKEEPER-4732](https://issues.apache.org/jira/browse/ZOOKEEPER-4732)- improve Reproducible Builds[ZOOKEEPER-4753](https://issues.apache.org/jira/browse/ZOOKEEPER-4753)- Explicit handling of DIGEST-MD5 vs GSSAPI in quorum auth\nTask\n[ZOOKEEPER-4751](https://issues.apache.org/jira/browse/ZOOKEEPER-4751)- Update snappy-java to 1.1.10.5 to address CVE-2023-43642[ZOOKEEPER-4754](https://issues.apache.org/jira/browse/ZOOKEEPER-4754)- Update Jetty to avoid CVE-2023-36479, CVE-2023-40167, and CVE-2023-41900[ZOOKEEPER-4755](https://issues.apache.org/jira/browse/ZOOKEEPER-4755)- Handle Netty CVE-2023-4586\nRelease Notes - ZooKeeper - Version 3.9.0\nSub-task\n[ZOOKEEPER-4327](https://issues.apache.org/jira/browse/ZOOKEEPER-4327)- Flaky test: RequestThrottlerTest\nBug\n[ZOOKEEPER-2108](https://issues.apache.org/jira/browse/ZOOKEEPER-2108)- Compilation error in ZkAdaptor.cc with GCC 4.7 or later[ZOOKEEPER-3652](https://issues.apache.org/jira/browse/ZOOKEEPER-3652)- Improper synchronization in ClientCnxn[ZOOKEEPER-3908](https://issues.apache.org/jira/browse/ZOOKEEPER-3908)- zktreeutil multiple issues[ZOOKEEPER-3996](https://issues.apache.org/jira/browse/ZOOKEEPER-3996)- Flaky test: ReadOnlyModeTest.testConnectionEvents[ZOOKEEPER-4026](https://issues.apache.org/jira/browse/ZOOKEEPER-4026)- CREATE2 requests embeded in a MULTI request only get a regular CREATE response[ZOOKEEPER-4296](https://issues.apache.org/jira/browse/ZOOKEEPER-4296)- NullPointerException when ClientCnxnSocketNetty is closed without being opened[ZOOKEEPER-4308](https://issues.apache.org/jira/browse/ZOOKEEPER-4308)- Flaky test: EagerACLFilterTest.testSetDataFail[ZOOKEEPER-4393](https://issues.apache.org/jira/browse/ZOOKEEPER-4393)- Problem to connect to zookeeper in FIPS mode[ZOOKEEPER-4466](https://issues.apache.org/jira/browse/ZOOKEEPER-4466)- Support different watch modes on same path[ZOOKEEPER-4471](https://issues.apache.org/jira/browse/ZOOKEEPER-4471)- Remove WatcherType.Children break persistent watcher's child events[ZOOKEEPER-4473](https://issues.apache.org/jira/browse/ZOOKEEPER-4473)- zooInspector create root node fail with path validate[ZOOKEEPER-4475](https://issues.apache.org/jira/browse/ZOOKEEPER-4475)- Persistent recursive watcher got NodeChildrenChanged event[ZOOKEEPER-4477](https://issues.apache.org/jira/browse/ZOOKEEPER-4477)- Single Kerberos ticket renewal failure can prevent all future renewals since Java 9[ZOOKEEPER-", "Source: nats_io\nURL: https://nats.io\nURL: https://nats.io\nURL: https://nats.io\nURL: https://nats.io\nURL: https://nats.io\nURL: https://nats.io\nURL: https://nats.io\nURL: https://nats.io\nURL: https://nats.io\nURL: https://nats.io\nURL:", "Task: Compress the following text to approximately 100 tokens for use as an AI system prompt.\nRules:\n1. Prioritize key information, concepts, and technical details.\n2. Remove redundancies and verbose explanations.\n3. Use concise language and technical terminology.\n4. Maintain factual accuracy and important specifics.\n5. Aim for 100 tokens, but do not go below this target.\n\nText to compress:\nSource: apache_kafka\nURL: https://kafka.apache.org/20/documentation.html\n\n[You're viewing documentation for an older version of Kafka - check out our current documentation here.](/documentation)\nDocumentation\nKafka 2.0 Documentation\nPrior releases:[0.7.x](/07/documentation.html),\n[0.8.0](/08/documentation.html),\n[0.8.1.X](/081/documentation.html),\n[0.8.2.X](/082/documentation.html),\n[0.9.0.X](/090/documentation.html),\n[0.10.0.X](/0100/documentation.html),\n[0.10.1.X](/0101/documentation.html),\n[0.10.2.X](/0102/documentation.html),\n[0.11.0.X](/0110/documentation.html),\n[1.0.X](/10/documentation.html),\n[1.1.X](/11/documentation.html).\n[1. Getting Started](#gettingStarted)\n[1.1 Introduction](#introduction)\n[1.2 Use Cases](#uses)\nHere is a description of a few of the popular use cases for Apache Kafka\u00ae.\nFor an overview of a number of these areas in action, see [this blog post](https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying/).\n[Messaging](#uses_messaging)\nKafka works well as a replacement for a more traditional message broker.\nMessage brokers are used for a variety of reasons (to decouple processing from data producers, to buffer unprocessed messages, etc).\nIn comparison to most messaging systems Kafka has better throughput, built-in partitioning, replication, and fault-tolerance which makes it a good\nsolution for large scale message processing applications.\nIn our experience messaging uses are often comparatively low-throughput, but may require low end-to-end latency and often depend on the strong durability guarantees Kafka provides.\nIn this domain Kafka is comparable to traditional messaging systems such as [ActiveMQ](http://activemq.apache.org) or\n[RabbitMQ](https://www.rabbitmq.com).\n[Website Activity Tracking](#uses_website)\nThe original use case for Kafka was to be able to rebuild a user activity tracking pipeline as a set of real-time publish-subscribe feeds.\nThis means site activity (page views, searches, or other actions users may take) is published to central topics with one topic per activity type.\nThese feeds are available for subscription for a range of use cases including real-time processing, real-time monitoring, and loading into Hadoop or\noffline data warehousing systems for offline processing and reporting.\nActivity tracking is often very high volume as many activity messages are generated for each user page view.\n[Metrics](#uses_metrics)\nKafka is often used for operational monitoring data.\nThis involves aggregating statistics from distributed applications to produce centralized feeds of operational data.\n[Log Aggregation](#uses_logs)\nMany people use Kafka as a replacement for a log aggregation solution.\nLog aggregation typically collects physical log files off servers and puts them in a central place (a file server or HDFS perhaps) for processing.\nKafka abstracts away the details of files and gives a cleaner abstraction of log or event data as a stream of messages.\nThis allows for lower-latency processing and easier support for multiple data sources and distributed data consumption.\nIn comparison to log-centric systems like Scribe or Flume, Kafka offers equally good performance, stronger durability guarantees due to replication,\nand much lower end-to-end latency.\n[Stream Processing](#uses_streamprocessing)\nMany users of Kafka process data in processing pipelines consisting of multiple stages, where raw input data is consumed from Kafka topics and then\naggregated, enriched, or otherwise transformed into new topics for further consumption or follow-up processing.", "http://expanso.io/newsroom/why-samsung-next-invested-in-expanso-a-distributed-compute-platform/)\n\nThe next part of the article will be about the upcoming launch of the Galaxy S7 and S7 Edge.\nWe\u2019ll see more details about the S7 and S7 Edge here.\nAs for the S7 Edge, we are happy to announce the launch of the S7 Edge", "https://sna-projects.com/kafka/downloads.php)\nCompressed output", "The following output shows the output of a file in the form of a file in the form of a file in the form of a file in the form of a file in the form of a file in the form of a file in the form of a file in the form of a file in the form of a file in the form of a file in the form of a file in the form of a file in the form of a file in the form of a file in the form of a file", "", "Task: Compress the following text to approximately 100 tokens for use as an AI system prompt.\nRules:\n1. Prioritize key information, concepts, and technical details.\n2. Remove redundancies and verbose explanations.\n3. Use concise language and technical terminology.\n4. Maintain factual accuracy and important specifics.\n5. Aim for 100 tokens, but do not go below this target.\n\nText to compress:\nSource: apache_hadoop\nURL: http://hadoop.apache.org/docs/r3.4.0/hadoop-project-dist/hadoop-common/release/3.4.0/CHANGELOG.3.4.0.html\n\n[HDFS-15196](https://issues.apache.org/jira/browse/HDFS-15196) |\nRBF: RouterRpcServer getListing cannot list large dirs correctly |\nCritical |\nrbf |\nFengnan Li |\nFengnan Li |\n[HDFS-15252](https://issues.apache.org/jira/browse/HDFS-15252) |\nHttpFS: setWorkingDirectory should not accept invalid paths |\nMajor |\nhttpfs |\nHemanth Boyina |\nHemanth Boyina |\n[HDFS-15256](https://issues.apache.org/jira/browse/HDFS-15256) |\nFix typo in DataXceiverServer#run() |\nTrivial |\ndatanode |\nLisheng Sun |\nLisheng Sun |\n[HDFS-15249](https://issues.apache.org/jira/browse/HDFS-15249) |\nThrottledAsyncChecker is not thread-safe. |\nMajor |\nfederation |\nToshihiro Suzuki |\nToshihiro Suzuki |\n[HDFS-15263](https://issues.apache.org/jira/browse/HDFS-15263) |\nFix the logic of scope and excluded scope in Network Topology |\nMajor |\nnet |\nAyush Saxena |\nAyush Saxena |\n[YARN-10207](https://issues.apache.org/jira/browse/YARN-10207) |\nCLOSE_WAIT socket connection leaks during rendering of (corrupted) aggregated logs on the JobHistoryServer Web UI |\nMajor |\nyarn |\nSiddharth Ahuja |\nSiddharth Ahuja |\n[YARN-10226](https://issues.apache.org/jira/browse/YARN-10226) |\nNPE in Capacity Scheduler while using %primary_group queue mapping |\nCritical |\ncapacity scheduler |\nPeter Bacsko |\nPeter Bacsko |\n[HDFS-15269](https://issues.apache.org/jira/browse/HDFS-15269) |\nNameNode should check the authorization API version only once during initialization |\nBlocker |\nnamenode |\nWei-Chiu Chuang |\nWei-Chiu Chuang |\n[HADOOP-16962](https://issues.apache.org/jira/browse/HADOOP-16962) |\nMaking `getBoolean` log warning message for unrecognized value |\nMajor |\nconf |\nCtest |\nCtest |\n[HADOOP-16967](https://issues.apache.org/jira/browse/HADOOP-16967) |\nTestSequenceFile#testRecursiveSeqFileCreate fails in subsequent run |\nMinor |\ncommon, test |\nCtest |\nCtest |\n[MAPREDUCE-7272](https://issues.apache.org/jira/browse/MAPREDUCE-7272) |\nTaskAttemptListenerImpl excessive log messages |\nMajor |\ntest |\nAhmed Hussein |\nAhmed Hussein |\n[HADOOP-16958](https://issues.apache.org/jira/browse/HADOOP-16958) |\nNPE when hadoop.security.authorization is enabled but the input PolicyProvider for ZKFCRpcServer is NULL |\nCritical |\ncommon, ha |\nCtest |\nCtest |\n[YARN-10219](https://issues.apache.org/jira/browse/YARN-10219) |\nYARN service placement constraints is broken |\nBlocker |\nyarn |\nEric Yang", "Task: Compress the following text to approximately 100 tokens for use as an AI system prompt.\nRules:\n1. Prioritize key information, concepts, and technical details.\n2. Remove redundancies and verbose explanations.\n3. Use concise language and technical terminology.\n4. Maintain factual accuracy and important specifics.\n5. Aim for 100 tokens, but do not go below this target.\n\nText to compress:\nsues.apache.org/jira/browse/HDFS-16350) |\nDatanode start time should be set after RPC server starts successfully |\nMinor |\ndatanode |\nViraj Jasani |\nViraj Jasani |\n[YARN-11007](https://issues.apache.org/jira/browse/YARN-11007) |\nCorrect words in YARN documents |\nMinor |\ndocumentation |\nguophilipse |\nguophilipse |\n[YARN-10975](https://issues.apache.org/jira/browse/YARN-10975) |\nEntityGroupFSTimelineStore#ActiveLogParser parses already processed files |\nMajor |\ntimelineserver |\nPrabhu Joseph |\nRavuri Sushma sree |\n[HDFS-16361](https://issues.apache.org/jira/browse/HDFS-16361) |\nFix log format for QueryCommand |\nMinor |\ncommand, diskbalancer |\nTao Li |\nTao Li |\n[HADOOP-18027](https://issues.apache.org/jira/browse/HADOOP-18027) |\nInclude static imports in the maven plugin rules |\nMajor |\nbuild |\nViraj Jasani |\nViraj Jasani |\n[HDFS-16359](https://issues.apache.org/jira/browse/HDFS-16359) |\nRBF: RouterRpcServer#invokeAtAvailableNs does not take effect when retrying |\nMajor |\nrbf |\nTao Li |\nTao Li |\n[HDFS-16332](https://issues.apache.org/jira/browse/HDFS-16332) |\nExpired block token causes slow read due to missing handling in sasl handshake |\nMajor |\ndatanode, dfs, dfsclient |\nShinya Yoshida |\nShinya Yoshida |\n[HADOOP-18021](https://issues.apache.org/jira/browse/HADOOP-18021) |\nProvide a public wrapper of Configuration#substituteVars |\nMajor |\nconf |\nAndras Gyori |\nAndras Gyori |\n[HDFS-16369](https://issues.apache.org/jira/browse/HDFS-16369) |\nRBF: Fix the retry logic of RouterRpcServer#invokeAtAvailableNs |\nMajor |\nrbf |\nAyush Saxena |\nAyush Saxena |\n[HDFS-16370](https://issues.apache.org/jira/browse/HDFS-16370) |\nFix assert message for BlockInfo |\nMinor |\nblock placement |\nTao Li |\nTao Li |\n[HDFS-16293](https://issues.apache.org/jira/browse/HDFS-16293) |\nClient sleeps and holds \u2018dataQueue\u2019 when DataNodes are congested |\nMajor |\nhdfs-client |\nYuanxin Zhu |\nYuanxin Zhu |\n[YARN-9063](https://issues.apache.org/jira/browse/YARN-9063) |\nATS 1.5 fails to start if RollingLevelDb files are corrupt or missing |\nMajor |\ntimelineserver, timelineservice |\nTarun Parimi |\nAshutosh Gupta |\n[YARN-10757](https://issues.apache.org/jira/browse/YARN-10757) |\njsonschema2pojo-maven-plugin version is not defined |\nMajor |\nbuild |\nAkira Ajisaka |\nTamas Domok |\n[YARN-11023](https://issues.apache.org/jira/browse/YARN-11023) |\nExtend the root QueueInfo with max-parallel-apps in CapacityScheduler |\nMajor |\ncapacity scheduler |", "Hadoop version 2.2.2.0.0\nSource: apache_hadoop\nURL: https://hadoop.apache.org/docs/r3.2.4/index.html\nApache Hadoop 3.2.4 is a point release in the 3.2.x release line, building upon the previous stable release 3.2.3.\nUsers are encouraged to read [release notes](./hadoop-", "https://d2908q01vomqb2.cloudfront.net/fb644351560d8296fe6da332236b1f8d61b28a/2023/06/24/NASCAR-1-scaled.jpg)\nSee also:\nSee also:\nhttps://www.facebook.com/NASCAR/NASCAR/NASCAR/NASCAR/NASCAR/NASCAR/NASCAR/NASCAR", "Task: Compress the following text to approximately 100 tokens for use as an AI system prompt.\nRules:\n1. Prioritize key information, concepts, and technical details.\n2. Remove redundancies and verbose explanations.\n3. Use concise language and technical terminology.\n4. Maintain factual accuracy and important specifics.\n5. Aim for 100 tokens, but do not go below this target.\n\nText to compress:\nSource: cap_theorem\nURL: https://en.wikipedia.org/wiki/Doi_(identifier)\n\nDigital object identifier\n[Doi (identifier)](/w/index.php?title=Doi_(identifier)&redirect=no))\n![]() |\n[International DOI Foundation](#IDF_organizational_structure)[10.1000/182](https://doi.org/10.1000/182)[www](https://www.doi.org/the-identifier/what-is-a-doi/).doi.org /the-identifier /what-is-a-doi /A digital object identifier (DOI) is a [persistent identifier](/wiki/Persistent_identifier) or [handle](/wiki/Handle_(computing)) used to uniquely identify various objects, standardized by the [International Organization for Standardization](/wiki/International_Organization_for_Standardization) (ISO).[[1]](#cite_note-iso-1) DOIs are an implementation of the [Handle System](/wiki/Handle_System);[[2]](#cite_note-2)[[3]](#cite_note-3) they also fit within the URI system ([Uniform Resource Identifier](/wiki/Uniform_Resource_Identifier)). They are widely used to identify academic, professional, and government information, such as [journal](/wiki/Academic_journal) articles, research reports, data sets, and official [publications](/wiki/Publication).\nA DOI aims to resolve to its target, the information object to which the DOI refers. This is achieved by binding the DOI to [metadata](/wiki/Metadata) about the object, such as a [URL](/wiki/URL) where the object is located. Thus, by being actionable and [interoperable](/wiki/Interoperability), a DOI differs from [ISBNs](/wiki/ISBN) or [ISRCs](/wiki/International_Standard_Recording_Code) which are identifiers only. The DOI system uses the [indecs Content Model](/wiki/Indecs_Content_Model) for representing [metadata](/wiki/Metadata).\nThe DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI should provide a more stable link than directly using its URL. But if its URL changes, the publisher must update the metadata for the DOI to maintain the link to the URL.[[4]](#cite_note-4)[[5]](#cite_note-5)[[6]](#cite_note-6) It is the publisher's responsibility to update the DOI database. If they fail to do so, the DOI resolves to a [dead link](/wiki/Link_rot), leaving the DOI useless.[[7]](#cite_note-7)\nThe developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000.[[8]](#cite_note-8) Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.[[9]](#cite_note-dd-9) The DOI system is implemented through a federation of registration agencies coordinated by the IDF.[[10]](#cite_note-10) By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations,[[11]](#cite_note-11) and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations[[citation needed](/wiki/Wikipedia:Citation_needed)].\nNomenclature and syntax\n[[edit](/w/index.php?title=Digital_object_identifier&action=edit\u00a7ion=1)]\nA DOI is a type of Handle System handle, which takes the form of a [character string](/wiki/String_(computer_science)) divided into two parts, a prefix and a suffix, separated by a slash.\nprefix/suffix\nThe prefix identifies the registrant of the identifier and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal [Unic", "Task: Compress the following text to approximately 100 tokens for use as an AI system prompt.\nRules:\n1. Prioritize key information, concepts, and technical details.\n2. Remove redundancies and verbose explanations.\n3. Use concise language and technical terminology.\n4. Maintain factual accuracy and important specifics.\n5. Aim for 100 tokens, but do not go below this target.\n\nText to compress:\nSource: grpc_io\nURL: https://grpc.io/docs/languages/python/basics/\n\nBasics tutorial\nA basic tutorial introduction to gRPC in Python.\nBasics tutorial\nThis tutorial provides a basic Python programmer\u2019s introduction to working with gRPC.\nBy walking through this example you\u2019ll learn how to:\n- Define a service in a\n.proto\nfile. - Generate server and client code using the protocol buffer compiler.\n- Use the Python gRPC API to write a simple client and server for your service.\nIt assumes that you have read the [Introduction to gRPC](/docs/what-is-grpc/introduction/) and are familiar\nwith [protocol\nbuffers](https://protobuf.dev/overview). You can\nfind out more in the [proto3 language\nguide](https://protobuf.dev/programming-guides/proto3) and [Python\ngenerated code\nguide](https://protobuf.dev/reference/python/python-generated).\nWhy use gRPC?\nOur example is a simple route mapping application that lets clients get information about features on their route, create a summary of their route, and exchange route information such as traffic updates with the server and other clients.\nWith gRPC we can define our service once in a.proto\nfile and generate clients\nand servers in any of gRPC\u2019s supported languages, which in turn can be run in\nenvironments ranging from servers inside a large data center to your own tablet \u2014\nall the complexity of communication between different languages and environments is\nhandled for you by gRPC. We also get all the advantages of working with protocol\nbuffers, including efficient serialization, a simple IDL, and easy interface\nupdating.\nExample code and setup\nThe example code for this tutorial is in\n[grpc/grpc/examples/python/route_guide](https://github.com/grpc/grpc/tree/v1.66.0/examples/python/route_guide).\nTo download the example, clone the grpc\nrepository by running the following\ncommand:\n$ git clone -b v1.66.0 --depth 1 --shallow-submodules https://github.com/grpc/grpc\nThen change your current directory to examples/python/route_guide\nin the repository:\n$ cd grpc/examples/python/route_guide\nYou also should have the relevant tools installed to generate the server and\nclient interface code - if you don\u2019t already, follow the setup instructions in\n[Quick start](../quickstart/).\nDefining the service\nYour first step (as you\u2019ll know from the [Introduction to gRPC](/docs/what-is-grpc/introduction/)) is to\ndefine the gRPC service and the method request and response types using\n[protocol\nbuffers](https://protobuf.dev/overview). You can\nsee the complete.proto\nfile in\n[ examples/protos/route_guide.proto](https://github.com/grpc/grpc/blob/v1.66.0/examples/protos/route_guide.proto).\nTo define a service, you specify a named service\nin your.proto\nfile:\nservice RouteGuide {\n// (Method definitions not shown)\n}\nThen you define rpc\nmethods inside your service definition, specifying their\nrequest and response types. gRPC lets you define four kinds of service method,\nall of which are used in the RouteGuide\nservice:\nA simple RPC where the client sends a request to the server using the stub and waits for a response to come back, just like a normal function call.\n// Obtains the feature at a given position. rpc GetFeature(Point) returns (Feature) {}\nA response-streaming RPC where the client sends a request to the server and gets a stream to read a sequence of messages back. The client reads from the returned stream until there are no more messages. As you can see in the example, you specify a response-streaming method by placing the\nstream\nkeyword before the response type.//", "Task: Compress the following text to approximately 100 tokens for use as an AI system prompt.\nRules:\n1. Prioritize key information, concepts, and technical details.\n2. Remove redundancies and verbose explanations.\n3. Use concise language and technical terminology.\n4. Maintain factual accuracy and important specifics.\n5. Aim for 100 tokens, but do not go below this target.\n\nText to compress:\nSource: kubernetes_io\nURL: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\n\nLabels and Selectors\nLabels are key/value pairs that are attached to\n[objects](/docs/concepts/overview/working-with-objects/#kubernetes-objects) such as Pods.\nLabels are intended to be used to specify identifying attributes of objects\nthat are meaningful and relevant to users, but do not directly imply semantics\nto the core system. Labels can be used to organize and to select subsets of\nobjects. Labels can be attached to objects at creation time and subsequently\nadded and modified at any time. Each object can have a set of key/value labels\ndefined. Each Key must be unique for a given object.\n\"metadata\": {\n\"labels\": {\n\"key1\" : \"value1\",\n\"key2\" : \"value2\"\n}\n}\nLabels allow for efficient queries and watches and are ideal for use in UIs\nand CLIs. Non-identifying information should be recorded using\n[annotations](/docs/concepts/overview/working-with-objects/annotations/).\nMotivation\nLabels enable users to map their own organizational structures onto system objects in a loosely coupled fashion, without requiring clients to store these mappings.\nService deployments and batch processing pipelines are often multi-dimensional entities (e.g., multiple partitions or deployments, multiple release tracks, multiple tiers, multiple micro-services per tier). Management often requires cross-cutting operations, which breaks encapsulation of strictly hierarchical representations, especially rigid hierarchies determined by the infrastructure rather than by users.\nExample labels:\n\"release\" : \"stable\"\n,\"release\" : \"canary\"\n\"environment\" : \"dev\"\n,\"environment\" : \"qa\"\n,\"environment\" : \"production\"\n\"tier\" : \"frontend\"\n,\"tier\" : \"backend\"\n,\"tier\" : \"cache\"\n\"partition\" : \"customerA\"\n,\"partition\" : \"customerB\"\n\"track\" : \"daily\"\n,\"track\" : \"weekly\"\nThese are examples of\n[commonly used labels](/docs/concepts/overview/working-with-objects/common-labels/);\nyou are free to develop your own conventions.\nKeep in mind that label Key must be unique for a given object.\nSyntax and character set\nLabels are key/value pairs. Valid label keys have two segments: an optional\nprefix and name, separated by a slash (/\n). The name segment is required and\nmust be 63 characters or less, beginning and ending with an alphanumeric\ncharacter ([a-z0-9A-Z]\n) with dashes (-\n), underscores (_\n), dots (.\n),\nand alphanumerics between. The prefix is optional. If specified, the prefix\nmust be a DNS subdomain: a series of DNS labels separated by dots (.\n),\nnot longer than 253 characters in total, followed by a slash (/\n).\nIf the prefix is omitted, the label Key is presumed to be private to the user.\nAutomated system components (e.g. kube-scheduler\n, kube-controller-manager\n,\nkube-apiserver\n, kubectl\n, or other third-party automation) which add labels\nto end-user objects must specify a prefix.\nThe kubernetes.io/\nand k8s.io/\nprefixes are\n[reserved](/docs/reference/labels-annotations-taints/) for Kubernetes core components.\nValid label value:\n- must be 63 characters or less (can be empty),\n- unless empty, must begin and end with an alphanumeric character (\n[a-z0-9A-Z]\n), - could contain dashes (\n-\n), underscores (_\n), dots (.\n), and alphanumerics between.\nFor example, here's a manifest for a Pod that has two labels\nenvironment: production\nand app: nginx\n:\napiVersion: v1\nkind: Pod\nmetadata:\nname: label-demo\nlabels:", "Content:\nContent:\nContent:\nContent:\nContent:\nContent:\nContent:\nContent:\nContent:\nContent:\nContent:\nContent:\nContent:\nContent:\nContent:\nContent:\nContent:\nContent:\nContent:\nContent:\nContent:\nContent:\nContent:\nContent:\nContent:\nContent:\nContent:\nContent:\nContent:\nContent:\nContent:\nContent:\nContent:", "Task: Compress the following text to approximately 100 tokens for use as an AI system prompt.\nRules:\n1. Prioritize key information, concepts, and technical details.\n2. Remove redundancies and verbose explanations.\n3. Use concise language and technical terminology.\n4. Maintain factual accuracy and important specifics.\n5. Aim for 100 tokens, but do not go below this target.\n\nText to compress:\nSource: kubernetes_io\nURL: https://kubernetes.io/docs/concepts/extend-kubernetes/operator/\n\nOperator pattern\nOperators are software extensions to Kubernetes that make use of\n[custom resources](/docs/concepts/extend-kubernetes/api-extension/custom-resources/)\nto manage applications and their components. Operators follow\nKubernetes principles, notably the [control loop](/docs/concepts/architecture/controller/).\nMotivation\nThe operator pattern aims to capture the key aim of a human operator who is managing a service or set of services. Human operators who look after specific applications and services have deep knowledge of how the system ought to behave, how to deploy it, and how to react if there are problems.\nPeople who run workloads on Kubernetes often like to use automation to take care of repeatable tasks. The operator pattern captures how you can write code to automate a task beyond what Kubernetes itself provides.\nOperators in Kubernetes\nKubernetes is designed for automation. Out of the box, you get lots of built-in automation from the core of Kubernetes. You can use Kubernetes to automate deploying and running workloads, and you can automate how Kubernetes does that.\nKubernetes' [operator pattern](/docs/concepts/extend-kubernetes/operator/)\nconcept lets you extend the cluster's behaviour without modifying the code of Kubernetes\nitself by linking [controllers](/docs/concepts/architecture/controller/) to\none or more custom resources. Operators are clients of the Kubernetes API that act as\ncontrollers for a [Custom Resource](/docs/concepts/extend-kubernetes/api-extension/custom-resources/).\nAn example operator\nSome of the things that you can use an operator to automate include:\n- deploying an application on demand\n- taking and restoring backups of that application's state\n- handling upgrades of the application code alongside related changes such as database schemas or extra configuration settings\n- publishing a Service to applications that don't support Kubernetes APIs to discover them\n- simulating failure in all or part of your cluster to test its resilience\n- choosing a leader for a distributed application without an internal member election process\nWhat might an operator look like in more detail? Here's an example:\n- A custom resource named SampleDB, that you can configure into the cluster.\n- A Deployment that makes sure a Pod is running that contains the controller part of the operator.\n- A container image of the operator code.\n- Controller code that queries the control plane to find out what SampleDB resources are configured.\n- The core of the operator is code to tell the API server how to make\nreality match the configured resources.\n- If you add a new SampleDB, the operator sets up PersistentVolumeClaims to provide durable database storage, a StatefulSet to run SampleDB and a Job to handle initial configuration.\n- If you delete it, the operator takes a snapshot, then makes sure that the StatefulSet and Volumes are also removed.\n- The operator also manages regular database backups. For each SampleDB resource, the operator determines when to create a Pod that can connect to the database and take backups. These Pods would rely on a ConfigMap and / or a Secret that has database connection details and credentials.\n- Because the operator aims to provide robust automation for the resource it manages, there would be additional supporting code. For this example, code checks to see if the database is running an old version and, if so, creates Job objects that upgrade it for you.\nDeploying operators\nThe most common way to deploy an operator is to add the\nCustom Resource Definition and its associated Controller to your cluster.\nThe Controller will normally run outside of the\n[control plane](/docs/reference/glossary/?all=true#term-control-plane),\nmuch as you would run any containerized application.\nFor example, you can run the controller in your cluster as a Deployment.\nUsing an operator\nOnce you have an operator deployed, you'd use it by adding, modifying or deleting the kind of resource that the operator uses. Following the above example,", "https://github.com/jelica/hadoop/distribution-source-pipermail/issues/1422\nThe source code is available on Github.\nSource code is available on GitHub.\nSource code is available on GitHub.\nSource code is available on GitHub.\nSource code is available on GitHub.\nSource code is available on GitHub.\nSource code is available on GitHub.\nSource code is available on GitHub.\nSource code is available on GitHub", "Task: Compress the following text to approximately 100 tokens for use as an AI system prompt.\nRules:\n1. Prioritize key information, concepts, and technical details.\n2. Remove redundancies and verbose explanations.\n3. Use concise language and technical terminology.\n4. Maintain factual accuracy and important specifics.\n5. Aim for 100 tokens, but do not go below this target.\n\nText to compress:\nSource: grpc_io\nURL: https://grpc.io/docs/languages/cpp/basics/\n\nBasics tutorial\nA basic tutorial introduction to gRPC in C++.\nBasics tutorial\nThis tutorial provides a basic C++ programmer\u2019s introduction to working with gRPC.\nBy walking through this example you\u2019ll learn how to:\n- Define a service in a\n.proto\nfile. - Generate server and client code using the protocol buffer compiler.\n- Use the C++ gRPC API to write a simple client and server for your service.\nIt assumes that you have read the [Introduction to gRPC](/docs/what-is-grpc/introduction/) and are familiar\nwith [protocol\nbuffers](https://protobuf.dev/overview). Note\nthat the example in this tutorial uses the proto3 version of the protocol\nbuffers language: you can find out more in\nthe [proto3 language\nguide](https://protobuf.dev/programming-guides/proto3) and [C++\ngenerated code\nguide](https://protobuf.dev/reference/cpp/cpp-generated).\nWhy use gRPC?\nOur example is a simple route mapping application that lets clients get information about features on their route, create a summary of their route, and exchange route information such as traffic updates with the server and other clients.\nWith gRPC we can define our service once in a.proto\nfile and generate clients\nand servers in any of gRPC\u2019s supported languages, which in turn can be run in\nenvironments ranging from servers inside a large data center to your own tablet \u2014\nall the complexity of communication between different languages and environments is\nhandled for you by gRPC. We also get all the advantages of working with protocol\nbuffers, including efficient serialization, a simple IDL, and easy interface\nupdating.\nExample code and setup\nThe example code is part of the grpc\nrepo under [examples/cpp/route_guide](https://github.com/grpc/grpc/tree/v1.66.0/examples/cpp/route_guide).\nGet the example code and build gRPC:\nFollow the Quick start instructions to\n[build and locally install gRPC from source](/docs/languages/cpp/quickstart/#install-grpc).From the repo folder, change to the route guide example directory:\n$ cd examples/cpp/route_guide\nRun\ncmake\n$ mkdir -p cmake/build $ cd cmake/build $ cmake -DCMAKE_PREFIX_PATH=$MY_INSTALL_DIR../..\nDefining the service\nOur first step (as you\u2019ll know from the [Introduction to gRPC](/docs/what-is-grpc/introduction/)) is to\ndefine the gRPC service and the method request and response types using\n[protocol buffers](https://protobuf.dev/overview).\nYou can see the complete.proto\nfile in\n[ examples/protos/route_guide.proto](https://github.com/grpc/grpc/blob/v1.66.0/examples/protos/route_guide.proto).\nTo define a service, you specify a named service\nin your.proto\nfile:\nservice RouteGuide {\n...\n}\nThen you define rpc\nmethods inside your service definition, specifying their\nrequest and response types. gRPC lets you define four kinds of service method,\nall of which are used in the RouteGuide\nservice:\nA simple RPC where the client sends a request to the server using the stub and waits for a response to come back, just like a normal function call.\n// Obtains the feature at a given position. rpc GetFeature(Point) returns (Feature) {}\nA server-side streaming RPC where the client sends a request to the server and gets a stream to read a sequence of messages back. The client reads from the returned stream until there are no more messages. As you can see in our example, you specify a server-side streaming method by placing the\nstream\nkeyword before the response type.//", "Task: Compress the following text to approximately 100 tokens for use as an AI system prompt.\nRules:\n1. Prioritize key information, concepts, and technical details.\n2. Remove redundancies and verbose explanations.\n3. Use concise language and technical terminology.\n4. Maintain factual accuracy and important specifics.\n5. Aim for 100 tokens, but do not go below this target.\n\nText to compress:\nSource: kubernetes_io\nURL: https://kubernetes.io/docs/tasks/job/parallel-processing-expansion/\n\nParallel Processing using Expansions\nThis task demonstrates running multiple [Jobs](/docs/concepts/workloads/controllers/job/)\nbased on a common template. You can use this approach to process batches of work in\nparallel.\nFor this example there are only three items: apple, banana, and cherry. The sample Jobs process each item by printing a string then pausing.\nSee [using Jobs in real workloads](#using-jobs-in-real-workloads) to learn about how\nthis pattern fits more realistic use cases.\nBefore you begin\nYou should be familiar with the basic,\nnon-parallel, use of [Job](/docs/concepts/workloads/controllers/job/).\nYou need to have a Kubernetes cluster, and the kubectl command-line tool must\nbe configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a\ncluster, you can create one by using\n[minikube](https://minikube.sigs.k8s.io/docs/tutorials/multi_node/)\nor you can use one of these Kubernetes playgrounds:\nFor basic templating you need the command-line utility sed\n.\nTo follow the advanced templating example, you need a working installation of\n[Python](https://www.python.org/), and the Jinja2 template\nlibrary for Python.\nOnce you have Python set up, you can install Jinja2 by running:\npip install --user jinja2\nCreate Jobs based on a template\nFirst, download the following template of a Job to a file called job-tmpl.yaml\n.\nHere's what you'll download:\napiVersion: batch/v1\nkind: Job\nmetadata:\nname: process-item-$ITEM\nlabels:\njobgroup: jobexample\nspec:\ntemplate:\nmetadata:\nname: jobexample\nlabels:\njobgroup: jobexample\nspec:\ncontainers:\n- name: c\nimage: busybox:1.28\ncommand: [\"sh\", \"-c\", \"echo Processing item $ITEM && sleep 5\"]\nrestartPolicy: Never\n# Use curl to download job-tmpl.yaml\ncurl -L -s -O https://k8s.io/examples/application/job/job-tmpl.yaml\nThe file you downloaded is not yet a valid Kubernetes\n[manifest](/docs/reference/glossary/?all=true#term-manifest).\nInstead that template is a YAML representation of a Job object with some placeholders\nthat need to be filled in before it can be used. The $ITEM\nsyntax is not meaningful to Kubernetes.\nCreate manifests from the template\nThe following shell snippet uses sed\nto replace the string $ITEM\nwith the loop\nvariable, writing into a temporary directory named jobs\n. Run this now:\n# Expand the template into multiple files, one for each item to be processed.\nmkdir./jobs\nfor i in apple banana cherry\ndo\ncat job-tmpl.yaml | sed \"s/\\$ITEM/$i/\" >./jobs/job-$i.yaml\ndone\nCheck if it worked:\nls jobs/\nThe output is similar to this:\njob-apple.yaml\njob-banana.yaml\njob-cherry.yaml\nYou could use any type of template language (for example: Jinja2; ERB), or write a program to generate the Job manifests.\nCreate Jobs from the manifests\nNext, create all the Jobs with one kubectl command:\nkubectl create -f./jobs\nThe output is similar to this:\njob.batch/process-item-apple created\njob.batch/process-item-banana created\njob.batch/process-item-cherry created\nNow, check on the jobs:\nkubectl get", "GOOGLE25\nSource: grpc_io\nURL: https://grpc.io/docs/languages/node/\ngRPConf 2025 is happening next week!\nThis will be the first time we will have a language-specific page on the GOOGLE25.\nPlease note: if you're already familiar with the GOOGLE25, you'll need to download the GOOGLE25 version of the software before you can get the", "https://github.com/etcd-io/website/issues/new).\nThe code is in the file /etc/common/contrib/\nIf you want to see the current output, please [send it to us](https://github.com/etcd-io/website/issues/new).\nIf you want to see the current output, please [send it to us](https://github.com/etcd-io/website/", "", "Source: bacalhau_docs\nNote: This is not the only source we have used for this project. We also have a bunch of other projects in our development pipeline.\nSource: bacalhau_docs", "Source: required)\n: Specifies the S3 bucket that contains the start function or the main execution code of the task. The output should point to the location of the WASM binary.Entrypoint\n(string: <optional>)\n: An array of strings containing arguments that will be supplied to the program as ARGV. This allows parameterized execution of the WASM task.EnvironmentVariables\n(map[string]string: <optional>)\n: An array of strings containing", "CKAs are the most commonly used and most common examples of CKAs for the Kubernetes training partners.\nThe CKAs are the most commonly used and most common examples of CKAs for the Kubernetes training partners.\nThe CKAs are the most commonly used and most common examples of CKAs for the Kubernetes training partners.\nThe CKAs are the most commonly used and most common examples of CKAs for the", "Kubernetes Training Partners\nVetted training providers who have deep experience in cloud native technology training.\nInterested in becoming\n[KTP](https://www.cncf.io/certification/training/)?\nKubernetes Training Partners\nVetted training providers who have deep experience in cloud native technology training.\nInterested in becoming\n[KTP](https://www.cncf.io/certification/training/)?\nKubernet"], "total_size": 46770, "total_tokens_processed": 43720, "total_tokens": 5971061, "iteration": 0}